<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张逸霄的技术小站</title>
  
  <subtitle>欢迎RSS订阅我的个人主页！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ldzhangyx.github.io/"/>
  <updated>2019-10-16T07:21:59.614Z</updated>
  <id>http://ldzhangyx.github.io/</id>
  
  <author>
    <name>张逸霄</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使音乐模型更好地训练的一些技巧（2019.10.16）</title>
    <link href="http://ldzhangyx.github.io/2019/10/16/practice-1016/"/>
    <id>http://ldzhangyx.github.io/2019/10/16/practice-1016/</id>
    <published>2019-10-16T07:17:32.000Z</published>
    <updated>2019-10-16T07:21:59.614Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch与TensorBoard的交互"><a href="#PyTorch与TensorBoard的交互" class="headerlink" title="PyTorch与TensorBoard的交互"></a>PyTorch与TensorBoard的交互</h1><p>PyTorch最近一直在改进TensorBoard的支持，包括在最新v1.3中，也在一直改进。</p><p>最基本的使用方法是，新建一个<code>SummaryWriter</code>对象，之后在合适的位置进行<code>add_scalar()</code>记录，最后<code>close()</code>即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line">writer.add_scalar(<span class="string">"valid loss"</span>, loss, n_iter) <span class="comment"># 三个参数代表变量，数值，横坐标</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>文件会保存在代码的<code>.run/</code>目录里，访问<code>localhost:6006</code>即可进行观察。</p><h1 id="音乐数据集的移调数据扩增"><a href="#音乐数据集的移调数据扩增" class="headerlink" title="音乐数据集的移调数据扩增"></a>音乐数据集的移调数据扩增</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_X_augment = []</span><br><span class="line">train_Y_augment = []</span><br><span class="line"><span class="keyword">for</span> i, target <span class="keyword">in</span> enumerate(train_Y):</span><br><span class="line">    train_X_augment.append(pad(train_X[i], pad_size))</span><br><span class="line">    train_Y_augment.append(pad(train_Y[i], pad_size))</span><br><span class="line">    <span class="keyword">if</span> augment_data:</span><br><span class="line">        <span class="keyword">for</span> direction <span class="keyword">in</span> [<span class="number">-1</span>, <span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">for</span> shift <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">12</span>):</span><br><span class="line">                train_X_temp = (train_X[i]).clone()</span><br><span class="line">                train_X_temp[:] += direction * shift</span><br><span class="line">                train_X_augment.append(pad(train_X_temp, pad_size))</span><br><span class="line">                train_Y_augment.append(pad(train_Y[i], pad_size))</span><br><span class="line">train_X = torch.stack(train_X_augment)</span><br><span class="line">train_Y= torch.stack(train_Y_augment)</span><br></pre></td></tr></table></figure><h1 id="统计模型总参数量"><a href="#统计模型总参数量" class="headerlink" title="统计模型总参数量"></a>统计模型总参数量</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">params = list(model.parameters())</span><br><span class="line">total_params = sum(x.size()[<span class="number">0</span>] * x.size()[<span class="number">1</span>] <span class="keyword">if</span> len(x.size()) &gt; <span class="number">1</span> <span class="keyword">else</span> x.size()[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> params <span class="keyword">if</span> x.size())</span><br><span class="line">print(<span class="string">'Args:'</span>, args)</span><br><span class="line">print(<span class="string">'Model total parameters:'</span>, total_params)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PyTorch与TensorBoard的交互&quot;&gt;&lt;a href=&quot;#PyTorch与TensorBoard的交互&quot; class=&quot;headerlink&quot; title=&quot;PyTorch与TensorBoard的交互&quot;&gt;&lt;/a&gt;PyTorch与TensorBoard的
      
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="【music generation, PyTorch, 心得体会]" scheme="http://ldzhangyx.github.io/tags/%E3%80%90music-generation-PyTorch-%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
  </entry>
  
  <entry>
    <title>音乐相关会议的deadline一览</title>
    <link href="http://ldzhangyx.github.io/2019/09/12/music-conference-deadline/"/>
    <id>http://ldzhangyx.github.io/2019/09/12/music-conference-deadline/</id>
    <published>2019-09-12T15:05:35.000Z</published>
    <updated>2019-10-16T07:21:19.453Z</updated>
    
    <content type="html"><![CDATA[<p>网站运营的简要介绍。</p><a id="more"></a><p>之前看到aideadlin.es对AI各个领域会议做了一个汇总。音乐领域没有专门的网站做汇集，于是我fork过来重新整理发布了一个网站，挂载在我特意注册的小号上。</p><p>网址是：yixiao-music.github.io，直接点这个网页的侧栏tab也可以直接进去。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>网页更新采用ISMIR社区的回复更新：<a href="https://groups.google.com/a/ismir.net/forum/#!topic/community/8CnJfljcx0E" target="_blank" rel="noopener">https://groups.google.com/a/ismir.net/forum/#!topic/community/8CnJfljcx0E</a></p><p>我计划手动根据回复对这个网站进行数据更新，以避免我的GitHub小号因不常登陆而收不到PR的情况。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;网站运营的简要介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="http://ldzhangyx.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="MIR" scheme="http://ldzhangyx.github.io/tags/MIR/"/>
    
      <category term="community" scheme="http://ldzhangyx.github.io/tags/community/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch中，LSTM的两种不同形式</title>
    <link href="http://ldzhangyx.github.io/2019/09/11/lstm/"/>
    <id>http://ldzhangyx.github.io/2019/09/11/lstm/</id>
    <published>2019-09-11T10:20:56.000Z</published>
    <updated>2019-09-11T10:20:56.885Z</updated>
    
    <content type="html"><![CDATA[<p>最近在LSTM这一块有点扯不清，现在回想一下发现自己将两种LSTM记混了。</p><a id="more"></a><h1 id="LSTM-input与label无关"><a href="#LSTM-input与label无关" class="headerlink" title="LSTM(input与label无关)"></a>LSTM(input与label无关)</h1><p>第一种是这样的：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>t时刻的input x与t-1时刻的label y没有关系，完成的是一个序列到另一个序列的转换。</p><p>Neural Language Models通过input和output的错位，用一行LSTM完成操作。</p><p>PyTorch中，直接</p><h1 id="LSTM（input与label有关）"><a href="#LSTM（input与label有关）" class="headerlink" title="LSTM（input与label有关）"></a>LSTM（input与label有关）</h1><p>LSTM被广泛地用于构造Encoder-Decoder模型。Encoder部分没什么问题，但是Decoder部分与上面提到的结构不同，因为Decoder的过程是一步步解码的过程，是将t-1时刻应有的输出，传递给t时刻的cell。这意味着在编码的时候不能一步到位。</p><p>TensorFlow的解决方案是包装了一个decoder。</p><p>在训练的时候，PyTorch的官方文档框架（<a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html）是这样的：" target="_blank" rel="noopener">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html）是这样的：</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">if use_teacher_forcing:</span><br><span class="line">    # Teacher forcing: Feed the target as the next input</span><br><span class="line">    for di in range(target_length):</span><br><span class="line">        decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">            decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">        loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">        decoder_input = target_tensor[di]  # Teacher forcing</span><br><span class="line"></span><br><span class="line">else:</span><br><span class="line">    # Without teacher forcing: use its own predictions as the next input</span><br><span class="line">    for di in range(target_length):</span><br><span class="line">        decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">            decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">        topv, topi = decoder_output.topk(1)</span><br><span class="line">        decoder_input = topi.squeeze().detach()  # detach from history as input</span><br><span class="line"></span><br><span class="line">        loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">        if decoder_input.item() == EOS_token:</span><br><span class="line">            break</span><br></pre></td></tr></table></figure><p>可以看到Decoder是通过for循环进行逐步解码的。这和上面那类LSTM模型的运作规律<strong>不一致</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在LSTM这一块有点扯不清，现在回想一下发现自己将两种LSTM记混了。&lt;/p&gt;
    
    </summary>
    
      <category term="基础技巧" scheme="http://ldzhangyx.github.io/categories/%E5%9F%BA%E7%A1%80%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
      <category term="基础" scheme="http://ldzhangyx.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="坑" scheme="http://ldzhangyx.github.io/tags/%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>诗歌生成模型“九歌”：《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/08/10/jiuge/"/>
    <id>http://ldzhangyx.github.io/2019/08/10/jiuge/</id>
    <published>2019-08-10T07:22:58.000Z</published>
    <updated>2019-08-10T07:30:27.415Z</updated>
    
    <content type="html"><![CDATA[<p>清华孙茂松老师组的工作，从效果来看，“九歌”的效果相当不错。适逢最近这个模型开源，我希望能梳理一下和这个模型有关的论文，尤其是《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》的工作（发表在EMNLP 2018）。</p><a id="more"></a><h1 id="关于模型开源的相关信息"><a href="#关于模型开源的相关信息" class="headerlink" title="关于模型开源的相关信息"></a>关于模型开源的相关信息</h1><p>今年7月，九歌模型开源。其诗歌生成的模型在这个链接里：<a href="https://github.com/THUNLP-AIPoet/StylisticPoetry" target="_blank" rel="noopener">https://github.com/THUNLP-AIPoet/StylisticPoetry</a></p><p>同时，他们组也放出了相关领域的论文列表：<a href="https://github.com/THUNLP-AIPoet/PaperList" target="_blank" rel="noopener">https://github.com/THUNLP-AIPoet/PaperList</a></p><p>以及诗歌生成的相关数据集：<a href="https://github.com/THUNLP-AIPoet/Datasets" target="_blank" rel="noopener">https://github.com/THUNLP-AIPoet/Datasets</a></p><p>有兴趣的话可以深入了解。</p><h1 id="相关论文梳理"><a href="#相关论文梳理" class="headerlink" title="相关论文梳理"></a>相关论文梳理</h1><p>《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》这篇论文在发表之后，被相关的后续论文引用。</p><ul><li><p>Sentiment-Controllable Chinese Poetry Generation</p></li><li><p>Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation</p></li><li><p>Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System</p></li><li><p>GPT-based Generation for Classical Chinese Poetry（华为诺亚方舟实验室）</p></li></ul><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>论文的目标在于无监督生成风格诗歌。之前的诗歌生成更注重于一致性、连续性。本文更关注于同一输入下生成不同风格的诗歌。</p><p>同一意象下，人们可以以不同的风格写出不同的诗歌：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ul><li>诗歌并没有明确的风格label。本文使用了无监督的方法生成不同风格的诗词。</li><li>论文中，我们提出了一个模型，可以从诗歌中解耦出不同的风格，并在给定人工风格输入后生成特定风格的诗歌。模型总体是seq2seq的，同时通过最大化衡量两个随机变量之间依赖的互信息，以强化人工风格输入和生成的特定风格输出的关系。</li><li>实验结果表明模型可以生成不同风格的诗歌而不丢失一致性和连贯性。</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>模型输入：$s_\text{input}$和风格id，$k \in K$，K为风格总数。</p><h3 id="互信息-Mutual-Information"><a href="#互信息-Mutual-Information" class="headerlink" title="互信息(Mutual Information)"></a>互信息(Mutual Information)</h3><p>互信息常被用于无监督生成模型。当我们遇到存在潜在的类别差别而没有标签数据，我们就需要一类能够无监督地辨别出这类潜在标签的数据。互信息在生成模型中被人们注意，最早应该是InfoGAN的提出。</p><p>给定两个随机变量$X$，$Y$，可以将两个随机变量的互信息记为$I(X, Y)$。互信息表示在一个随机变量中包含另一个随机变量的信息的数量。也可以理解为两个变量的相关性。</p><p>互信息可以表示为联合概率$P(X,Y)$与边缘概率的$P(X)P(Y)$的相关性：</p><script type="math/tex; mode=display">I(X, Y)=\int_{Y} \int_{X} p(X, Y) \log \frac{p(X, Y)}{p(X) p(Y)} d X d Y</script><h3 id="风格解耦的Decoder模型"><a href="#风格解耦的Decoder模型" class="headerlink" title="风格解耦的Decoder模型"></a>风格解耦的Decoder模型</h3><p>定义输入句子$X$，输出句子$Y$，字典为$V$，时间步为$T$。</p><p>模型如图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>将Encoder最后一步的hidden state与风格的one-hot向量concatenate起来，送Decoder。</p><p>我们无法从理论上证明，Decoder的output能与风格K正确地对应起来。output很可能会忽略掉风格输入，使得输出不受风格影响。因此我们显式添加一个约束，强制风格K与output有强烈的依赖关系。</p><p>假定K的id分布k是一个均匀分布的随机向量，就是这样：</p><script type="math/tex; mode=display">P_r(\text{sty} = k) = \frac{1}{k}, \text{for }k= 1,2,...,K</script><p>目标转化为：<strong>最大化风格分布$P_r(\text{sty})$与输出分布$P_r(Y;X)$之间的互信息。</strong></p><p>互信息这么计算：</p><script type="math/tex; mode=display">\begin{aligned} & I(\operatorname{Pr}(\text {Sty}), \operatorname{Pr}(Y ; X)) \\=& \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \int_{Y | k ; X} \log \frac{\operatorname{Pr}(Y, S t y=k ; X)}{\operatorname{Pr}(\text {Sty}=k) \operatorname{Pr}(Y ; X)} d Y \\=& \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \int_{Y | k ; X} \log \frac{\operatorname{Pr}(Y, S t y=k ; X)}{\operatorname{Pr}(Y ; X)} d Y \\ &-\sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \log \operatorname{Pr}(\text {Sty}=k)\\=& \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \int_{Y | k ; X} \log \operatorname{Pr}(\text {Sty}=k | Y) d Y+\log K \\ =&\int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k | Y) \log P(\text {Sty}=k | Y) d Y+\log K\end{aligned}</script><p>因为输入变量$\text{sty}$与$X$相互独立，所以</p><script type="math/tex; mode=display">\operatorname{Pr}(S t y=k | Y ; X)=\operatorname{Pr}(S t y=k | Y)</script><p>由于后验分布$\operatorname{Pr}(S t y=k | Y)$是未知的，我们无法对它计算积分。于是我们使用变分推理最大化的方式，训练一个参数化的函数$Q(\text{Sty}=k|Y)$，这个函数估计了后验分布，最大化方程互信息的下界：</p><script type="math/tex; mode=display">\begin{aligned} & I(\operatorname{Pr}(S t y), \operatorname{Pr}(Y ; X))-\log K \\=& \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log \operatorname{Pr}(S t y=k | Y) d Y \\=& \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log Q(S t y=k | Y) d Y \\ &+\int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log \frac{\operatorname{Pr}(S t y=k | Y)}{Q(S t y=k | Y)} d Y \\=& \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log Q(S t y=k | Y) d Y \\ &+\int_{Y ; X} K L(\operatorname{Pr}(S t y | Y), Q(\operatorname{Sty} | Y)) d Y \\ \geq & \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log Q(S t y=k | Y) d Y \\=& \sum_{k=1}^{K} \operatorname{Pr}(S t y=k) \int_{Y | k ; X} \log Q(S t y=k | Y) d Y \end{aligned}</script><p>我们知道KL散度一定是非负的，故上式的不等式一定成立。</p><p>因此我们只需要设计一个参数化的函数Q，使得Q被模型最大化，那么目的就达到了。</p><h3 id="后验分布估计"><a href="#后验分布估计" class="headerlink" title="后验分布估计"></a>后验分布估计</h3><p>给定输出序列$Y$，函数$Q$估计序列$Y$的风格的概率分布。直接计算序列$Y$的平均embedding，用矩阵$W$映射为一个值：</p><script type="math/tex; mode=display">Q(S t y | Y)=\operatorname{softmax}\left(W \cdot \frac{1}{T} \sum_{i=1}^{T} e\left(y_{i}\right)\right)</script><p>最后要做的是计算$Q$在$Y|k;X$上的积分，然而序列$Y$的搜索空间是字典长度的指数，因此采用枚举的方式计算积分。如果我们采样部分Y，这个结果是不可微的。这里我们使用embedding的期望进行积分的近似。</p><h3 id="期望的词向量embedding"><a href="#期望的词向量embedding" class="headerlink" title="期望的词向量embedding"></a>期望的词向量embedding</h3><p>之前的相关工作是《Semantic parsing with semi-supervised sequential autoencoders》。即，只生成一个期望embedding序列，$Y|k;X$，拥有100%的生成概率。下式表达第i词的分布：</p><script type="math/tex; mode=display">p\left(y_{i} | y_{1}, y_{2}, \ldots y_{i-1}, X\right)=g\left(y_{i}, s_{i}\right)</script><p>所以第i词的期望就是：</p><script type="math/tex; mode=display">\operatorname{expect}(i ; k, X)=\sum_{c \in V} g\left(c | s_{i}\right) e(c)</script><p>之后将其喂给下一步的输出：</p><script type="math/tex; mode=display">s_{i+1}=L S T M_{d e c o d e r}\left(s_{i},\left[e x p e c t(i ; k, X), a_{i+1}\right]\right)</script><p>然后使用这些期望向量近似$Y|k;X$的概率分布。</p><p>所以，上面推导出来的</p><script type="math/tex; mode=display">Q(S t y | Y)=\operatorname{softmax}\left(W \cdot \frac{1}{T} \sum_{i=1}^{T} e\left(y_{i}\right)\right)</script><p>可以被重写为：</p><script type="math/tex; mode=display">\mathcal{L}_{r e g}=\frac{1}{K} \sum_{k=1}^{K} \log \left\{\operatorname{softmax}\left(W * \frac{1}{T} \sum_{i=1}^{T} \operatorname{expect}(i ; k, X)\right)[k]\right\}</script><p>k代表第k个风格。所以上面一直推导的的积分公式就可以表达为上式。</p><p>于是上面那个公式就可以作为正则项加入训练过程，作为一个额外的loss：</p><script type="math/tex; mode=display">\operatorname{Train}(X, Y)=\sum_{i=1}^{T} \log p\left(y_{i} | y_{1} y_{2} \ldots y_{i-1}, X\right)+\lambda \mathcal{L}_{\text { reg }}</script><p>loss的前半部分与风格没有什么关系，保证生成的效果，公式中的X置零；而后半部分保证了结果的风格相关。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>主要通过人工评估。</p><h3 id="数据集与模型细节"><a href="#数据集与模型细节" class="headerlink" title="数据集与模型细节"></a>数据集与模型细节</h3><p>16.8W首古诗，一半五言，一半七言。8:1:1划分。将连续的两个句子作为训练对$(X,Y)$。</p><p>embedding和Encoder的hidden size为512，风格K=10，所以Decoder的hidden state维度为1034维。</p><p>batch size=50，前5W个batch中$\lambda=0$，之后设为1.</p><h4 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h4><ul><li>Seq2seq，主要用于对比风格模块的提升。</li><li>Polish(Yan, 2016)，特点是多次修改生成的句子。</li><li>Memory(Zhang et al., 2017)，将memory融入到诗歌生成，可以被视为一个正则化行为。</li></ul><p>不考虑rule-based或模板模型。</p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>将第一句话作为评估，依次生成后续三个句子。由于模型需要指定风格，于是我们使用上文的函数Q来估计第一句的风格，在后面三句中套用同样的风格。</p><ul><li>流畅性</li><li>一致性</li><li>意义性</li><li>诗歌性</li></ul><p>上述四个指标被纳入考虑。</p><p>分别生成了20首五言和20首七言，作为评估。邀请了10个中国文学专家，做了两组实验。第一组对比seq2seq，第二组对比别的先进模型。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>seq2seq生成的是混合风格的模型，更可能产生无意义的诗歌。风格作为统一信息，为一致性提供了帮助，并且缩小了空间，因此在风格内部能学习得更加准确、紧凑。</p><h3 id="习得风格的可解释性"><a href="#习得风格的可解释性" class="headerlink" title="习得风格的可解释性"></a>习得风格的可解释性</h3><p>10种风格的关键词如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>首先输入每种风格各生成5首诗，再由人类专家进行分类。由于是无监督生成的，故模型很可能不会严格对齐人类的风格标注。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>可以看见，人类可以以更高的概率成功识别出很多风格。观察结果表明，模型的学习风格只有两三个关键词有意义、可识别。除此之外，生成的诗歌需要是多样的，否则他们不能被区分开。</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;清华孙茂松老师组的工作，从效果来看，“九歌”的效果相当不错。适逢最近这个模型开源，我希望能梳理一下和这个模型有关的论文，尤其是《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》的工作（发表在EMNLP 2018）。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="诗歌生成" scheme="http://ldzhangyx.github.io/tags/%E8%AF%97%E6%AD%8C%E7%94%9F%E6%88%90/"/>
    
      <category term="generation" scheme="http://ldzhangyx.github.io/tags/generation/"/>
    
      <category term="poem generation" scheme="http://ldzhangyx.github.io/tags/poem-generation/"/>
    
      <category term="distanglement" scheme="http://ldzhangyx.github.io/tags/distanglement/"/>
    
  </entry>
  
  <entry>
    <title>如何评估树结构的相似性？也许可以了解一下Tree Editing Distance</title>
    <link href="http://ldzhangyx.github.io/2019/08/07/tree-evaluation/"/>
    <id>http://ldzhangyx.github.io/2019/08/07/tree-evaluation/</id>
    <published>2019-08-07T09:03:45.000Z</published>
    <updated>2019-08-07T09:03:45.412Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇关于树编辑距离相关算法和相关包的介绍。</p><a id="more"></a><p>如何衡量两个树结构之间有多相似？这个领域有很多相关研究。在这里介绍一个可能的算法：Tree Editing Distance。</p><p>这个词被翻译为树编辑距离。形象地理解，就是从一棵树通过编辑，变为另一棵树的复杂程度。</p><h1 id="树编辑距离的形式化"><a href="#树编辑距离的形式化" class="headerlink" title="树编辑距离的形式化"></a>树编辑距离的形式化</h1><p>形式化地来说，有序标记树之间的树编辑距离，是奖一棵树转换为另一棵树的节点操作的最小成本序列。定义下面三种编辑：</p><ol><li>删除节点，将其子节点连接到父节点上，以保持有序。</li><li>在一个已知的节点，和这个节点的连续子节点的子序列之间插入一个节点。</li><li>更改这个节点的label。</li></ol><p>每编辑一次需要花费一点成本，而问题的目标是找到一个操作序列，使得总成本最小。</p><h1 id="现有算法"><a href="#现有算法" class="headerlink" title="现有算法"></a>现有算法</h1><p>Queen’s University有一份slides，专门用于讲述一些比较早期的算法：<a href="http://research.cs.queensu.ca/TechReports/Reports/1995-372.pdf" target="_blank" rel="noopener">http://research.cs.queensu.ca/TechReports/Reports/1995-372.pdf</a></p><p>这个问题可以被递归地解决，然而其具有指数级的复杂度。Zhang and Shasha（1989年）将这个问题的复杂度降低到了$O(m^2n^2)$级别，而最新的算法（Demaine et al.）将算法的时间复杂度降低到了$O(n^2m(1+\log \frac{m}{n}))$级别。</p><p>具体的论文亮点可以参考这个网址：<a href="http://tree-edit-distance.dbresearch.uni-salzburg.at/#bibliography" target="_blank" rel="noopener">http://tree-edit-distance.dbresearch.uni-salzburg.at/#bibliography</a></p><h1 id="可以使用的包"><a href="#可以使用的包" class="headerlink" title="可以使用的包"></a>可以使用的包</h1><p>GitHub上有人复现了Zhang and Shasha的算法，并且可以直接通过pip方式安装，import调用： <a href="https://github.com/timtadh/zhang-shasha" target="_blank" rel="noopener">https://github.com/timtadh/zhang-shasha</a></p><p>奥地利萨尔茨堡大学公开了一个Java程序，用于实现RTED等复杂度更低的算法：<a href="http://tree-edit-distance.dbresearch.uni-salzburg.at" target="_blank" rel="noopener">http://tree-edit-distance.dbresearch.uni-salzburg.at</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇关于树编辑距离相关算法和相关包的介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="structure" scheme="http://ldzhangyx.github.io/tags/structure/"/>
    
      <category term="algorithm" scheme="http://ldzhangyx.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch使用的日常（2019.8.5）</title>
    <link href="http://ldzhangyx.github.io/2019/08/05/pytorch-0801/"/>
    <id>http://ldzhangyx.github.io/2019/08/05/pytorch-0801/</id>
    <published>2019-08-05T08:23:48.000Z</published>
    <updated>2019-08-27T09:59:07.060Z</updated>
    
    <content type="html"><![CDATA[<h1 id="nn-Embedding"><a href="#nn-Embedding" class="headerlink" title="nn.Embedding"></a>nn.Embedding</h1><p>我一直对nn.Embedding层感到困惑。它在训练中有没有改变过呢？</p><p>查询StackOverflow，我得到了一个答案：</p><ol><li>我可以随机初始化一个词向量，也可以导入一个预训练的词向量；</li><li>我可以选择是否让它参与训练（默认参与了训练）。</li></ol><p>如果要固定某几层不进行训练，需要做两件事：</p><ol><li>层数的requires_grad参数设为False；</li><li>传给optimizer的parameters需要手动过滤。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)</span><br></pre></td></tr></table></figure><h1 id="CUDA-out-of-memory"><a href="#CUDA-out-of-memory" class="headerlink" title="CUDA out of memory"></a>CUDA out of memory</h1><p>遇到了一次这样的情况。考虑optimizer产生的大量中间结果，使得显存爆炸。</p><p>解决方案：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    ....</span><br></pre></td></tr></table></figure><p>将validation代码放在里面，减少显存占用。实际起到的效果和<code>model.eval()</code>是一样的。</p><h1 id="Learning-Rate-Scheduler"><a href="#Learning-Rate-Scheduler" class="headerlink" title="Learning Rate Scheduler"></a>Learning Rate Scheduler</h1><p>学习率衰减（weight decay）被证明与参数的二阶范数正则化等价。PyTorch使用<code>lr_scheduler</code>进行学习率衰减操作。下面是一类</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr=0.0005)</span><br><span class="line">scheduler = ExponentialLR(optimizer, gamma=0.95, minimum=1e-5)</span><br></pre></td></tr></table></figure><p>在训练的时候这样调整学习率：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler.step()</span><br></pre></td></tr></table></figure><p>将这行代码插入你认为应该调整的地方。下面是一种常见做法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">output = model(input)</span><br><span class="line">loss = loss(output, label)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">schedule.step()</span><br><span class="line">...</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;nn-Embedding&quot;&gt;&lt;a href=&quot;#nn-Embedding&quot; class=&quot;headerlink&quot; title=&quot;nn.Embedding&quot;&gt;&lt;/a&gt;nn.Embedding&lt;/h1&gt;&lt;p&gt;我一直对nn.Embedding层感到困惑。它在训练中有没有
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Regularizing and Optimizing LSTM Language Models》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/07/31/awd-lstm/"/>
    <id>http://ldzhangyx.github.io/2019/07/31/awd-lstm/</id>
    <published>2019-07-31T13:43:44.000Z</published>
    <updated>2019-08-01T07:53:53.966Z</updated>
    
    <content type="html"><![CDATA[<p>人称“语言建模的王者”，AWD-LSTM模型。</p><a id="more"></a><p>原文地址：<a href="https://openreview.net/references/pdf?id=rJI9awpBf" target="_blank" rel="noopener">https://openreview.net/references/pdf?id=rJI9awpBf</a></p><h1 id="论文亮点"><a href="#论文亮点" class="headerlink" title="论文亮点"></a>论文亮点</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文为了解决词级别的语言建模问题，研究了正则化、优化LSTM类模型的策略。本文提出了weighted-dropped LSTM，在hidden to hidden weights上使用了DropConnect，作为循环正则化的形式；此外引入了NT-AvSGD，是平均随机梯度方法的非单调触发变体，使用NT条件自动确定平均触发。</p><p>使用这些和其他正则化策略，AWD-LSTM在PTB和WikiTest-2上达到了最优ppl。模型可用于Q-RNN（Quasi-RNN，和SRU目的一致，都是对RNN进行了并行化改进）。</p><h2 id="Weight-Dropped-LSTM"><a href="#Weight-Dropped-LSTM" class="headerlink" title="Weight-Dropped LSTM"></a>Weight-Dropped LSTM</h2><p>首先给出LSTM的公式：</p><script type="math/tex; mode=display">\begin{aligned} i_{t} &=\sigma\left(W^{i} x_{t}+U^{i} h_{t-1}\right) \\ f_{t} &=\sigma\left(W^{f} x_{t}+U^{f} h_{t-1}\right) \\ o_{t} &=\sigma\left(W^{o} x_{t}+U^{o} h_{t-1}\right) \\ \tilde{c}_{t} &=\tanh \left(W^{c} x_{t}+U^{c} h_{t-1}\right) \\ c_{t} &=i_{t} \odot \tilde{c}_{t}+f_{t} \odot+\tilde{c}_{t-1} \\ h_{t} &=o_{t} \odot \tanh \left(c_{t}\right) \end{aligned}</script><p>正则化技术用于防止RNN过度拟合。之前的递归正则化对$h_{t-1}$或$c_{t}$起作用，阻止了黑盒RNN的实现。</p><p>建议使用DropConnect。DropConnect应用于隐藏状态之间的权重矩阵(Ui, Uf, Uo, Uc)，而不是隐藏状态或记忆状态。这一丢弃操作只在前向和反向传播前进行一次，从而最小化对训练速度的影响，并且适用于任何标准的黑盒RNN优化实现。通过丢弃隐藏状态之间的权重矩阵的部分信息，可以防止LSTM循环连接的过拟合。</p><h2 id="NT-ASGD"><a href="#NT-ASGD" class="headerlink" title="NT-ASGD"></a>NT-ASGD</h2><p>在语言建模过程中，不带动量的SGD的表现比其他优化方法更好。 我们调查AvSGD进一步改善训练过程。AvSGD展示了许多惊讶的结果，比如说而渐近二阶收敛。普通SGD更新公式如下：</p><script type="math/tex; mode=display">w_{k+1}=w_{k}-\gamma_{k} \hat{\nabla} f\left(w_{k}\right)</script><p>但AvSGD不使用最后一步的迭代作为解，而是使用</p><script type="math/tex; mode=display">\frac{1}{(K-T+1)} \sum_{i=T}^{K} w_{i}</script><p>其中K是迭代总数，T是用户指定的平均计算触发器。</p><p>ASGD的缺点在于，学习率$\ita_k$和T的调参没有明确的方法论。</p><p>理想情况下，SGD收敛到稳态分布时，需要触发平均。语言建模使用的一种常见策略是指标停滞时降低学习率，而触发也可以参照这种方法。</p><p>NT-ASGD的作法是，</p><ul><li>仅当验证测度在多次循环后没有改善的情况下才触发平均。所以，当验证测度在n次循环（n称为非单调间隔超参数）后没有改善时，算法换用ASGD。论文作者发现n=5这一设置效果良好。</li><li>使用恒定学习率，因此无需进一步调整。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="其他正则化策略"><a href="#其他正则化策略" class="headerlink" title="其他正则化策略"></a>其他正则化策略</h2><h3 id="变长BPTT"><a href="#变长BPTT" class="headerlink" title="变长BPTT"></a>变长BPTT</h3><p>论文作者指出了固定长度BPTT的低效。假设我们在100个元素上进行长度为10的固定窗口的BPTT。在这一情形下，任何可以被10整除的元素将没有任何元素可以反向传播。固定长度BPTT阻止了1/10的数据以循环的方式改善自身，还有8/10的数据仅仅使用部分BPTT窗口。</p><p>变长BPTT首先选择一个基本序列长度。人工制定一个BPTT，然后基本BPTT长度有p的概率选择BPTT，也有1-p的概率选择BPTT/2。</p><p>然后通过基本BPTT长度计算得到sequence length：</p><script type="math/tex; mode=display">\text{sequence length} = \max(5, l \in N(bptt, \sigma))</script><p>l从正态分布中取样得到结果。这一步是必要的，因为取样任意序列长度的情况下， 使用固定学习率将倾向于短序列。</p><h3 id="变分Dropout"><a href="#变分Dropout" class="headerlink" title="变分Dropout"></a>变分Dropout</h3><p>在标准dropout中，每次调用dropout时取样一个新的dropout掩码。而在变分dropout中，dropout掩码只在第一次调用时取样，接着锁定的dropout掩码重复应用于前向和反向过程中的所有连接。</p><p>尽管RNN的隐藏到隐藏转换中使用了DropConnect，但其他所有dropout操作中使用了变分dropout，特别是在给定的前向和反向传播中，LSTM的所有输入和输出使用同样的dropout掩码。mini-batch内的每个样本使用不同的dropout掩码，而不是在所有样本上使用同一个掩码，以确保元素丢弃的多样性。</p><h3 id="嵌入Dropout"><a href="#嵌入Dropout" class="headerlink" title="嵌入Dropout"></a>嵌入Dropout</h3><p>实际上就是在Embedding Matrix上使用dropout，使得该字在完整的前向、反向传播上都消失了。该技术最早由A Theoretically Grounded Application of Dropout in Recurrent Neural Networks这篇论文提出。</p><h3 id="权重绑定"><a href="#权重绑定" class="headerlink" title="权重绑定"></a>权重绑定</h3><p>权重绑定在embedding和softmax layer上共享了权重，减少了模型中的总参数。该技术具有理论动机（Inan等，2016），并防止模型必须学习输入和输出之间的一对一对应，从而对标准LSTM语言模型进行实质性改进。</p><h3 id="独立embedding-size和hidden-size"><a href="#独立embedding-size和hidden-size" class="headerlink" title="独立embedding size和hidden size"></a>独立embedding size和hidden size</h3><p>在大多数自然语言处理任务中，预训练和训练的单词矢量都具有相对较低的维度 - 通常在100到400维之间。大多数先前的LSTM语言模型将单词向量的维度与LSTM的隐藏状态的维度联系起来。即使减少单词嵌入大小对防止过度拟合也没有好处，语言模型的总参数的最简单减少是减少单词向量大小。</p><p>为了实现这一点，修改第一个和最后一个LSTM层，使得它们的输入和输出维度分别等于减小的嵌入大小。</p><h3 id="激活正则化和时域激活正则化"><a href="#激活正则化和时域激活正则化" class="headerlink" title="激活正则化和时域激活正则化"></a>激活正则化和时域激活正则化</h3><p>L2正则化除用于网络参数上，还可以用在独立单元的激活上，和不同时间步里，RNN的输出上。</p><p>激活正则化惩罚显著过大的激活：</p><script type="math/tex; mode=display">\alpha L_{2}\left(m \odot h_{t}\right)</script><p>时域激活正则化，惩罚过大的hidden state波动：</p><script type="math/tex; mode=display">\beta L_{2}\left(h_{t}-h_{t+1}\right)</script><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><p>在PTB和WikiText-2上实验。PTB的词汇量约1W单词，导致大量词汇OOV。WT2的词汇量超过3W。</p><p>LSTM隐单元为三层，1150神经元，Embedding size为400.loss被examples和timesteps平均。embedding被均匀初始化在[-0.1, 0.1]之间，其他权重在$[-\frac{1}{\sqrt{H}}, \frac{1}{\sqrt{H}}]$之间，H为hidden size。</p><p>NT-AvSGD算法训练750 epoches，L相当于1 epoch，n=5. batch size为80（WT2）和40（PTB）。经验表明batch size较大时表现更好。完成后，运行AvSGD，T=0，热启动w0作为finetuning step以进一步改进解。对于这个finetuning步骤，使用算法1的相同的标准终止执行。</p><p>最大范数为0.25的梯度裁剪，初始学习率为30，随机BPTT长度设置为N(70, 5)，p=0.95和N(35, 5)，p=0.05. 用于word vector的dropout、LSTM层间输出，LSTM最上层输出，embedding dropout分别为(0.4, 0.3, 0.4, 0.1)。对WD-LSTM，dropour=0.5用在rurrent weight matrices，而WT2上值增加到0.65，考虑到增加到词汇量。</p><p>对于所以实验，分别使用2和1的AR和TAR值，并将embedding和softmax权重联系起来。所有超参数通过反复实验选择。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="指针模型"><a href="#指针模型" class="headerlink" title="指针模型"></a>指针模型</h3><p>在过去的工作中，已经证明基于指针的注意力模型在改进语言建模方面非常有效（Merity等，2016; Grave等，2016）。鉴于对基础神经语言模型的这种实质性改进，关于指针增强的有效性仍然是一个悬而未决的问题，特别是当重量绑定等改进可能以相互排斥的方式起作用时。</p><p>可以以可忽略的成本在预训练的语言模型之上添加神经缓存模型（Grave等，2016）。神经缓存将先前隐藏状态存储在存储器单元中，然后使用由缓存建议的概率分布和用于预测的语言模型的简单凸组合。缓存模型有三个超参数：缓存的内存大小（窗口），组合的系数（确定两个分布如何混合），以及缓存分布的平坦度。一旦获得训练有素的语言模型，所有这些都在验证集上进行调整，并且不需要自己进行培训，使得使用起来非常便宜。这些超参数的调整值分别为PTB（2000,0.1,1.0）和WT2（3785,0.1279,0.662）。</p><p>在表1和表2中，我们表明该模型进一步改善了语言模型的困惑，PTB的6个困惑点和WT2的11个点。<br>虽然这比Grave等报道的增益要小。<br>（2016），使用LSTM没有重量绑定，这仍然是一个实质性的下降。<br>鉴于神经缓存模型的简单性以及缺乏任何受过训练的组件，这些结果表明现有的神经语言模型基本上缺乏，无法捕获长期依赖关系或有效记住最近看到的单词。<br>为了理解指针对模型的影响，特别是验证集的困惑，我们详细说明了每个单词对表3中缓存模型的整体困惑的贡献。<br>我们计算WikiText-2数据集的验证部分中的目标字的LSTM和LSTM与缓存模型之间的损失函数值（即，对数困扰）的总差异的总和。<br>我们提出差异总和的结果而不是均值，因为后者不合适地过分强调了不经常出现的单词，其中高速缓存有助于显着地忽略频繁出现的单词，其中高速缓存提供适度的改进，累积地做出强有力的贡献。<br>最大累积增益在提高的<unk>令牌的处理，虽然这是超过11540个的情况。<br>第二个最好的改进，大约五分之一由<unk>令牌给定的增益，为经，然而，这仅字发生161次。<br>这表明缓存对于相对罕见的单词仍然有显着帮助，丘吉尔，布莱斯或索尼克进一步证明了这一点。<br>当处理频繁的单词类别（例如标点符号或停用单词）时，缓存不是有益的，语言模型很可能适合这些单词类别。<br>这些观察结果激发了缓存框架的设计，该框架更加了解两个模型的相对优势。</unk></unk></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="AWD-QRNN"><a href="#AWD-QRNN" class="headerlink" title="AWD-QRNN"></a>AWD-QRNN</h3><p>概括一下就是模型也适合Q-RNN。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="模型消融分析"><a href="#模型消融分析" class="headerlink" title="模型消融分析"></a>模型消融分析</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>详见原论文。 最明显的困惑度提升来自LSTM hidden to hidden的lSTM正则化，也就是DropConnect。</p><h2 id="代码与训练速度"><a href="#代码与训练速度" class="headerlink" title="代码与训练速度"></a>代码与训练速度</h2><p><a href="https://github.com/salesforce/awd-lstm-lm" target="_blank" rel="noopener">https://github.com/salesforce/awd-lstm-lm</a></p><p>代码在上述网址开源，记载了更加详细的实验结果。readme里特意提及了速度的问题。NVIDIA Quadro GP100的速度在PTB上大约是65秒一个epoch。考虑到我们将会使用的HookTheory，这个速度相比WT2的速度更具有参考价值。作者体积K80的速度大约是1/3，而我做实验可以用到一块1080Ti(for each task)。根据NVIDIA提供的compute Capability数值来看：</p><ul><li>Tesla K80：3.7</li><li>Tesla V100：7.0</li><li>Tesla P100：6.0</li><li>Quadro GP100：6.0</li><li>GTX 1080Ti：6.1</li><li>Jetson Nano：5.3</li></ul><p>猜想在词级语言建模任务上，我达到45秒每epoch的速度是正常的。另外这个计算力表其实和我的认知差别有点大。K80这么弱的吗？<br>值得一提的是Jetson Nano，达到5.3的分数意味着可以一用了。过段时间我会调研一下树莓派4和Jetson Nano。</p><h1 id="想法和见解"><a href="#想法和见解" class="headerlink" title="想法和见解"></a>想法和见解</h1><p>还能说什么呢，一年被引用200+次，只能说大佬牛逼，工作量和模型质量都是一流的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人称“语言建模的王者”，AWD-LSTM模型。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="AWD-LSTM" scheme="http://ldzhangyx.github.io/tags/AWD-LSTM/"/>
    
      <category term="language model" scheme="http://ldzhangyx.github.io/tags/language-model/"/>
    
  </entry>
  
  <entry>
    <title>简洁明快的命令行解析器argparse简明指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/30/argparse/"/>
    <id>http://ldzhangyx.github.io/2019/07/30/argparse/</id>
    <published>2019-07-30T04:22:23.000Z</published>
    <updated>2019-07-30T07:49:14.937Z</updated>
    
    <content type="html"><![CDATA[<p>为了避免Python代码在需要设置的情况下进行hard code，我们有很多种方式将参数指定在一个地方，便于集中、解耦地修改参数。可以使用的解决办法很多，如设立config类，全局变量指明，以及使用argparse。</p><p>本文是argparse的简明指南，指明了argparse模块的快速上手方法。</p><h1 id="指南"><a href="#指南" class="headerlink" title="指南"></a>指南</h1><h2 id="argparse应该放在文件的什么地方？"><a href="#argparse应该放在文件的什么地方？" class="headerlink" title="argparse应该放在文件的什么地方？"></a>argparse应该放在文件的什么地方？</h2><p>argparse可以放在main.py文件的最上方，仅在import语句块下。</p><h2 id="argparse分为几个步骤？"><a href="#argparse分为几个步骤？" class="headerlink" title="argparse分为几个步骤？"></a>argparse分为几个步骤？</h2><p>四个步骤。</p><p>第一步，import argparse。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br></pre></td></tr></table></figure><p>第二步，定义parser。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser(description=&apos;Process some integers.&apos;)</span><br></pre></td></tr></table></figure><p>第三步，增加参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&apos;integers&apos;, metavar=&apos;N&apos;, type=int, nargs=&apos;+&apos;,</span><br><span class="line">                    help=&apos;an integer for the accumulator&apos;)</span><br><span class="line">parser.add_argument(&apos;--sum&apos;, dest=&apos;accumulate&apos;, action=&apos;store_const&apos;,</span><br><span class="line">                    const=sum, default=max,</span><br><span class="line">                    help=&apos;sum the integers (default: find the max)&apos;)</span><br></pre></td></tr></table></figure><p>第四步，解析参数并实例化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><p>在调用main.py时，在后面增添命令行参数，或以default值的方式指定参数。后面的参数值可以以args变量的各个attribute直接使用，如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args.sum</span><br></pre></td></tr></table></figure><h2 id="add-augment接收几种参数？"><a href="#add-augment接收几种参数？" class="headerlink" title="add_augment接收几种参数？"></a>add_augment接收几种参数？</h2><blockquote><p>name or flags - 一个命名或者一个选项字符串的列表，例如 foo 或 -f, —foo。<br>action - 当参数在命令行中出现时使用的动作基本类型。<br>nargs - 命令行参数应当消耗的数目。<br>const - 被一些 action 和 nargs 选择所需求的常数。<br>default - 当参数未在命令行中出现时使用的值。<br>type - 命令行参数应当被转换成的类型。<br>choices - 可用的参数的容器。<br>required - 此命令行选项是否可省略 （仅选项可用）。<br>help - 一个此选项作用的简单描述。<br>metavar - 在使用方法消息中使用的参数值示例。<br>dest - 被添加到 parse_args() 所返回对象上的属性名。</p></blockquote><ul><li><p>name or flags<br>这几乎是必须的。<code>add_augment</code>后需要为参数命名。一般来说，使用<code>--name</code>字段即可。这样调用时写<code>--name=xxx</code>进行参数指定。</p></li><li><p>action<br>默认action是<code>store</code>，代表存储参数的值。</p></li><li><p>nargs<br>关联剁个参数到一个arguments里。<code>N</code>代表之后N个参数会形成一个列表。<code>+</code>代表片段内所有参数被聚集到列表（直到下一个参数）。</p></li><li><p>default<br>指定默认值。最常用。</p></li><li><p>type<br>指定参数类型，指定为<code>str</code>或<code>int</code>最常用。</p></li><li><p>help<br>填写参数的提示。</p></li></ul><p>参数实例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&apos;--model&apos;, type=str, default=&apos;LSTM&apos;,</span><br><span class="line">                    help=&apos;type of recurrent net (LSTM, QRNN, GRU)&apos;)</span><br><span class="line">parser.add_argument(&apos;--emsize&apos;, type=int, default=100,</span><br><span class="line">                    help=&apos;size of word embeddings&apos;)</span><br><span class="line">parser.add_argument(&apos;--when&apos;, nargs=&quot;+&quot;, type=int, default=[-1],</span><br><span class="line">                    help=&apos;When (which epochs) to divide the learning rate by 10 - accepts multiple&apos;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为了避免Python代码在需要设置的情况下进行hard code，我们有很多种方式将参数指定在一个地方，便于集中、解耦地修改参数。可以使用的解决办法很多，如设立config类，全局变量指明，以及使用argparse。&lt;/p&gt;
&lt;p&gt;本文是argparse的简明指南，指明了a
      
    
    </summary>
    
      <category term="简明指南" scheme="http://ldzhangyx.github.io/categories/%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/"/>
    
    
      <category term="argparse" scheme="http://ldzhangyx.github.io/tags/argparse/"/>
    
      <category term="python" scheme="http://ldzhangyx.github.io/tags/python/"/>
    
      <category term="tutorial" scheme="http://ldzhangyx.github.io/tags/tutorial/"/>
    
  </entry>
  
  <entry>
    <title>MobaXTerm简明指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/29/mobaxterm-tutorial/"/>
    <id>http://ldzhangyx.github.io/2019/07/29/mobaxterm-tutorial/</id>
    <published>2019-07-29T09:24:38.000Z</published>
    <updated>2019-07-29T10:10:12.163Z</updated>
    
    <content type="html"><![CDATA[<p>这个简明指南旨在介绍MobaXTerm，Windows下的一个现代SSH客户端。</p><a id="more"></a><ol><li><p>首先下载MobaXTerm的对应版本。网址如下：<a href="https://mobaxterm.mobatek.net/download.html" target="_blank" rel="noopener">https://mobaxterm.mobatek.net/download.html</a></p></li><li><p>左上角点击Session，新建一个SSH连接。</p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li>配置完成。</li></ol><p>这篇文章主要想说明MobaXTerm一些优点：</p><ol><li><p>可以检测服务器运行情况。Remote Monitoring开启后，服务器的运行情况可以被实时地监控。</p></li><li><p>自带一个SFTP协议的可视化文件树，并且支持拖动上传下载文件。</p></li><li><p>附带了一个文本编辑器，是vi的良好替代。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个简明指南旨在介绍MobaXTerm，Windows下的一个现代SSH客户端。&lt;/p&gt;
    
    </summary>
    
      <category term="简明指南" scheme="http://ldzhangyx.github.io/categories/%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/"/>
    
    
      <category term="tutorial" scheme="http://ldzhangyx.github.io/tags/tutorial/"/>
    
      <category term="software" scheme="http://ldzhangyx.github.io/tags/software/"/>
    
  </entry>
  
  <entry>
    <title>Coding on Server! PyCharm远程调试、文件同步、GitHub版本控制简明指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/29/ssh-for-pycharm/"/>
    <id>http://ldzhangyx.github.io/2019/07/29/ssh-for-pycharm/</id>
    <published>2019-07-29T09:22:55.000Z</published>
    <updated>2019-07-29T10:10:17.026Z</updated>
    
    <content type="html"><![CDATA[<p>这个指南适用于想在PyCharm上debug，同时使用远程服务器上的解释器运行代码的人。</p><a id="more"></a><p>使用PyCharm本地编辑代码，调用远程服务器的Python解释器。这个方案支持PyCharm单步调试等操作。</p><ol><li><p>新建或打开你想运行的项目。</p></li><li><p>Tools -&gt; Deployment -&gt; Configuration，打开界面。</p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li>在界面中配置对服务器的连接。</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li>在Mappings里面设置文件映射，映射到服务器的对应目录。</li></ol><p>注意，对应的目录是步骤3的root path + Mappings的Deployment path。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li><p>点击左边侧栏的√，使得服务器名字变为粗体。这代表你启用了这个Deployment。</p></li><li><p>Tools -&gt; Deployment -&gt; Settings，将文件同步设置，更改为仅在ctrl+S时保存。至此，所有的本地代码文件都会被同步到服务器上。</p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li>File -&gt; Settings -&gt; Project -&gt; Project Interpreter，打开界面，右上角选择小齿轮， Add，选择服务器上的python.exe应用程序，链接到远程解释器。</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li><p>此时不需要特意设置path mappings，因为会沿用Deployment的mapping。</p></li><li><p>我们开始版本控制。VCS -&gt; Import into Version Control -&gt; Share project on GitHub。</p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>此时你的每一次保存会被视为一次commit。</p><p>配置完毕。</p><p>以下是你可能遇到的问题：</p><ol><li><p>File Transfer失败。可以检查一下mapping地址是否正确。</p></li><li><p>Error:Python helpers are not copied yet to the remote host. Please wait until remote interpreter initialization finishes. 如果你的服务器名称是字符串，推荐改用IP地址；如果无效，请删除服务器上的.pycharm_helper文件夹，等待重新建立index。</p></li></ol><p>以下是一些附带的常用操作。</p><ol><li>PyCharm在debug的时候，可以在右下角的Console Tab点击Python图标，进入交互式Python，你可以在断点debug时运行各种表达式。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个指南适用于想在PyCharm上debug，同时使用远程服务器上的解释器运行代码的人。&lt;/p&gt;
    
    </summary>
    
      <category term="简明指南" scheme="http://ldzhangyx.github.io/categories/%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/"/>
    
    
      <category term="python" scheme="http://ldzhangyx.github.io/tags/python/"/>
    
      <category term="tutorial" scheme="http://ldzhangyx.github.io/tags/tutorial/"/>
    
      <category term="software" scheme="http://ldzhangyx.github.io/tags/software/"/>
    
      <category term="PyCharm" scheme="http://ldzhangyx.github.io/tags/PyCharm/"/>
    
      <category term="SSH" scheme="http://ldzhangyx.github.io/tags/SSH/"/>
    
  </entry>
  
  <entry>
    <title>《Deep Music Analogy via Latent Representation Distanglement》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/07/29/ec2vae/"/>
    <id>http://ldzhangyx.github.io/2019/07/29/ec2vae/</id>
    <published>2019-07-29T06:30:57.000Z</published>
    <updated>2019-07-31T13:28:26.633Z</updated>
    
    <content type="html"><![CDATA[<p>组里大佬们的工作。</p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>类比制作（Analogy-making）是使用计算机算法生成自然而有创造力的音乐作品的关键方法。通常来说，将音乐的抽象形式（如，高层表示及这些表示之间的关系）做部分地转移，可以达成类比的目的。</p><p>这个过程要求对音乐表示做解耦（如，保留风格，创作内容不一样的曲子）。对于人类音乐家来说，是轻而易举的事情，但是这对计算机来说却是一个难题。要完成这个目标，我们有三个步骤：</p><ol><li>从观察（observation）中提取潜在的表示（latent representations）；</li><li>对表示进行解耦，使得解耦后每一部分有唯一的的语义解释；</li><li>将潜在表示映射回实际的音乐。</li></ol><p>这篇论文提出了一个具有明确约束（explicitly-constrained）的VAE模型（简称$EC^2-VAE$），作为所有三个子问题的统一解决方案。</p><p>更确切地说，本文专注于对8-beat音乐片段，以和弦（chord）作为condition，解耦音高（pitch）和节奏（rhythm）的表示。这个模型借用了其他音乐片段的表示，帮助我们实现了假设情景：一个片段如果使用别的音高、节奏、和弦将会怎么样？</p><p>最后，本文用客观的测量方法验证了模型的解耦方法，并通过一个主观的研究，评估了类比的实际例子。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>类比是一个生成高质量艺术作品的有效方法。一般来说，如果两个系统有共同的抽象表示、共同的表示间关系，那么这两个系统就是相似的。举个例子，“氢原子就像太阳系”，形式化表示为$A:B::C:D$的形式：“原子核:氢原子::太阳::太阳系”。两个系统共享的抽象就是“更大的部分是一个系统的中心”。对于生成算法来说，解决一个“what if”问题，即已知ABC，D是什么的问题，是生成算法的一个捷径。</p><p><code>A:B::C:D</code></p><p>对于音乐生成来说，如果A是B的抽象，A与B的关系是，A是音乐B的节奏pattern（一个例子）。那么，如果我们有另一个节奏pattern C，那么B搭配上C的pattern，得到的D将会是什么样子呢？</p><p>类比生成的一大优势是能够产生自然、有创意的结果。之所以自然，是因为模型复用了人类作品实例的表示（如image style等），以及这些概念之间的内在关系；之所以有创意，是因为这些表示使用了新颖的方式进行重新组合，进行了不同的表达。</p><p>我们清楚地意识到，类比的实质是对抽象表征的迁移，而不是浅层observation的组合和复读。要进行有意义的类比，需要对高层表征进行解耦，而这对计算机来说是一个难题。</p><p>EC2-VAE模型，就是一个学习解耦的有效工具。具体来说，Encoder从Observation中获取latent representation，语义约束（semantic constraints）解耦表征，使得每个部分具有唯一的解释，然后Decoder将解耦表示映射回实际的音乐，同时保持表示之间的内在关系。</p><p>作为生成模型，我们希望EC2-VAE有三个特性。</p><ol><li>对表示的解耦是明确的。即我们可以指定哪些维度表示哪些语义因素。</li><li>解耦不能对重建造成大量牺牲。</li><li>训练阶段不需要任何类似的例子，但该模型能够在推理阶段进行类比。</li></ol><p>为了评估，我们提出了一个新的指标（metric）进行调查。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="类比生成"><a href="#类比生成" class="headerlink" title="类比生成"></a>类比生成</h3><p>两种类比算法：</p><ol><li>监督学习。需要大量标注，表示学习的需求较少。</li><li>严格的类比算法，不仅需要学习表示，还需要解耦表示，允许模型通过操纵任何解耦的表示来进行domain-free的类比。</li></ol><h3 id="表示学习-amp-解耦"><a href="#表示学习-amp-解耦" class="headerlink" title="表示学习 &amp; 解耦"></a>表示学习 &amp; 解耦</h3><p>（相关研究略）</p><p>本文模型并不直接约束z，而是将loss应用于与latent factor有关的中间输出。间接而显式的约束使得模型能够进一步将表示分解为音调、节奏等语义因素。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>目标是解开时长为8拍的旋律的节奏和音高两个方面。数据均来源于Nottingham数据集。分辨率为1/4拍。</p><h3 id="数据表示"><a href="#数据表示" class="headerlink" title="数据表示"></a>数据表示</h3><p>每个8拍旋律都可以用一个32x130的矩阵表示。32为时间维，130为音高、保持和休止符维。</p><p>额外设计了一个节奏pattern约束网络的中间输出。8拍节奏模式表示为32x3的矩阵，3为one-hot向量，代表：(onset, holding, rest)。</p><p>chord作为条件，表示成32x12的矩阵（色度图），如12维的multi-hot向量。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型基本结构与Vanilla sequence VAE一致。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>模型的创新点在于，为Decoder的一部分（橙色）设计了一个子任务，通过明确地鼓励$z_r$的中间输出，匹配节点的节奏特征，从而将潜在（latent）节奏表示$z_r$从整体的表示z中解开。</p><p>因此z的另一部分是节奏以外的一切，被解释为音高表示$z_p$。这样明确编码的解耦技术非常灵活，因为可以设计多个子部分，解开z的多个语义可解释的因子。只要可以定义相应潜在因子的中间输出。</p><p>新模型将chord作为condition，优点是使z免于存储和弦信息。</p><p>Decoder中，为了解耦，本文将z分割成了两半：$z_p$和$z_r$，让两边的向量各有128个维度。</p><h3 id="具有解耦功能的ELBO目标的理论正当性"><a href="#具有解耦功能的ELBO目标的理论正当性" class="headerlink" title="具有解耦功能的ELBO目标的理论正当性"></a>具有解耦功能的ELBO目标的理论正当性</h3><p>表征解耦有时会牺牲重建能力。本节主要证明了模型没有遭受大量的解耦-重构悖论，并且模型的likelihood bound接近原始条件的VAE，在一些情况下甚至可能相等。</p><p>ELBO目标函数：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{ELBO}(\phi, \theta)=& \mathbb{E}_{Q}\left[\log P_{\phi}(x | z, c)\right] \\ &-\mathbb{K} \mathbb{L}\left[Q_{\theta}(z | x, c)| | P_{\phi}(z | c)\right] \leq \log P_{\phi}(x | c) \end{aligned}</script><p>下面用$\mathcal{D}$代替$&amp;-\mathbb{K} \mathbb{L}\left[Q_{\theta}(z | x, c)| | P_{\phi}(z | c)\right]$。如果我们将图1(b)的中间节奏输出看作网络的hidden variable，模型的ELBO只会增加基于原始节奏的节奏重建loss，达到原始ELBO的下限。</p><script type="math/tex; mode=display">\begin{aligned} & \mathrm{ELBO}^{\mathrm{new}}(\phi, \theta) \\=& \mathbb{E}_{Q}\left[\log P_{\phi}(x | z, c)\right]-\mathcal{D}+\mathbb{E}_{Q}\left[\log P_{\phi_{r}}\left(r(x) | z_{r}\right)\right] \\=& \operatorname{ELBO}(\phi, \theta)+\mathbb{E}_{Q}\left[\log P_{\phi_{r}}\left(r(x) | z_{r}\right)\right] \end{aligned}</script><p>其中$\phi_r$代表节奏decoder的参数。新ELBO是原始ELBO的一个下界，因为$\mathbb{E}_{Q}\left[\log P_{\phi_{r}}\left(r(x) | z_{r}\right)\right] \leq 0$。</p><p>此外，如果将global decoder的其余部分采用原始节奏而不是节奏decoder的中间输出作为输入，则目标可以重写为：</p><script type="math/tex; mode=display">\begin{aligned} & \operatorname{ELBO}^{\mathrm{new}}(\phi, \theta) \\=& \mathbb{E}_{Q}\left[\underbrace{\log P_{\phi}\left(x | r(x), z_{p}, c\right)+\log P_{\phi}\left(r(x) | z_{r}, c\right)}_{x \underline{ \|} z_{r} | r(x),c;r(x)\underline{ \|}z_p}\right] -\mathcal{D}\\=& \mathbb{E}_{Q}\left[\log P_{\phi}(x, r(x) | z, c)\right]-\mathcal{D} \\=& \mathbb{E}_{Q}\left[P_{\phi}(x | z, c)+\log P_{\phi}(r(x) | x, z, c)\right]-\mathcal{D} \\=& \operatorname{ELBO}(\phi, \theta) \end{aligned}</script><p>第二个等号表示一个完美的解耦，最后一个等号成立，因为$r(x)$由$x$决定。换而言之，我们展示了在某些假设下，两个ELBO相同。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="客观指标"><a href="#客观指标" class="headerlink" title="客观指标"></a>客观指标</h3><p>一旦成功解耦，音高的任何变化都不应影响潜在的节奏表示。为此设计了两个指标：</p><ol><li>换位后的Δz；</li><li>一个augmantation-based序列的F-score。</li></ol><h4 id="音高转换的Δz可视化"><a href="#音高转换的Δz可视化" class="headerlink" title="音高转换的Δz可视化"></a>音高转换的Δz可视化</h4><p>将$F_i$定义为一个操作，将所有音符转$i$个半音。使用L1范数测量z的变化。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>增加音高时，音高的潜在表示的变化$z_p$远高于节奏的潜在表示变化$z_r$。这充分证明了解耦的成功。</p><h4 id="F-score"><a href="#F-score" class="headerlink" title="F-score"></a>F-score</h4><p>从IR角度，可以实现新的评估方法。我们将模型结构定义中的pitch-rhythm split作为ground truth，将factor-wise数据增强（保持一个，改变另一个，vice versa），作为query。将因此出现了最大variance的实际上latent dimensions，成为结果集（result set）。用这种方法，可以量化评估模型的P、R和F分数。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>这是第一个为明确编码的解耦模型定制的测量方法，因此将random作为baseline。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>后面的case study略。</p><h1 id="想法与见解"><a href="#想法与见解" class="headerlink" title="想法与见解"></a>想法与见解</h1><p>一路看下来感觉很流畅，从音乐的结果来看也是一个非常好的模型，起解耦能力值得肯定。</p><p>值得挑一些刺的地方我认为有两点。</p><ol><li><p>在模型描述上，模型的图画得并不是很清楚。比如说在对z的分割上，原文中提到对z对半分割，而图中的描述并不能体现这一点。</p></li><li><p>F-score的evaluation做得并不是很清晰。这个measurement从实现上来说是衡量解耦效果的，但是与random对比并不是好的选择。我猜测，使用“没有明确解耦”的普通VAE比较，也许是一个更好的选择。这就像是ON-LSTM的论文，也有人质疑，普通的LSTM会不会也有一定的order性呢？</p></li></ol><p>模型可以发展的地方相比缺点来说要多得多。首先是解耦设计的灵活性，几乎可以无成本地在之后进行扩展；其次，解耦类比生成的这个topic本身，也应该是正确的方向。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;组里大佬们的工作。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="VAE" scheme="http://ldzhangyx.github.io/tags/VAE/"/>
    
      <category term="representation learning" scheme="http://ldzhangyx.github.io/tags/representation-learning/"/>
    
      <category term="analogy" scheme="http://ldzhangyx.github.io/tags/analogy/"/>
    
  </entry>
  
  <entry>
    <title>我们来聊聊EsAC和Essen Folk Song Database</title>
    <link href="http://ldzhangyx.github.io/2019/07/26/esac/"/>
    <id>http://ldzhangyx.github.io/2019/07/26/esac/</id>
    <published>2019-07-26T07:01:14.000Z</published>
    <updated>2019-07-27T08:39:22.296Z</updated>
    
    <content type="html"><![CDATA[<p>古董级民歌数据集，对数据格式做了一些整理，在本文进行解释和说明。</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>EsAC全称是Essen Associative Code，可能是现存最长的音乐编码系统。EsAC的概念可以追溯到200年前，于1982年正式形成。因为其年代过于久远，现在很多音乐文档，尤其是欧洲音乐文档都在使用这种编码格式。</p><p>1980年-1994年，项目在Helmut Schaffrath在Essen领导推动这个项目。1994年Schaffrach意外去世后，他生前所在的实验室继续着他的工作。</p><h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><p>EsAC项目搜集了世界各地（尤其是欧洲）的民歌和历史上的音乐，编码后用于方便地进行搜索、分析和表示研究。</p><h1 id="格式说明"><a href="#格式说明" class="headerlink" title="格式说明"></a>格式说明</h1><p>EsAC被编码为Database record，在esac-data.org能下载到其txt版本。在这里，一条record对应一首曲调。</p><p>一条常见的记录如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ALTDEU</span><br><span class="line">CUT[Landsknecht als Schlemmer]</span><br><span class="line">REG[Europa, Mitteleuropa, Deutschland]</span><br><span class="line">KEY[A0359  08  G 4/4]</span><br><span class="line">MEL[1_  3b_3b_4_4_  5_5_5_7b_  ^_656_6_  5__0_</span><br><span class="line">    3b_  2_3b_2_2_  1__.+1_  +1_7b_6_6_  5__0_</span><br><span class="line">    5_  +1__+1__  7b__5__  7b__6b__  5__0_</span><br><span class="line">    7b_  5_5_5_3b_  5_5_5_1_  4_3b_2_2_  1__0_</span><br><span class="line">    1_  3b_3b_4_4_  5_5_5_7b_  ^_656_6_  5__0_</span><br><span class="line">    3b_  2_3b_2_2_  1__.+1_  +1_7b_6_6_  5__0_</span><br><span class="line">    5_  +1__+1__  7b__5__  7b__6b__  5__0_</span><br><span class="line">    7b_  5_5_5_3b_  5_5_5_1_  4_3b_2_2_  1__0__</span><br><span class="line">    +1__+1_+1_  7b__6b__  5_5_5__  4_4_4__  3b__3b__</span><br><span class="line">    7b_7b_7b__  5__5_5_  4_4_4__</span><br><span class="line">    3b_3b_3b_3b_  4_3b_2_2_  1__3b__</span><br><span class="line">    4_.41_2_  3b_.45_.6b  7b__.6b_  5_4_3b_2_  1__. //] &gt;&gt;</span><br><span class="line">FCT[Romanze, Ballade, Lied]</span><br><span class="line">CMT[Forster II. 1540 No. 17.]</span><br></pre></td></tr></table></figure><p>其中音乐旋律信息为单音旋律，由简谱格式构成。是的，当时他们采用了简谱格式。旋律包含在MEL字段里。</p><ul><li>CUT：标题</li><li>REG：区域</li><li>TRD：曲调来源，如书籍、录音带</li><li>KEY：签名行</li><li>MEL：旋律线</li><li>FKT：歌曲功能，如舞曲</li><li>BEM：标记</li><li>TXT：歌词</li></ul><h2 id="KEY"><a href="#KEY" class="headerlink" title="KEY"></a>KEY</h2><p>签名行包含了正确解释旋律所需要的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KEY[A0359  08  G 4/4]</span><br></pre></td></tr></table></figure><p>这一行包含了：签名、最小节奏单位、调性、拍子，四个信息。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="EsAC-syntax"><a href="#EsAC-syntax" class="headerlink" title="EsAC syntax"></a>EsAC syntax</h2><p>EsAC的语法基本与简谱一致，但在细微之处做了很多的补充。<br>MEL中的数字代表首调音高，而2、3等数字代表相对音高。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>节奏以下划线和点作为标记。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ul><li>我自己在看数据的时候遇到了”^”的标记，这个意味着跨小节延音。</li><li>两个空格代表小节线</li><li>换行代表断句。</li><li>注意，这里的附点一般出现在延音符后，会将延音后总时长*150%。</li><li>注意，延音符连着出现，时长成倍增长。</li></ul><p>i.e. <code>5__</code>的时长是4倍，<code>5__.</code>的时长是6倍，<code>5___</code>的时长是8倍。</p><ul><li>标注中会出现括号，意味着出现了三连音等连音。</li></ul><p>i.e. <code>(-3-71)  3_2_(-5-72)  4_3_</code></p><p>即使经过这样的处理之后，还是会有这等肮脏的例子，可以说是非常恶心了……</p><p>i.e. <code>33  2__(^_1#_2_)(2_3_4_)  5__(^_4#_5_)(5_3_1_)</code></p><p>甚至还有打拍子……：</p><p>i.e. <code>xxxx  xxxx  1_2-5  1_ //] &gt;&gt;</code></p><p>甚至还有神奇的调子（居然还是德国标注法，将B调标为H调是什么鬼辣！）：</p><p>i.e. <code>KEY[U0002  16  H 6/8]</code></p><p>甚至还有不在旋律结束处加//标记的：</p><p>i.e. <code>1_+2+31_+3+51_+5_+5__  1_+5+61_+6++11_++1_++1__] &gt;&gt;</code></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://wiki.ccarh.org/wiki/EsAC" target="_blank" rel="noopener">http://wiki.ccarh.org/wiki/EsAC</a></li><li><a href="http://www.cs.uu.nl/events/dech1999/dahlig/tsld001.htm" target="_blank" rel="noopener">http://www.cs.uu.nl/events/dech1999/dahlig/tsld001.htm</a></li><li><a href="http://www.esac-data.org/" target="_blank" rel="noopener">http://www.esac-data.org/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;古董级民歌数据集，对数据格式做了一些整理，在本文进行解释和说明。&lt;/p&gt;
    
    </summary>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
      <category term="dataset" scheme="http://ldzhangyx.github.io/tags/dataset/"/>
    
      <category term="music information retrival" scheme="http://ldzhangyx.github.io/tags/music-information-retrival/"/>
    
      <category term="MIR" scheme="http://ldzhangyx.github.io/tags/MIR/"/>
    
      <category term="音乐结构分析" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>《Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/07/21/gnn-for-performance/"/>
    <id>http://ldzhangyx.github.io/2019/07/21/gnn-for-performance/</id>
    <published>2019-07-21T08:38:46.000Z</published>
    <updated>2019-07-21T16:15:10.050Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章的亮点在于利用了图网络编码额外信息，来改进其他任务效果的思想。这篇文章写作清晰，想法新颖，值得关注。文章出自韩国KAIST的Juhan Nam老师组（出身Stanford的CCRMA实验室，师承Malcolm Slaney老师）。</p><a id="more"></a><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文使用图神经网络表示乐谱，将其应用于钢琴演奏的渲染过程中。具体地说，本文使用了音符级别的门控图网络（GGNN）和在小节级别的带分层Attention机制（HAN）的bi-LSTM，进行模型设计。为了建模不同的表现风格，本文使用了VAE。试验结果表明，本文提出的模型产生了更像人类的表现。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>如同演员的表现能更好地调动观众情绪一样，音乐的演奏也具有很多细节。使用计算方法表达演奏，从基于规则的方法、高斯过程、卡尔曼滤波、贝叶斯网络、条件随机场、循环神经网络，都有相关的研究。然而，之前的方法仍然在这个task上有缺陷。</p><p>近年来，生成模型在各个领域取得了很多成果，音乐上如自动作曲、音乐转录、声音合成等，都有显著成就。</p><p>将神经网络应用到音乐数据，一个主要问题在于定义其输入结构。一般来说，乐谱根据其音高，转换成一个一维的序列数据(Simon &amp; Oore, 2017; Oore et al., 2018; Jeong et al., 2018)。但是一维的编码可能会丢失音符之间的一些多边关系。</p><p>另一种输入表示是通过采样，得到Piano Roll的2D矩阵。这使得CNN可以使用，但是基于采样的表示需要更高的维度，和随乐曲复杂性而跟着增长的时间分辨率。这种高维可能会阻碍模型学习长期结构。</p><p>为了解决这个问题，我们提出了一种基于GNN的模型，音符作为图中的节点，而乐谱中音符的关系被转化为一个表（后文会详细论述表的构造）。</p><p>如图1所示，我们将GNN结合到轻量的RNN来学习长期结构。此外，我们建议使用迭代循环来使用彼此的结果更新GNN和RNN的输入。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>与其他生成任务类似，给定condition，生成各种结果是建模有表现力的演奏的重要目标。我们采用VAE训练模型，包含相同条件C但具有不同输出Y的数据。</p><p>系统的范围侧重于为MusicXML格式的乐谱生成MIDI格式的演奏。</p><p>论文两点有两个：</p><ol><li>首次尝试图网络学习乐谱表示；</li><li>HAN+RNN的新方法，对钢琴演奏的模仿是无需数据上的额外注释的。</li></ol><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><p>虽然CNN和RNN分别在图像处理和序列建模方面取得了重大进展，但是有各种类型的数据无法用这些网络正确处理，图形就是其中一个例子。利用图形神经网络（GNN）处理图形数据的早期研究由（Gori等人，2005）和（Scarselli等人，2009）引入。（Li et al。，2016）介绍了门控图神经网络（GGNN），它结合了现代RNN实践的门控循环单元和GNN。虽然之前的模型受到收缩映射的限制，但GGNN模型首先克服了这一局限。最近使用GNN的研究在各种任务中取得了最新成果，例如分析引文网络（Kipf＆Welling，2016），分子结构（Jin等，2018），程序代码（Allamanis等，2018）），学习结构化政策（Wang et al。，2018a）。</p><h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>我们的模型组合了音符级的GGNN，小节级的HAN（使用LSTM），使用在迭代方法中。</p><h2 id="GGNN"><a href="#GGNN" class="headerlink" title="GGNN"></a>GGNN</h2><p>我们采用了有向多类别边GGNN（其实就是有向图，箭头还分了个类）学习输入乐谱中的隐含表示。图可以表示为边与节点的集合，即$G=(V,E)$，其中V是节点，E是边。在音乐中，E就是相邻音符之间的连接线。</p><p>我们定义了六种边：next，rest，onset，sustain，voice，slur。</p><p>next：将音符连接到接下来的音符上。如接下来的音符在上一个音符结束时恰好开始。<br>rest：将音符连接到休止符后面的音符上。多个休止符会被当做单个处理。<br>onset：连接两个同时开始的音符。<br>sustain：在一个音符的开始和结尾中间出现的音符用sustain连接。<br>voice：是next边集的子集。它们仅用以连接同一个语音内的音符。<br>slur：在同一个圆滑线下的音符通过slur边互相连接。</p><p>（sustain和slur的定义有一些模糊，因为其没有注明是否仅为邻接的）</p><p>除了onset边以外，所有的边都是有向的。所以我们将前向和后向的边视为两种不同的类型。同时，为每一个音符添加自我连接，这样一共得到12种不同的边。每种边共享不同的权重参数。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>我们使用GGNN，因为它在学习图中的节点级表示方面有优势。</p><p>GGNN的概念介绍如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="r1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="r2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="r3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="r4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>论文中对于GGNN的表述没有超出上面所介绍的范围。</p><h2 id="Hierarchical-Attention-RNN（HAN）"><a href="#Hierarchical-Attention-RNN（HAN）" class="headerlink" title="Hierarchical Attention RNN（HAN）"></a>Hierarchical Attention RNN（HAN）</h2><p>我们之前的工作曾使用过HAN渲染有表现力的钢琴演奏。（对比了HRNN和HMRNN）</p><p>在分层RNN模型中，使用HAN的原因是其直接适用于GNN。事实上，因为HAN使用注意力来总结较低级别的表示，因此它可以直接应用于任何类型的网络。</p><p>我们的系统里，使用了context attention（《Hierarchical attention networks for document classification》里的），将音符表示转为小节向量。attention同时使用了多头注意力机制（multi-head attention）。通过这种方法，每一个小节的内容可以转化到高层的一个节点里。</p><p>作者的公式有一些问题，不过大致上就是attention计算那一套，其中$u_c$是context vector。因为context vector并没有接受其他输入，实际上这是一个trainable的参数。计算细节不在此赘述。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="eq2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="Iterative-Sequential-Graph-Network"><a href="#Iterative-Sequential-Graph-Network" class="headerlink" title="Iterative Sequential Graph Network"></a>Iterative Sequential Graph Network</h2><p>组合来自不同分层单元的输出的简单方法是将它们连接为单个向量。然而，这种方法具有以下限制：较低级别的层不能以较高层中编码的长期上下文为条件，因为较高级别的输出不会影响较低级别的层。<br>在HRNN和HM-RNN中，较低级别的隐藏状态在分层边界处被馈送到较高级别，反之亦然。然而，在HAN中，隐藏状态传播仅在自下而上的方向上进行。<br>当模型的目标结果是给定顺序输入的单个输出时，例如在最初应用HAN的文档分类中，这种限制并不重要。但是为每个音符学习音乐表示时，我们希望高层的信息也能传到底层去，利用更加扩展的上下文学习。<br>为了克服这个限制，我们提出了GGNN和HAN的组合，称为迭代顺序图网络（ISGN），GGNN和HAN以迭代的方式将它们的结果互相馈送：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>GGNN不仅接受音符表示，还接受高层的hidden state，以concatenate的方式输送。第一次迭代中，高层信息为0，之后再用HAN高层状态接进来。</p><p>这样的结构有两个优点：</p><ol><li>GGNN可以将HAN输出作为输入，考虑更长时间的context。</li><li>RNN可以自回归推理，可以补偿GGNN中缺乏的自回归机制。</li></ol><p>另外，多次迭代也一定程度上能弥补非自回归模型的缺点。</p><h1 id="Expressive-Performance-Rendering-System"><a href="#Expressive-Performance-Rendering-System" class="headerlink" title="Expressive Performance Rendering System"></a>Expressive Performance Rendering System</h1><p>下图展示了模型结构。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h2><p>模型使用预定义的乐谱和演奏作为输入和输出。特征提取方案在《 Score and performance features for rendering expressive music performances》有详述。输入feature包括各类音乐信息，比如音高、市场、速度、响度、起始偏差、清晰度、踏板等。</p><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><p>系统包括三个结构：乐谱编码器$E_s$，演奏编码器$E_p$，演奏解码器$D_p$。对于给定输入X，$E_s$推断乐谱条件$C$。模块包括两层GGNN，和一层LSTM。输入X经过三层选链接，第一层GGNN仅更新音符级别特征（固定小节级别的特征）；第二层GGNN更新整个隐状态。输出进行skip connection组成C。</p><p>根据C和对应的演奏特征Y，$E_p$编码出latent vector Z。Encoder的输入使用了全连接，对concatenated data进行了降维。编码器由GGNN和LSTM组成，具体如图。</p><p>Decoder解码器产生演奏特征Y，迭代地推断输入乐谱中每个音符的演奏参数。解码采用了分层解码的概念（《A hierarchical latent vector model for learning long-term structure in music》）。</p><p>解码器的输入是C、小节级别演奏风格向量$z_m$，和初始演奏参数的concatenatation。具体设置可以参考原论文。</p><h1 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h1><p>最近使用深度神经网络进行音乐生成的最值得注意的研究是Music Transformer（Huang et al。，2019）。该研究的目的是通过将作曲和表演结合为一个单一阶段来产生音乐。基于由self-attention组成的原始Transformer模型（Vaswani等，2017），他们提出了相对位置嵌入的音高和时间，并成功地通过超越他们以前的基于LSTM的模型生成具有长期结构的音乐作品 （Oore等，2018）。然而，如上所述，该任务与本文的范围不同。我们的研究侧重于解释和执行给定的乐谱，但Music Transformer更像是制作即兴创作。</p><p>最近，VAE已被用于若干音乐数据生成模型中，例如音乐生成（Roberts等，2018），音乐风格转移（Brunner等，2018）。<br>（Maezawa，2018）正如我们在这项工作中所做的那样，采用有condition的VAE来表现音乐表现。然而，VAE中的潜在向量是在音符级别生成的，而我们的模型使用VAE编码整个演奏，因此单个潜在向量可以表示整个片段的表现风格。<br>已经有关于使用包括神经网络的数据驱动方法自动生成表达演奏任务的研究，其在（Cancino-Chacon等人，’2018）中得到了很好的总结，但是它们实现了有限的演奏要素。<br>例如，他们只推断了速度（Malik＆Ek，2017），忽略了速度变化（Lauly，2010; Giraldo＆Ramirez，2016），假设旋律总是高音（Flossmann等，2013; Kim等 。，2013），或使用标准化的节奏（Grachten＆Cancino Chacon’，2017）。<br>我们的模型旨在实现全面的演奏要素。</p><p>将图连接应用在音乐上，生成有表现力的演奏，也有研究(Moulieras &amp; Pachet, 2016)。但是系统仅限于生成单音旋律，且图连接仅用于捕获单音上的音符特征。这个研究与我们的有基本性的不同。</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>收集了MuseScore、Musicalion（这是公开的吗？）的MusicXML数据，以及Yamaha e-competition的MIDI数据，将其做了对齐（Makamura, 2017）。</p><p>收集到了16位作曲家的227个作品，以及1061个演奏，包含3606930个音符。</p><h2 id="对比模型"><a href="#对比模型" class="headerlink" title="对比模型"></a>对比模型</h2><p>本文使用了基于HAN的VAE模型。HAN模型包括使用HAN的score encoder和演奏decoder。score encoder使用音符和语音LSTM作为字符表示，HAN作为节拍几别和小节级别的表示。Baseline省略了HAN和语音LSTM，仅使用音符级别的LSTM。仅使用GGNN替换语音和音符LSTM的修改版本被称为G-HAN。</p><p>模型对比如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h2><p>具体参见原论文。</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="重建误差（Reconstruction-Loss）"><a href="#重建误差（Reconstruction-Loss）" class="headerlink" title="重建误差（Reconstruction Loss）"></a>重建误差（Reconstruction Loss）</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="聆听测试"><a href="#聆听测试" class="headerlink" title="聆听测试"></a>聆听测试</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="8.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种迭代序列图网络，它结合了门控图神经网络和分层注意RNN，用于为给定的乐谱建模表现的钢琴演奏。定量和定性评估表明，与以前的模型相比，所提出的模型取得了显着的进步。对于未来的工作，我们将进一步研究如何定量评估表达性能模型。</p><h1 id="想法和见解"><a href="#想法和见解" class="headerlink" title="想法和见解"></a>想法和见解</h1><p>用图网络进行乐谱表示是一个很棒的主意。音乐不同于NLP其他任务的一点在于，它可以是多声部协调进行的；音乐的Piano Roll相比图像的丰富信息来说，也比较稀疏。乐谱上的音符稀疏度，用一个图来表示是相当合理的。规定图网络的连接线属性，对图拓扑结构进行定义和约束，图就能自然地表示出乐谱。</p><p>论文阐述了一个生成任务，VAE的中间结果z是随机取样的。但是从表示学习的角度来看，VAE能解耦出演奏风格，z必然有其意义。如果指定VAE中间的z，而不是随机生成，那么能否做到某种意义上的演奏风格迁移？</p><p>本文的模型设计也颇有意思。实际上，LSTM的表示是GNN的高层表示，这种表示方法是自然且高效的，如果不使用GNN，这种方法难以自然地导出。HAN和GNN的流动机制也是一个很有意思的点。</p><p>评估中，模型的性能并不突出，我个人猜测是因为音乐生成任务较困难，在其他的task上也许能有更好的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章的亮点在于利用了图网络编码额外信息，来改进其他任务效果的思想。这篇文章写作清晰，想法新颖，值得关注。文章出自韩国KAIST的Juhan Nam老师组（出身Stanford的CCRMA实验室，师承Malcolm Slaney老师）。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
      <category term="VAE" scheme="http://ldzhangyx.github.io/tags/VAE/"/>
    
      <category term="structure" scheme="http://ldzhangyx.github.io/tags/structure/"/>
    
      <category term="图网络" scheme="http://ldzhangyx.github.io/tags/%E5%9B%BE%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>音乐生成的兵器库：必要的包和数据集</title>
    <link href="http://ldzhangyx.github.io/2019/07/21/music-toolkits/"/>
    <id>http://ldzhangyx.github.io/2019/07/21/music-toolkits/</id>
    <published>2019-07-21T03:08:44.000Z</published>
    <updated>2019-08-09T02:08:25.953Z</updated>
    
    <content type="html"><![CDATA[ <a id="more"></a><p> 首先做个小宣传。因为很多想做这一块的同行本科生们不是很清楚国内外的院校列表。有一位前辈在GitHub上整理了大部分的院校列表出来，我也对这个列表做了一点贡献。<a href="https://github.com/beiciliang/intro2musictech/blob/master/INFO-ResearchGroups.md" target="_blank" rel="noopener">点击查看</a></p><p> Python官网整理的Python库有这些：<a href="https://wiki.python.org/moin/PythonInMusic" target="_blank" rel="noopener">链接</a></p><h1 id="基础组件"><a href="#基础组件" class="headerlink" title="基础组件"></a>基础组件</h1><h2 id="Python-MIDI"><a href="#Python-MIDI" class="headerlink" title="Python-MIDI"></a>Python-MIDI</h2><ul><li>项目官网：<a href="https://github.com/vishnubob/python-midi/tree/feature/python3" target="_blank" rel="noopener">https://github.com/vishnubob/python-midi/tree/feature/python3</a></li></ul><p>很多库的前置库，安装时记得选择其Python 3的branch。</p><h1 id="乐谱生成"><a href="#乐谱生成" class="headerlink" title="乐谱生成"></a>乐谱生成</h1><p> 相关的知乎问题：<a href="https://www.zhihu.com/question/23003386" target="_blank" rel="noopener">链接</a></p><h2 id="LilyPond"><a href="#LilyPond" class="headerlink" title="LilyPond"></a>LilyPond</h2><ul><li>项目官网：<a href="http://lilypond.org/index.html" target="_blank" rel="noopener">http://lilypond.org/index.html</a></li></ul><p>LilyPond是GNU Project的一部分，是免费的乐谱生成软件。生成的乐谱质量很高，支持从xml/mxl以及从midi转化成lilypond格式源代码（.ly），并且可以使用类LaTeX语言编写。</p><p>可以用两个方法通过Python代码调用LilyPond库：</p><ol><li>Python-ly库，其GitHub地址在<a href="https://github.com/wbsoft/python-ly" target="_blank" rel="noopener">这里</a></li><li>mingus的lilypond模块，GitHub地址在<a href="https://github.com/bspaans/python-mingus" target="_blank" rel="noopener">这里</a></li></ol><h2 id="MuseScore"><a href="#MuseScore" class="headerlink" title="MuseScore"></a>MuseScore</h2><ul><li>项目官网：<a href="https://musescore.org" target="_blank" rel="noopener">https://musescore.org</a></li><li>社区和乐谱集：<a href="https://musescore.org" target="_blank" rel="noopener">https://musescore.org</a></li></ul><p>MuseScore的特点在于其有充分活跃的社区。这个软件和LilyPond都可以通过XML和MIDI进行数据传送，所以在接口方面很灵活。</p><h1 id="音乐的符号化表示"><a href="#音乐的符号化表示" class="headerlink" title="音乐的符号化表示"></a>音乐的符号化表示</h1><h2 id="music21"><a href="#music21" class="headerlink" title="music21"></a>music21</h2><ul><li>项目官网：<a href="http://web.mit.edu/music21/" target="_blank" rel="noopener">http://web.mit.edu/music21/</a></li></ul><p>对Python3的支持很好，也是目前为止最完善的一个Python库、武器库。包括音乐的乐谱显示、音乐合成、矩阵分析、语料库、符号表达等。</p><h2 id="ABC格式"><a href="#ABC格式" class="headerlink" title="ABC格式"></a>ABC格式</h2><ul><li>项目官网：<a href="http://abcnotation.com/" target="_blank" rel="noopener">http://abcnotation.com/</a></li></ul><p>ABC格式是一种复杂而古早的模式，很多早期的数据集使用了这种标注。ABC格式可以转化为music21，或者被其他方法解析。</p><h2 id="Pretty-MIDI表示的Piano-Roll"><a href="#Pretty-MIDI表示的Piano-Roll" class="headerlink" title="Pretty_MIDI表示的Piano Roll"></a>Pretty_MIDI表示的Piano Roll</h2><ul><li>项目官网：<a href="https://github.com/craffel/pretty-midi" target="_blank" rel="noopener">https://github.com/craffel/pretty-midi</a></li><li>我以前写的文章：<a href="https://www.cnblogs.com/ldzhangyx/p/7789939.html" target="_blank" rel="noopener">https://www.cnblogs.com/ldzhangyx/p/7789939.html</a></li></ul><p>一个方便将MIDI文件转化为Piano Roll的库。这个库被用在Google Magenta里，我自己的项目也要使用它。master分支现在已经支持了Python3。</p><h2 id="libROSA"><a href="#libROSA" class="headerlink" title="libROSA"></a>libROSA</h2><ul><li>项目官网：<a href="https://librosa.github.io/librosa/" target="_blank" rel="noopener">https://librosa.github.io/librosa/</a></li></ul><p>libROSA库的display.specshow模块可以将piano roll可视化为一个（还可以看的）图片。</p><h2 id="MIR-eval"><a href="#MIR-eval" class="headerlink" title="MIR_eval"></a>MIR_eval</h2><ul><li>项目官网：<a href="https://craffel.github.io/mir_eval/" target="_blank" rel="noopener">https://craffel.github.io/mir_eval/</a></li></ul><p>Google的Colin Raffel写的一个库。这个Python库提供了一些和弦的向量表示，以及和弦分析的一些其他有用的工具。</p><h2 id="mingus"><a href="#mingus" class="headerlink" title="mingus"></a>mingus</h2><ul><li>项目官网：<a href="https://bspaans.github.io/python-mingus/" target="_blank" rel="noopener">https://bspaans.github.io/python-mingus/</a></li><li>Python 3版本：<a href="https://github.com/edudobay/python-mingus" target="_blank" rel="noopener">https://github.com/edudobay/python-mingus</a></li></ul><p>作者很久没有维护过了，但是从快速上手来看是一个不错的Python库。Python 3版本已经给出。</p><h1 id="音乐合成"><a href="#音乐合成" class="headerlink" title="音乐合成"></a>音乐合成</h1><h2 id="PySynth"><a href="#PySynth" class="headerlink" title="PySynth"></a>PySynth</h2><ul><li>项目官网：<a href="https://mdoege.github.io/PySynth/" target="_blank" rel="noopener">https://mdoege.github.io/PySynth/</a></li></ul><p>主要用处就是将midi或ABC格式的文件转化为wav文件，有多种合成方法可选，十分方便好用。</p><h2 id="pyfluidsynth3"><a href="#pyfluidsynth3" class="headerlink" title="pyfluidsynth3"></a>pyfluidsynth3</h2><ul><li>项目官网：<a href="https://github.com/tea2code/pyfluidsynth3" target="_blank" rel="noopener">https://github.com/tea2code/pyfluidsynth3</a></li></ul><p>Pretty-MIDI的前置库，用于合成音乐。这里推荐的是Python 3的重写版本。</p><ul><li>项目官网</li></ul><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><h2 id="Nottingham-Dataset"><a href="#Nottingham-Dataset" class="headerlink" title="Nottingham Dataset"></a>Nottingham Dataset</h2><ul><li>地址：<a href="https://github.com/jukedeck/nottingham-dataset" target="_blank" rel="noopener">https://github.com/jukedeck/nottingham-dataset</a></li></ul><p>特别有名的数据集，音乐界的MNIST。ABC格式可以被上面一些库直接解析，MIDI格式也可以转换。数据预处理的办法一般是shift到12个大调或者小调，速度调整至120bpm。</p><h2 id="Lakh-Dataset"><a href="#Lakh-Dataset" class="headerlink" title="Lakh Dataset"></a>Lakh Dataset</h2><ul><li>地址：<a href="https://colinraffel.com/projects/lmd/" target="_blank" rel="noopener">https://colinraffel.com/projects/lmd/</a></li><li>GitHub：<a href="https://github.com/craffel/midi-dataset" target="_blank" rel="noopener">https://github.com/craffel/midi-dataset</a></li></ul><p>Lakh MIDI数据集是176,581个独特MIDI文件的集合，其中45,129个已匹配并与Million Song Dataset中的条目对齐。</p><h2 id="Lakh-Pianoroll"><a href="#Lakh-Pianoroll" class="headerlink" title="Lakh Pianoroll"></a>Lakh Pianoroll</h2><ul><li>地址：<a href="https://salu133445.github.io/lakh-pianoroll-dataset/" target="_blank" rel="noopener">https://salu133445.github.io/lakh-pianoroll-dataset/</a></li></ul><p>上面数据集的转化版本，但是表示使用了Pianoroll。</p><h2 id="Million-Songs"><a href="#Million-Songs" class="headerlink" title="Million Songs"></a>Million Songs</h2><ul><li>地址：<a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener">https://labrosa.ee.columbia.edu/millionsong/</a></li></ul><p>不是midi文件而是波形文件，特点在于数据量极大。</p><h2 id="SALAMI-Dataset"><a href="#SALAMI-Dataset" class="headerlink" title="SALAMI Dataset"></a>SALAMI Dataset</h2><ul><li>地址：<a href="https://github.com/DDMAL/salami-data-public" target="_blank" rel="noopener">https://github.com/DDMAL/salami-data-public</a></li></ul><p>非常完整的标注数据集，值得一提的是可以从中提取和弦标注进行学习。</p><h2 id="MAESTRO-Piano-Dataset"><a href="#MAESTRO-Piano-Dataset" class="headerlink" title="MAESTRO Piano Dataset"></a>MAESTRO Piano Dataset</h2><ul><li>地址：<a href="https://magenta.tensorflow.org/datasets/maestro" target="_blank" rel="noopener">https://magenta.tensorflow.org/datasets/maestro</a></li></ul><p>Google Magenta一直在用的钢琴演奏数据集。有文字标注版和音乐原版，数据量极为庞大。</p><h2 id="IMSLP-Music-Library"><a href="#IMSLP-Music-Library" class="headerlink" title="IMSLP Music Library"></a>IMSLP Music Library</h2><ul><li>地址：<a href="https://imslp.org/" target="_blank" rel="noopener">https://imslp.org/</a></li></ul><p>国际上最大的乐谱典藏数据库。其亮点在于录音和乐谱非常全，也许可以用于图像识别领域。有些乐谱似乎是收费的。</p><h2 id="the-Classical-piano-MIDI-database"><a href="#the-Classical-piano-MIDI-database" class="headerlink" title="the Classical piano MIDI database"></a>the Classical piano MIDI database</h2><ul><li>地址：<a href="http://www.piano-midi.de/" target="_blank" rel="noopener">http://www.piano-midi.de/</a></li></ul><p>需要自己爬下来整理，钢琴MIDI数据集。</p><h2 id="The-Largest-MIDI-Dataset"><a href="#The-Largest-MIDI-Dataset" class="headerlink" title="The Largest MIDI Dataset"></a>The Largest MIDI Dataset</h2><ul><li>地址：<a href="https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/" target="_blank" rel="noopener">https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/</a></li></ul><p>可以说是很神奇了，在Reddit上贡献了出来。是我看到的最大的MIDI数据集（3.65Gb），没有之一。</p><h2 id="MuseData"><a href="#MuseData" class="headerlink" title="MuseData"></a>MuseData</h2><ul><li>地址：<a href="http://musedata.stanford.edu/" target="_blank" rel="noopener">http://musedata.stanford.edu/</a></li></ul><p>也是需要自己爬，但是这里的数据是根据流派做好划分的，非常一目了然。</p><h2 id="JSB-Chorales"><a href="#JSB-Chorales" class="headerlink" title="JSB Chorales"></a>JSB Chorales</h2><ul><li>地址：<a href="https://github.com/czhuang/JSB-Chorales-dataset" target="_blank" rel="noopener">https://github.com/czhuang/JSB-Chorales-dataset</a></li></ul><p>四声部众赞歌数据集。源地址挂掉了，这个GitHub提供了pickle包，很好用。</p><h2 id="FMA-Dataset"><a href="#FMA-Dataset" class="headerlink" title="FMA Dataset"></a>FMA Dataset</h2><ul><li>地址：<a href="https://github.com/mdeff/fma" target="_blank" rel="noopener">https://github.com/mdeff/fma</a></li></ul><p>这是一个音乐分析用的数据集，其亮点在于对流派、ID等数据的详尽标注。具体可以阅读readme。</p><h2 id="Midi-Archive"><a href="#Midi-Archive" class="headerlink" title="Midi Archive"></a>Midi Archive</h2><ul><li>地址：<a href="http://archive.cs.uu.nl/pub/MIDI/" target="_blank" rel="noopener">http://archive.cs.uu.nl/pub/MIDI/</a></li></ul><p>罗切斯特大学维护的一个数据集。</p><h2 id="The-Session"><a href="#The-Session" class="headerlink" title="The Session"></a>The Session</h2><ul><li>地址：<a href="https://thesession.org/tunes" target="_blank" rel="noopener">https://thesession.org/tunes</a></li></ul><p>非常非常完备的一个ABC格式的数据集，虽然需要自己爬，但是非常齐全。数据仍在不断增加中。</p><h2 id="MusicNet"><a href="#MusicNet" class="headerlink" title="MusicNet"></a>MusicNet</h2><ul><li>地址：<a href="https://homes.cs.washington.edu/~thickstn/start.html" target="_blank" rel="noopener">https://homes.cs.washington.edu/~thickstn/start.html</a></li></ul><p>这个数据集是raw的音频格式，大约10GB，提供了丰富的形式，标注详尽全面，甚至还做了PyTorch接口。</p><h2 id="the-Symbolic-Music-dataset-by-Walder"><a href="#the-Symbolic-Music-dataset-by-Walder" class="headerlink" title="the Symbolic Music dataset by Walder"></a>the Symbolic Music dataset by Walder</h2><ul><li>地址：<a href="http://users.cecs.anu.edu.au/~u1018264/data.html" target="_blank" rel="noopener">http://users.cecs.anu.edu.au/~u1018264/data.html</a></li></ul><p>清洗过的MIDI文件，作者目前还活跃在音乐研究一线。</p><h2 id="TheoryTab"><a href="#TheoryTab" class="headerlink" title="TheoryTab"></a>TheoryTab</h2><ul><li>地址：<a href="https://www.hooktheory.com/theorytab" target="_blank" rel="noopener">https://www.hooktheory.com/theorytab</a></li></ul><p>严格来说这不算是一个数据集，但是很多人在论文中用过它（比如MidiNet）。最出彩的地方在于其和弦标注。我觉得应该是网络上和弦标注最为全面的一个数据集了。这个网站现代化、可交互，非常值得看一看。</p><h2 id="J-S-Bach-Series"><a href="#J-S-Bach-Series" class="headerlink" title="J.S. Bach Series"></a>J.S. Bach Series</h2><ul><li>地址：<a href="http://www.jsbach.net/midi/" target="_blank" rel="noopener">http://www.jsbach.net/midi/</a></li><li>地址：<a href="http://bachcentral.com/" target="_blank" rel="noopener">http://bachcentral.com/</a></li></ul><p>emm，他们真的很喜欢巴赫。</p><h2 id="The-Josquin-Research-Project"><a href="#The-Josquin-Research-Project" class="headerlink" title="The Josquin Research Project"></a>The Josquin Research Project</h2><ul><li>地址：<a href="http://josquin.stanford.edu/" target="_blank" rel="noopener">http://josquin.stanford.edu/</a></li></ul><p>斯坦福的一个网站，好像也不能直接下载，但是数据整理得非常漂亮，提供了MIDI、XML等多种格式，收录了1420-1520年的复调音乐，并且支持在线搜索和试听。</p><h2 id="Drum-Dataset"><a href="#Drum-Dataset" class="headerlink" title="Drum Dataset"></a>Drum Dataset</h2><ul><li>地址：<a href="https://www.reddit.com/r/WeAreTheMusicMakers/comments/3anwu8/the_drum_percussion_midi_archive_800k/" target="_blank" rel="noopener">https://www.reddit.com/r/WeAreTheMusicMakers/comments/3anwu8/the_drum_percussion_midi_archive_800k/</a></li></ul><p>虽然数据很小只有800k，但是这是我见过的第一个专用于鼓点的数据集。</p><h2 id="Video-Game-Datasets"><a href="#Video-Game-Datasets" class="headerlink" title="Video Game Datasets"></a>Video Game Datasets</h2><ul><li>地址：<a href="https://www.vgmusic.com/" target="_blank" rel="noopener">https://www.vgmusic.com/</a></li></ul><p>收录了游戏的背景音乐，而且真的非常非常齐全。</p><h2 id="Essen-Folk-Songs-Database"><a href="#Essen-Folk-Songs-Database" class="headerlink" title="Essen Folk Songs Database"></a>Essen Folk Songs Database</h2><ul><li>地址：<a href="http://www.esac-data.org/" target="_blank" rel="noopener">http://www.esac-data.org/</a></li></ul><p>古老的数据集，用于音乐结构分析。数据的提供方式是简谱。这个网站似乎有反爬机制，注意下载方式。</p><h2 id="Billboard-Dataset"><a href="#Billboard-Dataset" class="headerlink" title="Billboard Dataset"></a>Billboard Dataset</h2><ul><li>地址：<a href="https://ddmal.music.mcgill.ca/research/The_McGill_Billboard_Project_(Chord_Analysis_Dataset)/" target="_blank" rel="noopener">https://ddmal.music.mcgill.ca/research/The_McGill_Billboard_Project_(Chord_Analysis_Dataset)/</a></li></ul><p>用作和弦分析的数据集，由McGill的DDMAL组提供。数据集提供了约900首音乐的和弦注释，但是tonic之间没有区分大小调。</p><h2 id="HookTheory"><a href="#HookTheory" class="headerlink" title="HookTheory"></a>HookTheory</h2><ul><li>地址：<a href="https://www.hooktheory.com/" target="_blank" rel="noopener">https://www.hooktheory.com/</a></li></ul><p>用作和弦分析的数据集。可惜的是，这个数据集似乎不提供公开下载。</p>]]></content>
    
    <summary type="html">
    
      
      
         &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt; 首先做个小宣传。因为很多想做这一块的同行本科生们不是很清楚国内外的院校列表。有一位前辈在GitHub上整理了大部分的院校列表出来，我也对这个列表做了一点贡献。&lt;a href=&quot;https://github.com/beiciliang
      
    
    </summary>
    
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
      <category term="Python" scheme="http://ldzhangyx.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>abc-midi</title>
    <link href="http://ldzhangyx.github.io/2019/07/14/abc-midi/"/>
    <id>http://ldzhangyx.github.io/2019/07/14/abc-midi/</id>
    <published>2019-07-14T09:38:09.000Z</published>
    <updated>2019-07-14T09:38:09.522Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SALAMI数据集介绍和使用指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/12/salami-dataset/"/>
    <id>http://ldzhangyx.github.io/2019/07/12/salami-dataset/</id>
    <published>2019-07-12T05:10:47.000Z</published>
    <updated>2019-07-13T06:00:35.318Z</updated>
    
    <content type="html"><![CDATA[<p>SALAMI数据集，最早被我注意到是在2018年的ISMIR上的《INTERACTIVE ARRANGEMENT OF CHORDS AND MELODIES BASED ON A TREE-STRUCTURED GENERATIVE MODEL》，里面使用了来自McGill University的SALAMI数据集。</p><p>这个数据集的特点是，人工标注了音乐的层次结构。从规模上来说，涵盖了一千余首音乐，足以充当模型的测试集，用于检查模型对音乐层级结构的推导准确性。</p><p>这篇文章主要是数据集公开时发表的论文《DESIGN AND CREATION OF A LARGE-SCALE DATABASE OF STRUCTURAL ANNOTATIONS》的全文翻译，附带了我使用这个数据集的一些心得体会。</p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文描述了一个前所未有的大型数据库的设计和创建，该数据库包含近1400种音乐录音的2400多种结构注释。该数据库旨在成为算法的测试集，用于分析数十万个记录的更大的语料库，作为大量音乐信息结构分析（SALAMI）项目的一部分。本文描述了数据库的设计目标以及在创建过程中遇到的实际问题。</p><p>特别是，我们讨论了录音的选择，注释格式的开发和适应Peeters和Deruty [10]的工作的程序，以及项目的管理和执行。我们还总结了所得到的注释语料库的一些属性，包括平均了标注者之间的一致。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>大量音乐信息（SALAMI）项目的结构分析是一项音乐学活动，其目标是为大量音乐制作结构分析 - 超过300,000个录音。这里的结构是指将一段音乐划分为多个部分，并将相似或重复的部分组合在一起。这些部分通常对应于功能独立的部分，例如流行歌曲的“诗歌（verse）”和“合唱（chorus）”部分，奏鸣曲的“呈示部（exposition）”和“展开部（development）” - 或者在更短的时间尺度，呈示部的“主题”，“转型”和次要主题”群体。</p><p>SALAMI语料库中的录音代表了各种类型，从klezmer到40强流行音乐，以及各种来源，包括专业录音室录音和观众录制的现场会议。SALAMI数据集将免费提供，可以为音乐理论家，音乐学家和其他音乐研究人员提供很好的服务，因为确定单个音乐片段的形式通常是一项耗时的任务。SALAMI数据集可以促进形式的大规模研究，目前这种研究相对不常见。</p><p>由于了解音乐片段结构的价值，追求自动生成结构描述的算法是一个活跃的研究领域。（有关评论，请参阅[9]。）SALAMI项目计划使用这些算法中的一些来分析其数十万个记录。但是，在使用这些算法之前，有必要在所代表的大量类型上验证它们的性能。这需要创建一个人类注释的地面实况数据集。设计和创建大型数据库（如SALAMI测试集）引发了许多与音乐选择，注释格式和过程相关的方法问题。本文解释了所涉及的问题以及我们为解决这些问题所做的决定。</p><p>本文的下一部分总结了几个现有结构注释语料库的内容和贡献，以及最近对注释过程本身的重要研究[1,10]。第3节描述了SALAMI测试集的创建，包括语料库选择，使用的注释格式和推荐的工作流程。第4节介绍并讨论了结果数据集的一些属性。</p><h1 id="之前的工作"><a href="#之前的工作" class="headerlink" title="之前的工作"></a>之前的工作</h1><h2 id="既有的数据"><a href="#既有的数据" class="headerlink" title="既有的数据"></a>既有的数据</h2><p>SALAMI需要一个包含大量流行音乐，爵士乐，古典音乐和世界音乐的数据库。然而，大多数以前的注释集合只考虑流行音乐。现有最大的三个注释数据库是TUTstructure07 [13]（557个注释），由坦佩尔理工大学（TUT）编制，主要包含流行音乐;由Alan Pollack创建并由两组独立同步的Beat les工作室目录的注释[5,14]（180注释）;和RWC音乐数据库（285注释）附带的AIST注释集[4]。RWC套装大约是一半流行音乐，每个爵士乐和古典音乐四分之一，还有少数世界音乐作品，但对于许多爵士乐和古典乐曲，只有“合唱”部分。</p><h2 id="注释格式"><a href="#注释格式" class="headerlink" title="注释格式"></a>注释格式</h2><p>几乎所有以前的注释语料库都​​使用了相同的直接注释格式。片段被分割成非重叠的片段，并且每个片段被给予单个标签，例如“前奏”或“合唱”，以指示哪些片段彼此相似或重复。标签还表明每个部分的音乐角色或功能。在一些语料库中，例如披头士乐队的注释[5]，标签可以指示乐器（例如，“verse_guitar”）或乐段的变体（例如，“verse_with_ending”）。</p><h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>正如Peeters和Deruty [10]所指出的，音乐相似性，功能和仪器的这种混合是有问题的。例如，歌曲的“outro”可以使用与之前的“transition”相同的音乐，但是将它们标记为未能记录它们的相似性。相反，具有单一功能的部分可能在音乐上是异质的，如同延伸的两部分介绍一样。Peeters和Deruty还批评了在某些注释集合中使用的大型，看似无约束的词汇表。再次考虑Isophonics Beatles注释[5]：在146个独特标签中，95种只使用一次。一次性标签可以为检查注释的人提供信息，其中其含义在上下文中是可理解的（例如，“intro<em>redux”，“verse</em>（slow）”），但是当注释正在进行时，具有太多唯一标签的用处不大 由机器使用。标准注释格式的另一个缺点是它仅描述单个时间尺度的结构。音乐结构最重要的属性之一是它被层次感知，并且理想的是在注释中捕获一些这样的信息。</p><h3 id="一种可替代的格式"><a href="#一种可替代的格式" class="headerlink" title="一种可替代的格式"></a>一种可替代的格式</h3><p>Peeters和Deruty提出了另一种注释格式，旨在解决这些问题。该格式使用了19个标签的受限词汇表，每个标签都涉及一个部分结构的三个方面之一：音乐相似性，音乐角色或乐器角色。在它们的格式中，通过将一个片段的每个部分标记为五个“组成型实体循环”（CSLoops）之一来指示音乐相似性。（如果需要超过五个，则使用第六个CSLoop，但格式并不意味着标有此最后一个标签的所有部分都相似。）功能标签是可选的，仅限于“intro / outro”，“transition”， “chrous”和“solo”。乐器标签表示是否存在主要或支持旋律的声音。Peeters和Deruty的格式也创造性地包含了一些有关结构的分层信息。两个标记“V1”和“V2”划分CSLoops; 第一个表示标记两侧的音乐片段相似，第二个表示它们不相似。</p><h2 id="标注过程"><a href="#标注过程" class="headerlink" title="标注过程"></a>标注过程</h2><p>与音调不同，在很大程度上，节拍，结构的感知是一种高度主观的现象，并且两个听众通常不同意音乐的形式。因此，开发一种注释程序是一项挑战，虽然可能不是客观的，但最大限度地提高了结果的可重复性。请注意，由于结构分析记录了听众的创造性解释和她的感知，因此客观性可以说是注释的不可能的目标。</p><p>一种方法是将注释的创建视为感知实验，并且只要让多个主体听到一个片段并且只要他们感知到结构边界就按下按钮。这些数据是由[2]收集的，他们指出，听众普遍同意他们认为最突出的边界的位置。这些边界被作者用作一种“基础事实”，用于评估一些计算模型在估计边界时的成功。</p><p>Bimbot等[1]通过精确指定注释程序设法获得一定程度的可重复性。他们定义了注释器应该使用的音乐标准和相似性判断，以便估计边界。（标记细分的任务仍然是未来的工作。）他们报告说，通过他们的程序，注释在注释者和随着时间的推移是非常一致的。注释器的目标是将一个片段分解为“自主和可比较的块”。自主意味着无论一个块是独立还是连续循环，结果应该是音乐上可接受的。如果两个块在节拍中具有相同的持续时间，可互换，或者就其时间组织而言是相似的，则它们可以是可比较的。</p><h1 id="SALAMI数据集的描述"><a href="#SALAMI数据集的描述" class="headerlink" title="SALAMI数据集的描述"></a>SALAMI数据集的描述</h1><p>我们使用独特的注释格式开发了一个新的注释语料库，以实现SALAMI项目的目标。为确保语料库可用作SALAMI的评估测试集，主要的设计考虑因素是语料库涵盖尽可能广泛的音乐类型。为了使注释在音乐上有用，注释格式的设计目标是独立描述音乐相似性，功能和主要乐器，并且注释反映音乐结构的等级性质。最后，格式和程序应该允许快速制作注释，以最大限度地降低成本，但要足够灵活，以便能够处理各种类型的作品，同时目标着标注者之间的一致。考虑到这些设计考虑因素，我们对以前的注释语料库和现有注释技术进行了调查。基于此调查以及我们自己对不同方法的实验，我们确定了本节中概述的语料库，格式和过程。</p><h2 id="SALAMI数据集的内容"><a href="#SALAMI数据集的内容" class="headerlink" title="SALAMI数据集的内容"></a>SALAMI数据集的内容</h2><p>设计语料库的第一步是决定放入什么。SALAMI的优先事项之一是为尽可能多种音乐提供结构分析，以匹配算法分析的音乐的多样性。除了流行音乐之外，SALAMI测试装置应该同等重视古典，爵士和非西方音乐，俗称“世界”音乐。为确保录音格式的多样性，我们还强调了录制现场录音。数据库的最终组成如表1所示。</p><p>SALAMI测试集的第二个目标是能够将我们的注释与以前的数据集进行比较。因此，我们重复了之前的一些工作：我们的测试集目前分别包括来自RWC和Isophonics数据集的97和35个记录。注意，这些记录都是单键的（即，由单个人注释），而大多数SALAMI测试语料库是双键的（由两个独立的注释器分析）。双键提供有用的信息，但更昂贵。鉴于其他组已经注释了这些条目，单键输入一些条目似乎是一个合理的妥协。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>手动选择语料库的歌曲会耗费时间，并会引入未知的方法论偏见。然而，从大多数来源随机选择歌曲会导致语料库严重偏向流行音乐。为了解决这个问题，大多数录音都来自Codaich [7]，这是一个包含精心策划的元数据的大型数据库，包括50多个子标签。这使我们能够在仍然随机选择单个作品的同时强制实现对流派的良好报道。测试集的其余部分是从Live Music Archive [6]中随机收集的。遗憾的是，这些录音的元数据不一致，无法按类型分发。大多数似乎是流行和爵士音乐。</p><h2 id="标注格式"><a href="#标注格式" class="headerlink" title="标注格式"></a>标注格式</h2><p>我们开发了一种新的注释格式，它采用Peeters和Deruty设计的格式，采用了许多重要的方式：我们借用它们之间的区别来区分表示音乐相似性，功能和仪器的标签，并且像我们一样，我们也严格限制功能标签的词汇量。但是，我们已经做了一些修改，以适应SALAMI的独特需求和更多的音乐学焦点。三层中的每一层中的标签在以下三个部分中描述。示例注释如图1所示。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="音乐相似性轨"><a href="#音乐相似性轨" class="headerlink" title="音乐相似性轨"></a>音乐相似性轨</h3><p>音乐相似性轨迹包括在不同时间尺度的两个层，每个时间尺度识别该部分的哪些部分使用类似的音乐想法。大型图层使用大写字母作为标签（“A”，“B”等），小型图层使用小写字母（“a”，“b”等）。字母标签的使用模仿了熟悉的音乐理论方法。必须为大型和小型图层中的每个记录部分分配一个字母标签。格式规范允许使用任意数量的小写或大写字母（如果字母表用尽，可以使用标签“aa”，“ab”等等）。然而，对于大规模层，指示注释器优选每次记录使用五个或更少的不同大写标签。该偏好规则并未表示在任何记录中存在五个或更少的不同音乐想法的假设。相反，它旨在引导注释器朝向某个抽象级别。当注释不太清晰地组织成不同部分的作品时，这个方向被证明是有用的，例如通过组合的部分。它也有助于注释作品，如奏鸣曲，可以组织成部分，但这些部分不是音乐上同质的，可能包括几个不同的音乐想法。</p><p>另外两个特殊标签表示沉默（“slience”）和非音乐，例如现场记录中的掌声或戏弄（“Z”）。我们还允许字母标记被素数符号（’）所反映，以表示与另一个明显相似的部分，但这被认为是实质上变化的。相似性判断本质上是主观的和不精确的，而主要符号是承认这一点的有用方式。它允许注释者忠实地记录他的解释，同时允许未来的用户根据他们的需要轻松地调整标签。例如，根据应用程序，用户可以删除主要标记（以便“a”和“a’”被重新标记为“a”）或将变体视为不同的部分（以便“a’”将是重新分配了与“a”不同的字母标签）。</p><h3 id="功能轨"><a href="#功能轨" class="headerlink" title="功能轨"></a>功能轨</h3><p>注释格式中的第二个轨道包含音乐功能标签，所有标签都必须从我们严格的20个标签词汇中提取。与字母标签不同，一件作品的每个部分都不一定要收到功能标签。词汇表列在表2中，分为各种相关类别。乐器，过渡和结束组都是同义词组。请注意，在结束组中，标签“fadeout”是除了任何其他标签之外还可以出现的特殊标签。例如，如果乐曲在重复合唱时淡出，则最后一部分可以同时给出两个标签：“合唱”和“淡出”。每个术语的完整定义在我们的注释器指南中指定，可在线获取[11]。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>请注意，某些标签是其他类型的特定类型的替代品：例如，爵士乐歌曲中的“head”类似于流行歌曲中的“chorus”，或者有时候是古典乐曲中的“主题”。此外，术语“exposition”，“development”和“recapitulation”一起特定于奏鸣曲形式，并且在特殊情况下可用于在大于大规模相似性标签的时间尺度上注释第三级结构关系。然而，“development”也具有更广泛的适用性：它可以用于表示对比中间部分的功能，这在许多情况下是相关的，从各种古典类型到渐进式摇滚。另外，词汇表的一些子集可以用作同义词组，如果需要，可以将其折叠成单个功能标签。例如，虽然我们的Annotator’s Guide定义了“pre-chorus,” “pre-verse,” “interlude,”和 “transition” 部分之间相对微妙的区别，但它们都是“ “transition” 的同义词。这种方法允许注释者犯错在精确度方面，同时使未来的数据用户可以忽略不需要的区别。</p><h3 id="领头乐器轨"><a href="#领头乐器轨" class="headerlink" title="领头乐器轨"></a>领头乐器轨</h3><p>注释格式中的最后一个轨道指示单个乐器或声音在哪个位置具有领先的，通常是旋律的角色。此轨道中的标签只是主要乐器的名称，因此词汇不受限制。此外，与其他轨道不同，主要乐器标签可能会重叠，如二重唱。请注意，与功能轨道一样，如果没有仪器满足主导作用，则可能存在没有主要仪器标签的记录部分。<br>请注意，在为此项目设计的书面格式中，描述小规模相似性片段的边界是注释函数和引导仪器轨迹时唯一可用的边界。同样，这有助于将注释器定位到适当的抽象级别，并减轻它们过于辛苦地指示仪器的变化。</p><h2 id="标注过程-1"><a href="#标注过程-1" class="headerlink" title="标注过程"></a>标注过程</h2><p>注释器使用软件Sonic Visualiser [3]来试听和注释碎片。Sonic Visualiser的键盘命令允许用户很快插入和标记边界。我们提出了以下工作流程：首先，每当感知到结构边界时，通过歌曲聆听并标记边界。其次，再次听取这个部分，调整边界并添加小写标签。第三，添加大写和功能标签，最后添加主要仪器标签。虽然我们发现此工作流程高效且简单，但我们并未要求注释者遵循此工作流程或任何其他特定工作流程。</p><h2 id="项目实现"><a href="#项目实现" class="headerlink" title="项目实现"></a>项目实现</h2><p>注释格式和数据收集在10个月的时间内进行。首先，研究了以前的注释格式和注释数据库。潜在的注释格式由项目负责人设计和测试，并在两个月结束时设定了一种暂定格式。接下来，候选注释器在注释格式和Sonic Visualiser环境中进行训练。聘请了八位成功的候选人，他们都在音乐理论或作曲中继续攻读研究生课程，并在接下来的一周开始收集数据。由于注释格式在工作开始之前尚未进行大规模测试，因此数据收集的前六周被视为延长试用期。每个星期或两个星期，新的流派都会给注释者一个新的作业，从流行开始，558第12届国际音乐信息检索学会（ISMIR 2011）预计将成为问题最少的，并继续按顺序进行爵士乐 ，古典和世界，预计难度越来越大。在六周结束时，放松了对注释者的监督，并且临时解决了任何问题。在接下来的12周内继续收集数据，此时大多数任务已经完成。我们收集了自我报告的时间，用于生成每个注释以评估生产率。时间被绘制为图2中前1700个注释的日期的函数。可以看出，在项目开始时忽略了一些异常值，注释时间从20分钟的模式中适度减少。前100天，其余时间为15分钟的模式，足以让3首完全听完普通歌曲，长度为4:21。平均注释时间也从21分钟下降到17分钟。早期的分析显示歌曲的长度与其注释时间之间略有相关性。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="标注格式和过程修改"><a href="#标注格式和过程修改" class="headerlink" title="标注格式和过程修改"></a>标注格式和过程修改</h3><p>在每次新任务之后，我们征求注释者的反馈意见，了解注释格式和程序中的弱点或含糊之处。大多数问题在定期小组会议上得到解决和解决，我们也在那里计划并商定词汇量。<br>反馈导致引入新的启发式方法（例如，我们建立了一种偏好，即使在拾取器存在的情况下，分段边界也会下降。在一个案例中，反馈导致了格式的重大修订。我们最初使用[10]描述的“V1”和“V2”标记在较短的时间尺度上隐含地编码音乐相似性。<br>然而，注释者发现在两个时间尺度上明确地描述结构在概念上更简单和更快。切换器对注释器感到满意，随后的注释也有更多信息。</p><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>在本节中，我们将报告所收集数据的某些属性，包括标注人员之间的一致协议。对于大规模分析，每个注释的平均段数为11.3，其中一半的分析具有8到14个段。对于小规模分析，这些数字为38.4，在20和49之间。平均而言，每个注释有4.0个独特的大型标签和7.2个独特的小型标签。从用于比较两个注释（在[12]中定义的）的各种度量中，我们估计了成对f-measure，边界f-measure和Rand指数。通过观察一组边界与另一组边界匹配的精度和召回来找到边界f-度量。如果边界位于彼此的某个容差窗口（0.5或3秒）内，则边界匹配。成对fmeasure将一个描述中具有相同标签的所有帧对视为一组相似关系，其他描述以一定的精度和回忆检索。Rand索引是类似的，除了它还标识在一个描述中具有不同标签的帧对在另一个中具有不同的标签。表3中报告了974对注释之间的协议。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>每个注释都描述了两个细节层次的音乐相似度，两者都应被视为有效描述。为了比较两个注释，我们可以仅比较大尺度标签或仅比较小尺度标签，但我们也可以找到所有对的相似性（包括从小到大和从大到小）并取最大值 相似性来估计标注者之间的一致性。这将使我们能够识别注释者关注不同时间尺度的情况。如表3所示，大规模标签之间的一致性（成对f = 0.76，Rand= 0.79）与小规模标签之间的一致（成对f = 0.69，Rand= 0.81），并且找到的平均最佳匹配是 略高于每一个（成对f = 0.81，Rand= 0.87）。为了比较，[8]报告了来自TUT集的30首歌曲的测试集上的成对f为0.89，并且[1] 0 25 50 75 100 0 100 200 300 400注释时间，以分钟为单位在项目开始日期之后完成的时间。在20首歌曲的测试集上的边界f度为0.91（使用0.75秒阈值）。该协议未被发现在很大程度上取决于该类型。这是合理的，因为这里考虑的每个广泛的类型都非常多样化，包含一些简单和复杂的部分。例如，流行的流派包括直截了当的流行音乐和更难以注释的渐进摇滚; 同样，虽然很多世界音乐对注释者构成了挑战，但像克莱兹默和凯尔特音乐这样的子类可以在结构上直截了当。我们在RWC数据集中复制了97个记录的注释。RWC注释通过向函数标签添加字母（例如，“诗歌A”，“诗歌B”等）来区分相似且相同的部分重复。我们创建了两个版本的RWC标签，一个保留，一个忽略其他字母标签。将这些与大规模和小规模的SALAMI注释进行比较，显示出适度的一致性（见表4）。除Rand指数外，结果表明，大规模SALAMI分析与RWC注释比小规模分析更相似。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>SALAMI测试集有超过2400个注释，描述了近1400种音乐的正式结构，来自各种类型，包括流行音乐，爵士音乐，古典音乐和世界音乐。该组可用于未来的各种研究：例如，关于音乐的表面特征与音乐形式的感知之间的关系，或者正式风格与诸如艺术家，流派和原产地之类的音乐参数之间的关联。很快我们的网站就可以获得测试数据和数十万计算结构描述[11]。<br>虽然语料库的价值最终将取决于研究人员对其的使用，但SALAMI测试集中信息的数量和丰富程度应该使其对音乐学家和音乐信息检索研究人员都具有吸引力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SALAMI数据集，最早被我注意到是在2018年的ISMIR上的《INTERACTIVE ARRANGEMENT OF CHORDS AND MELODIES BASED ON A TREE-STRUCTURED GENERATIVE MODEL》，里面使用了来自McGil
      
    
    </summary>
    
      <category term="数据集" scheme="http://ldzhangyx.github.io/categories/%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="dataset" scheme="http://ldzhangyx.github.io/tags/dataset/"/>
    
      <category term="data" scheme="http://ldzhangyx.github.io/tags/data/"/>
    
  </entry>
  
  <entry>
    <title>SLURM快速上手使用指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/08/slurm-quickstart/"/>
    <id>http://ldzhangyx.github.io/2019/07/08/slurm-quickstart/</id>
    <published>2019-07-08T07:53:23.000Z</published>
    <updated>2019-07-08T11:12:53.815Z</updated>
    
    <content type="html"><![CDATA[<p>学校的HPC计算资源通过SLURM进行管理。为了方便我进行配置，我决定整理一个快速上手指南。</p><h1 id="连接服务器"><a href="#连接服务器" class="headerlink" title="连接服务器"></a>连接服务器</h1><p>推荐使用MobaXTerm进行连接。MobaXTerm不仅可以进行连接，对目录还做了可视化处理，因此可以对文件进行方便的操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh &lt;YourID&gt;@hpc.xxxxx.edu</span><br></pre></td></tr></table></figure><p>输入密码后成功连接服务器。</p><h1 id="几个常用命令"><a href="#几个常用命令" class="headerlink" title="几个常用命令"></a>几个常用命令</h1><h2 id="module"><a href="#module" class="headerlink" title="module"></a>module</h2><p>SLURM上一般有一些预先配置好的库可以使用。</p><ul><li><code>module list</code>: 查看目前存在的模块</li><li><code>module avail</code>: 查看可以安装的模块</li><li><code>module purge</code>: 移除当前所有模块</li><li><code>module load XXX</code>: 加载XXX模块</li></ul><p>模块的加载工作一般直接写在下文提到的脚本里。我自己常用的模块一般就是anaconda/5.2.0和各个版本的cuda，cudnn对环境进行配置。</p><h2 id="sbatch"><a href="#sbatch" class="headerlink" title="sbatch"></a>sbatch</h2><p>sbatch有两种用法。你可以在sbatch命令后直接显式指定你的job要求，比如索取1个CPU，使用4小时，也可以将配置写在脚本文件里（推荐）。格式直接就是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbatch xxx.sh</span><br></pre></td></tr></table></figure><p>之后会收到服务器返回的job id。</p><h2 id="scontrol"><a href="#scontrol" class="headerlink" title="scontrol"></a>scontrol</h2><p>可以使用scontrol查看job情况。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scontrol show &lt;job id&gt;</span><br></pre></td></tr></table></figure><p>会返回这个job的所有状态。</p><h2 id="sview"><a href="#sview" class="headerlink" title="sview"></a>sview</h2><p>输入这个命令可以打开一个新窗口，监控集群结点上所有任务的情况。</p><h1 id="配置重点"><a href="#配置重点" class="headerlink" title="配置重点"></a>配置重点</h1><p>运行和提交一个新任务，只需要做两件事：</p><ol><li>准备并上传代码文件</li><li>编写并运行sh脚本</li></ol><p>下面分两种情况进行讨论。</p><h2 id="运行任务（py文件）"><a href="#运行任务（py文件）" class="headerlink" title="运行任务（py文件）"></a>运行任务（py文件）</h2><p>这种情况意味着你只想得到结果，观察输出。</p><ol><li>上传代码文件。</li><li>编写sh脚本。sh脚本的基本格式如下：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#SBATCH --mail-user=</span><br><span class="line">#SBATCH --mail-type=ALL</span><br><span class="line">#SBATCH --nodes=1</span><br><span class="line">#SBATCH --ntasks-per-node=1</span><br><span class="line">#SBATCH --time=1-01:10:00</span><br><span class="line">#SBATCH --mem=200G</span><br><span class="line">#SBATCH --output=Job.%j.out</span><br><span class="line">#SBATCH --error=Job.%j.err</span><br><span class="line">#SBATCH --account=testuser</span><br><span class="line">#SBATCH --partition=debug</span><br></pre></td></tr></table></figure><p>文件中的配置部分。—mail-type=<type>代表指定状态发生时，发送邮件通知，有效种类为（NONE, BEGIN, END, FAIL, REQUEUE, ALL）；—mail-user=<user>代表发送给对应邮箱。更多的参数我没有仔细研究，读者可以因地制宜进行控制。</user></type></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">module purge</span><br><span class="line">module load anaconda3/5.2.0</span><br><span class="line">python &lt; xxx.py</span><br></pre></td></tr></table></figure><p>加载模块，运行脚本。</p><h2 id="运行任务（Jupyter-Notebook）"><a href="#运行任务（Jupyter-Notebook）" class="headerlink" title="运行任务（Jupyter Notebook）"></a>运行任务（Jupyter Notebook）</h2><p>这种情况比较复杂，因为你需要保持一个状态，进行操作。</p><ol><li>上传代码文件（或者不传，之后新建notebook）。</li><li>编写sh脚本。sh脚本的格式如下：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#SBATCH -p serial</span><br><span class="line">#SBATCH -n1</span><br><span class="line">#SBATCH -N1</span><br><span class="line">#SBATCH -t 01:00:00</span><br><span class="line">#SBATCH --mem=4GB</span><br><span class="line">#SBATCH --job-name jupyter</span><br><span class="line">#SBATCH --output jupyter-log-%J.txt</span><br></pre></td></tr></table></figure><p>上面对各个资源进行了配置。n代表任务数量。#SBATCH指令的意义可以查看参考链接5，以及官方文档。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">module purge</span><br><span class="line">module load anaconda3/5.2.0</span><br></pre></td></tr></table></figure><p>对必要的模块进行加载。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">XDG_RUNTIME_DIR=&quot;&quot;</span><br><span class="line">ipnport=$(shuf -i8000-9999 -n1)</span><br><span class="line">ipnip=$(hostname -i)</span><br><span class="line"></span><br><span class="line">echo -e &quot;\n&quot;</span><br><span class="line">echo    &quot;  Paste ssh command in a terminal on local host (i.e., laptop)&quot;</span><br><span class="line">echo    &quot;  ------------------------------------------------------------&quot;</span><br><span class="line">echo -e &quot;  ssh -N -L $ipnport:$ipnip:$ipnport $USER@hpc.shanghai.nyu.edu\n&quot;</span><br><span class="line">echo    &quot;  Open this address in a browser on local host; see token below&quot;</span><br><span class="line">echo    &quot;  ------------------------------------------------------------&quot;</span><br><span class="line">echo -e &quot;  localhost:$ipnport</span><br></pre></td></tr></table></figure><p>输出提示信息，方便之后进行通道搭建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter-notebook --no-browser --port=$ipnport --ip=$ipnip</span><br></pre></td></tr></table></figure><p>运行jupyter notebook。</p><ol><li>运行之后，新开一个local terminal，输入以下命令，为本地localhost与服务器之间建立一个隧道。</li></ol><p>我们假设第二步的输出提示是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Paste ssh command in a terminal on local host (i.e., laptop)</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">ssh -N -L 8896:xx.xxx.xx.xx:8896 &lt;yourID&gt;@hpc.xxx.edu</span><br><span class="line"></span><br><span class="line">Open this address in a browser on local host; see token below</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">localhost:8896</span><br></pre></td></tr></table></figure><p>那么直接运行指令，建立隧道：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -L 8896:xx.xxx.xx.xx:8896 &lt;yourID&gt;@hpc.xxx.edu</span><br></pre></td></tr></table></figure><ol><li>在本地浏览器上进入下面网址，输入token，开始工作。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:8896</span><br></pre></td></tr></table></figure><h1 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h1><ol><li><a href="https://github.com/PDC-support/jupyter-notebook/blob/master/0-setup.ipynb" target="_blank" rel="noopener">https://github.com/PDC-support/jupyter-notebook/blob/master/0-setup.ipynb</a></li><li><a href="https://www.youtube.com/watch?v=U42qlYkzP9k&amp;feature=player_embedded" target="_blank" rel="noopener">https://www.youtube.com/watch?v=U42qlYkzP9k&amp;feature=player_embedded</a></li><li><a href="https://github.com/michael-qi/nyush_hpc" target="_blank" rel="noopener">https://github.com/michael-qi/nyush_hpc</a></li><li><a href="https://wikis.nyu.edu/pages/viewpage.action?spaceKey=NYUShanghaiITS&amp;title=NYU+Shanghai+HPC+cluster+Environment" target="_blank" rel="noopener">https://wikis.nyu.edu/pages/viewpage.action?spaceKey=NYUShanghaiITS&amp;title=NYU+Shanghai+HPC+cluster+Environment</a></li><li><a href="http://hpc.pku.edu.cn/guide_sbatch.html" target="_blank" rel="noopener">http://hpc.pku.edu.cn/guide_sbatch.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;学校的HPC计算资源通过SLURM进行管理。为了方便我进行配置，我决定整理一个快速上手指南。&lt;/p&gt;
&lt;h1 id=&quot;连接服务器&quot;&gt;&lt;a href=&quot;#连接服务器&quot; class=&quot;headerlink&quot; title=&quot;连接服务器&quot;&gt;&lt;/a&gt;连接服务器&lt;/h1&gt;&lt;p&gt;推荐使用
      
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="SLURM" scheme="http://ldzhangyx.github.io/tags/SLURM/"/>
    
      <category term="HPC" scheme="http://ldzhangyx.github.io/tags/HPC/"/>
    
  </entry>
  
  <entry>
    <title>《Ordered Neurons： Integrating Tree Structures into Recurrent Neural Networks》阅读笔记</title>
    <link href="http://ldzhangyx.github.io/2019/07/03/on-lstm/"/>
    <id>http://ldzhangyx.github.io/2019/07/03/on-lstm/</id>
    <published>2019-07-03T06:38:11.000Z</published>
    <updated>2019-08-07T09:29:21.046Z</updated>
    
    <content type="html"><![CDATA[<p>Ordered Neurons是ICLR 2019的The best paper之一。知乎上对这篇论文也有诸多讨论。本文主要探讨ON-LSTM在结构上的构建和分析，为我之后要做的音乐结构分析做一些准备。</p><h1 id="ON-LSTM"><a href="#ON-LSTM" class="headerlink" title="ON-LSTM"></a>ON-LSTM</h1><p>用一句话来概括这篇论文的核心思想，那就是：<strong>重新设计了LSTM中，历史信息的融合更新策略，使其能够显式地分层融合历史信息和当前信息。为了分层地融合信息，本文也设计了一个方法估计信息的层级，之后也利用层级，推导出了序列信息的树状结构</strong>。</p><h2 id="普通的LSTM"><a href="#普通的LSTM" class="headerlink" title="普通的LSTM"></a>普通的LSTM</h2><p>首先回忆一个普通的LSTM：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="LSTM Cell结构" title="">                </div>                <div class="image-caption">LSTM Cell结构</div>            </figure><p>从结构图从左往右依次有三个sigmoid模块：<br>第一个sigmoid是遗忘门，输出一个值，决定$c_{t-1}$应该被遗忘的比例：</p><script type="math/tex; mode=display">f_{t}  = \sigma( W_{f} x_{t} + U_{f} h_{t - 1} + b_{f}）</script><p>第二个sigmoid是输入门，输出一个值，决定输入$x_t$应该多大程度影响到$h_{t-1}$：</p><script type="math/tex; mode=display">o_{t}  = \sigma( W_{o} x_{t} + U_{o} h_{t - 1} + b_{o})</script><script type="math/tex; mode=display">\hat{c}_t  = \tanh ( W_{c} x_{t} + U_{c} h_{t - 1} + b_{c})</script><p>接下来是融合的过程。遗忘门决定了$c_{t-1}$的遗忘程度，输入门处理了输入，用点加的方式将两个向量进行融合：</p><script type="math/tex; mode=display">c_{t}  = f_{t} \circ c_{t - 1} + i_{t} \circ \hat{c}_t</script><p>第三个sigmoid是输出门，将$h_{t-1}$的结果与更新后的$c_t$进行综合考虑，进行输出。这里得到的$h_t$同时也就直接是这个时刻的output，也就是$y_t$：</p><script type="math/tex; mode=display">o_{t}  = \sigma ( W_{o} x_{t} + U_{o} h_{t - 1} + b_{o})</script><script type="math/tex; mode=display">h_{t}  = o_{t} \circ \tanh ( c_{t} )</script><h2 id="ON-LSTM-1"><a href="#ON-LSTM-1" class="headerlink" title="ON-LSTM"></a>ON-LSTM</h2><p>我们注意到，在融合的过程中（也就是点加的过程中），过去的信息、输入的信息、融合的信息，这些向量都是无序排列的。ON-LSTM的总体思想，是将$c_t$，也就是LSTM Cell的状态，赋予了三个层次。向量中，层次比较低的位置，更新比较频繁；层次比较高的位置，更新比较不频繁，这样可以更多地保留一些高层、远程的信息。</p><p>ON-LSTM具体是怎么做的呢？</p><p>假设我们有两个函数，可以在更新$c_t$前，预测出历史信息的层级$d_f$和当前输入的层级$d_i$：</p><script type="math/tex; mode=display">d_f = F_1(x_t, h_{t-1})</script><script type="math/tex; mode=display">f_i = F_2(x_t, h_{t-1})</script><p>新建一个空白的$c_t$，大致分为三个层级进行填充，三个层级的两个分界线，由$d_f$和$d_i$决定。现在分为两个情况：</p><ol><li>当前输入$x_t$的层级高于历史记录$h_{t-1}$的层级，那么$c_t$中，层次高于$d_i$的部分，保持$c_{t-1}$不变，层次低于$d_f$的部分，被当前输入的$\hat{c}_t$直接覆盖，而两个分界线中间的部分，使用点加的方式正常融合：</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><ol><li>当前输入$x_t$的层级低于历史记录$h_{t-1}$的层级，那么高层仍然保留，底层仍然用$\hat{c}_t$直接覆盖，而分界线中层的位置直接置0：</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>这就是ON—LSTM做出的重要更改。高层信息就可能保留相当长的距离（因为高层直接复制历史信息，导致历史信息可能不断被复制而不改变），而低层信息在每一步输入时都可能被更新（因为低层直接复制输入，而输入是不断改变的），所以就通过信息分级来嵌入了层级结构。更通俗地说就是分组更新，更高的组信息传得更远（跨度更大），更低的组跨度更小，这些不同的跨度就形成了输入序列的层级结构。</p><p>这里隐含了一个规律。因为我们总是遗忘$d_f$以下的所有内容，记住$d_i$以上的所有内容，所以，每当我们想更新或者擦除高层次的内容时，我们必定擦除或更新了更低层次的内容。这个结论是非常容易推理出来的。</p><h2 id="无监督树结构的生成"><a href="#无监督树结构的生成" class="headerlink" title="无监督树结构的生成"></a>无监督树结构的生成</h2><p>给定一个预训练好的ON-LSTM模型，输入一个序列${x_t}$，可以得到每一个LSTM Cell在融合时计算出来的历史信息结构的层级序列${d_f}$。找到层级最大值的下标，以它为分界线，将左右两个子序列分开，然后递归地重复这个步骤。</p><p>具体的论文细节在这里不详细描述了。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验分为四个task：语言建模，无监督依赖解析，针对性的句法评估，逻辑推理。笔记只关注语言建模部分。因为只有这个task在音乐界才有公认的数据集。</p><h3 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h3><p>Language Modeling通过测量PTB的困惑度（ppl）评估我们的模型。</p><p>模型使用三层ON-LSTM，hidden unit有1150个，embedding size为400。对于master gates，缩小因子C=10. Embedding dropout为(0.5, 0.3, 0.45, 0.1)。current matrics使用了0.45的weight dropout。</p><p>同等状态下ON-LSTM表现优于普通LSTM，值得注意的是，不需要增加skip connection或者是大量增加参数。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="见解和感想"><a href="#见解和感想" class="headerlink" title="见解和感想"></a>见解和感想</h1><p>树状结构在各个topic都是有用的。将树状结构引入神经网络语言模型可能会带来一些这样的好处：</p><ol><li>可以得到语言的分层表示；</li><li>可以建模语言的组分效应（compositional effects）；</li><li>可以提供反向求导的捷径，帮助解决长期依赖的问题；</li><li>一个更好的归纳偏置，从而降低训练数据的要求，提高泛化能力。</li></ol><p>在音乐领域，得到一个音乐片段的结构也是非常有用的。</p><p>知乎上面相关的讨论，我比较感兴趣的是一个现象。这样训练的Language Model同时对语法和语义进行了建模，所以对于语义不行而完全符合语法的句子，LSTM的得分也会较低。那么这样得出来的无监督树结构到底是语法的树表达，还是语义的树表达呢？</p><p>这里有一个猜测，ON-LSTM用于树结构生成时，使用了三层ON-LSTM中第二层的历史层级序列，可能暗含一个现象：模型的第一层LSTM偏句法，第二层偏语义。这个问题被ELMo (Peters et al., 2018)和Liu et al. (2019)从不同的角度得出过一些可以参考的见解。</p><h1 id="论文引用链"><a href="#论文引用链" class="headerlink" title="论文引用链"></a>论文引用链</h1><p>这篇文章有两篇前置文章，分别是：</p><ol><li>Neural Language Modeling by Jointly Learning Syntax and Lexicon, ICLR 2018</li></ol><p>这篇文章提出了PRPN模型，将类似的距离——树结构信息用在了这个模型。</p><ol><li>Straight to the Tree: Constituency Parsing with Neural Syntactic Distance, ACL 2018</li></ol><p>这篇文章正式提出了衡量两个节点之间距离的概念，并且提出了将距离与二叉树进行相互转换的算法。</p><p>这篇文章后面被这些文章引用：</p><ol><li>Unsupervised Recurrent Neural Network Grammars, NAACL 2019</li></ol><p>这篇文章通过一个VAE的类似结构，将树结构作为latent information传递。然而算法无监督，论文使用了变分推理以及Inside算法，推导出可能的语法树，而生成模型是一个RNNG，输入语法树，使用句子本身作为监督信号。更详细的讲解可以看这里：<a href="https://zhuanlan.zhihu.com/p/64032803" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/64032803</a></p><ol><li>An Imitation Learning Approach to Unsupervised Parsing, ACL 2019（最佳短文章提名）</li></ol><p>这个模型将PRPN的知识转移到了Tree-LSTM模型上，然后通过Gumbel-Softmax training将任务转换到语义导向上来。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li>苏剑林. (2019, May 28). 《ON-LSTM：用有序神经元表达层次结构 》[Blog post]. Retrieved from <a href="https://kexue.fm/archives/6621" target="_blank" rel="noopener">https://kexue.fm/archives/6621</a></li><li>知乎. 如何评价ICLR 2019 best paper: Ordered Neurons? <a href="https://www.zhihu.com/question/323190069" target="_blank" rel="noopener">https://www.zhihu.com/question/323190069</a></li><li><a href="https://arxiv.org/abs/1810.09536" target="_blank" rel="noopener">https://arxiv.org/abs/1810.09536</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Ordered Neurons是ICLR 2019的The best paper之一。知乎上对这篇论文也有诸多讨论。本文主要探讨ON-LSTM在结构上的构建和分析，为我之后要做的音乐结构分析做一些准备。&lt;/p&gt;
&lt;h1 id=&quot;ON-LSTM&quot;&gt;&lt;a href=&quot;#ON-L
      
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="NLG" scheme="http://ldzhangyx.github.io/tags/NLG/"/>
    
      <category term="NLP" scheme="http://ldzhangyx.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch工程中，提升代码体验的几点想法</title>
    <link href="http://ldzhangyx.github.io/2019/07/02/python-init/"/>
    <id>http://ldzhangyx.github.io/2019/07/02/python-init/</id>
    <published>2019-07-02T09:14:22.382Z</published>
    <updated>2018-11-16T05:36:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>提及PyTorch的代码写作，我没有找到一个大家约定俗成的规约。我自己遵循了一些能让Python代码更具有可读性和持续开发的规则，现记录如下。</p><a id="more"></a><h2 id="保持工程的模块化"><a href="#保持工程的模块化" class="headerlink" title="保持工程的模块化"></a>保持工程的模块化</h2><p>一个我认为整洁的项目应该遵循这样的结构，现在我以一个Encoder-Decoder框架为例进行描述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''The structure of my project.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">+--data</span><br><span class="line">|--+log</span><br><span class="line">|--+dataset</span><br><span class="line">|  |  +--train.txt</span><br><span class="line">|  |  +--test.txt</span><br><span class="line">+--model</span><br><span class="line">|  +--__init__.py</span><br><span class="line">|  +--encoder.py</span><br><span class="line">|  +--decoder.py</span><br><span class="line">|  +--(model_name).py</span><br><span class="line">|  +--highway.py</span><br><span class="line">|  ...</span><br><span class="line">+--utils</span><br><span class="line">|  +--__init__.py</span><br><span class="line">|  +--batch_loader.py</span><br><span class="line">|  +--parameters.py</span><br><span class="line">|  +--functional.py</span><br><span class="line">|  ...</span><br><span class="line">+--README.md</span><br><span class="line">+--LICENSE</span><br><span class="line">+--test.py</span><br><span class="line">+--train.py</span><br></pre></td></tr></table></figure><p>在data文件夹里，放置数据集和模型，以及log输出；<br>在model文件夹里，将模型尽可能模块化，并用一个顶层模块将子模块组合起来。这个顶层模块要实现训练与测试的类方法。<br>在utils里放置参数列表文件，小工具，以及对数据预处理的文件。（我曾见过有用Jupyter Notebook进行可视化的数据清理的做法）<br>如果要开源，请加入README.md和LICENSE，一般我会选择GPL v3的License。<br>在test.py和train.py里实例化模型，进行训练和测试。</p><p>一个可以参照的GitHub开源项目可以点这里：<a href="https://github.com/kefirski/pytorch_RVAE" target="_blank" rel="noopener">https://github.com/kefirski/pytorch_RVAE</a></p><h2 id="写一个好的注释"><a href="#写一个好的注释" class="headerlink" title="写一个好的注释"></a>写一个好的注释</h2><p>Google曾经发布过一个<a href="https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/" target="_blank" rel="noopener">代码风格指南</a>，里面详细地介绍了Python代码应当遵循的有用的规范。在里面的函数一节里，就提及了注释的规范。</p><p>对于文档注释，文档字符串是包, 模块, 类或函数里的第一个语句。这些字符串可以通过对象的<strong>doc</strong>成员被自动提取, 并且被pydoc所用。根据<a href="https://www.python.org/dev/peps/pep-0257/" target="_blank" rel="noopener">PEP-257</a>(这也是一个非常有名的指南)，文档字符串使用的注释风格应当是成对的三个双引号。</p><p>格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Add</span><span class="params">(arg_1, arg_2)</span>:</span></span><br><span class="line">    <span class="string">"""Add two numbers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    There should be some details if needed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        arg_1: The first number to be added.</span></span><br><span class="line"><span class="string">        arg_2: The second number to be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A number. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>如果是书写一个类，那么在类声明的下一行应该对其<strong>所有的类成员Attributes</strong>做出解释。</p><h2 id="在容易混淆的函数声明中使用类型约束"><a href="#在容易混淆的函数声明中使用类型约束" class="headerlink" title="在容易混淆的函数声明中使用类型约束"></a>在容易混淆的函数声明中使用类型约束</h2><p>这是一点个人的看法。<a href="https://www.python.org/dev/peps/pep-0484/" target="_blank" rel="noopener">PEP-484</a>文档对类型约束提出了规范。在必要的时候使用类型约束能让我在运行之前就能发现一些问题。</p><p>不加类型约束：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greetings</span><span class="params">(name_list)</span>:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><p>加入类型约束：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greetings</span><span class="params">(name_list: List<span class="params">(str)</span>)</span> -&gt; str:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><h2 id="使用数据类装饰器（Python-3-7特性）"><a href="#使用数据类装饰器（Python-3-7特性）" class="headerlink" title="使用数据类装饰器（Python 3.7特性）"></a>使用数据类装饰器（Python 3.7特性）</h2><p>在使用数据类的时候（比如我的parameters.py中的Parameters类）（根据我的个人习惯，我会将文件名全部小写，但是类会大写首字母），遵循这一条会让可读性有一定提升。<a href="https://www.python.org/dev/peps/pep-0557/" target="_blank" rel="noopener">PEP-557</a>规定了相关规则。</p><p>顺带一提，建议工程中所有的类都<strong>显式继承object类</strong>或者<strong>nn.Module类</strong>。</p><p>原版代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, b)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br></pre></td></tr></table></figure></p><p>使用类装饰器：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    a: str</span><br><span class="line">    b: str</span><br></pre></td></tr></table></figure></p><p>这个类越是数据类，好处越明显。而且这个装饰器还会自动地帮助你实现一些魔术方法。</p><h2 id="使用新的Super方法调用"><a href="#使用新的Super方法调用" class="headerlink" title="使用新的Super方法调用"></a>使用新的Super方法调用</h2><p>在我们初始化一个神经网络模块的时候，这样的代码在开源代码中十分常见：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>但是这个超类方法可以被下列写法完全替代：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><p>推荐使用。</p><h2 id="使用-init-py文件"><a href="#使用-init-py文件" class="headerlink" title="使用__init__.py文件"></a>使用__init__.py文件</h2><p>这个文件的作用是将文件夹变为一个Python模块。Python 中的每个模块的包中，都有__init__.py 文件。</p><p>通常__init__.py 文件为空，但是我们还可以为它增加其他的功能。我们在导入一个包时，实际上是导入了它的__init__.py文件。这样我们可以在__init__.py文件中批量导入我们所需要的模块，而不再需要一个一个的导入。</p><p>这样做在写代码时有个额外的好处：在Visual Studio Code等编辑器中，编辑器能够进行<strong>智能提示和自动补全</strong>。</p><h2 id="在必要的时候使用assert"><a href="#在必要的时候使用assert" class="headerlink" title="在必要的时候使用assert"></a>在必要的时候使用assert</h2><p>assert是一个不怎么被提起的功能，然而我在实际使用中感受到assert可以帮助我们进行debug。一般来说，我进行debug最常用的方法是进行单步调试，在<strong>关键的地方打断点，观察变量列表</strong>；然而断点的机制决定了它并不是持久化的debug策略，我们在与其他人协作写代码的时候，也无法提醒其重点关注和检查哪些问题。</p><p>用于持久化进行测试的常见方法一般是将需要观察的变量进行print输出，然后<strong>观察output控制台</strong>。然而print函数在debug过程中可是要去掉的。所以assert函数可以<strong>在关键的地方对变量进行检查</strong>。通过这种检查，函数的功能得以保证，且无需添加if，避免了过深的隐蔽bug；此外，assert语句可以提醒其他的阅读这段代码的人：这个地方的变量应当满足什么要求，是怎么样的一个变量。</p><h2 id="持续更新中"><a href="#持续更新中" class="headerlink" title="持续更新中"></a>持续更新中</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;提及PyTorch的代码写作，我没有找到一个大家约定俗成的规约。我自己遵循了一些能让Python代码更具有可读性和持续开发的规则，现记录如下。&lt;/p&gt;
    
    </summary>
    
      <category term="软件工程" scheme="http://ldzhangyx.github.io/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
      <category term="Python" scheme="http://ldzhangyx.github.io/tags/Python/"/>
    
      <category term="软件工程" scheme="http://ldzhangyx.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>之后几年里想做的研究（非正式Research Proposal）</title>
    <link href="http://ldzhangyx.github.io/2019/03/31/research-proposal/"/>
    <id>http://ldzhangyx.github.io/2019/03/31/research-proposal/</id>
    <published>2019-03-31T12:01:10.000Z</published>
    <updated>2019-04-01T04:34:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>The area I am passionate about is computational creativity[1]. It includes story generation, message generation, and even music generation. I want to endow machines with creative behaviors, and, I am doing good at music generation, indeed.</p><p>About generation model, there are several questions.</p><ul><li>Is that useful? My answer is yes. We have lots of ideas, but it is difficult to transform ideas to artworks. Most of us only have the abilities to finish a draft. If it can be controlled, then models can be used to assist people do creative task like writing and composition.</li><li>How can we control the model? Some approaches can be applied.</li></ul><ol><li>The first is to control input.[2] FAIR developed a model to generate stories from tags and topic words[3]. Hierarchical attention mechanism is used in this model. By controlling the input, they can create different kinds of stories.</li><li>The second way is to control structure. Skeleton model is used to keep generated narrative consistent[4]. They set a structure via reinforcement learning and generate stories progressively, to keep articles fluent and can be read.</li></ol><ul><li>What is the main difficulty?</li></ul><ol><li>We lack data. However, we can solve the problem by designing training methods. CocoNet is used to generate J.S. Bach’s chorales[5]. In the beginning, they have only 306 pieces of artworks. But they tried to erase different parts and let the model recover them, and then erase again. During this period, the model can gradually learn composition skills</li><li>We lack benchmarks. Now most generative models are evaluated manually. We can quantify the content of the assessment and achieve results similar to human assessment in mathematical form. A good example is VMO analysis, which can discover the hierarchy of music clips and analyze the logic of music.</li><li>We lack good learning target. In my opinion, traditional seq2seq model uses cross-entropy loss, which limits probabilities. My idea is that, we can analyze features, like consistency and local circulation, to train a discriminative model, like GAN.</li><li>Structure design is difficult. Skeleton model applied reinforcement learning, but I have a better idea – use meta-learning techniques. Learning to teach models[6] is my current research topic at Microsoft. Meta-learning can help us decide hypothesis space[7]. Maybe we can use this method to control the structure of artworks.</li></ol><ul><li>Your ideas?</li></ul><ol><li>Generate models hierarchically, but use meta-learning techniques as assistance, to make generated results controllable.</li><li>Graph Convolutional Network should be applied to used to make the text logical. Knowledge graph include prior knowledge in the network structure, and graph embedding allows embedding results to contain information about the knowledge graph[8].</li></ol><p>References<br>[1] Colton, S., Charnley, J. W., &amp; Pease, A. (2011, April). Computational Creativity Theory: The FACE and IDEA Descriptive Models. In ICCC (pp. 90-95).<br>[2] Briot, J. P., &amp; Pachet, F. (2017). Music generation by deep learning-challenges and directions. arXiv preprint arXiv:1712.04371.<br>[3] Fan, A., Lewis, M., &amp; Dauphin, Y. (2018). Hierarchical neural story generation. arXiv preprint arXiv:1805.04833.<br>[4] Xu, J., Zhang, Y., Zeng, Q., Ren, X., Cai, X., &amp; Sun, X. (2018). A skeleton-based model for promoting coherence among sentences in narrative story generation. arXiv preprint arXiv:1808.06945.<br>[5] Huang, C. Z. A., Cooijmans, T., Roberts, A., Courville, A., &amp; Eck, D. (2019). Counterpoint by convolution. arXiv preprint arXiv:1903.07227.<br>[6] Fan, Y., Tian, F., Qin, T., Li, X. Y., &amp; Liu, T. Y. (2018). Learning to teach. arXiv preprint arXiv:1805.03643.<br>[7] Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., … &amp; Sugiyama, M. (2018). Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems (pp. 8536-8546).<br>[8] Liu, W., Wang, Z., Liu, X., Zeng, N., Liu, Y., &amp; Alsaadi, F. E. (2017). A survey of deep neural network architectures and their applications. Neurocomputing, 234, 11-26.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The area I am passionate about is computational creativity[1]. It includes story generation, message generation, and even music generatio
      
    
    </summary>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="NLG" scheme="http://ldzhangyx.github.io/tags/NLG/"/>
    
      <category term="个人想法" scheme="http://ldzhangyx.github.io/tags/%E4%B8%AA%E4%BA%BA%E6%83%B3%E6%B3%95/"/>
    
      <category term="machine learning" scheme="http://ldzhangyx.github.io/tags/machine-learning/"/>
    
      <category term="Master" scheme="http://ldzhangyx.github.io/tags/Master/"/>
    
  </entry>
  
</feed>
