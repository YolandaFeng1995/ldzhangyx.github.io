<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张逸霄的技术小站</title>
  
  <subtitle>当然也可能有一些非技术内容</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ldzhangyx.github.io/"/>
  <updated>2019-01-02T07:15:02.605Z</updated>
  <id>http://ldzhangyx.github.io/</id>
  
  <author>
    <name>张逸霄</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>arima</title>
    <link href="http://ldzhangyx.github.io/2019/01/02/arima/"/>
    <id>http://ldzhangyx.github.io/2019/01/02/arima/</id>
    <published>2019-01-02T07:15:02.000Z</published>
    <updated>2019-01-02T07:15:02.605Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>音乐生成的武器库——集中介绍</title>
    <link href="http://ldzhangyx.github.io/2018/12/29/music-toolkits/"/>
    <id>http://ldzhangyx.github.io/2018/12/29/music-toolkits/</id>
    <published>2018-12-29T03:08:44.000Z</published>
    <updated>2018-12-29T06:52:03.539Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是之后系列文章的汇总集中目录。</p> <a id="more"></a><p> 首先做个小宣传。因为很多想做这一块的同行本科生们不是很清楚国内外的院校列表。有一位前辈在GitHub上整理了大部分的院校列表出来，我也对这个列表做了一点贡献。<a href="https://github.com/beiciliang/intro2musictech/blob/master/INFO-ResearchGroups.md" target="_blank" rel="noopener">点击查看</a></p><p> Python官网整理的Python库有这些：<a href="https://wiki.python.org/moin/PythonInMusic" target="_blank" rel="noopener">链接</a></p><h1 id="基础组件"><a href="#基础组件" class="headerlink" title="基础组件"></a>基础组件</h1><h2 id="Python-MIDI"><a href="#Python-MIDI" class="headerlink" title="Python-MIDI"></a>Python-MIDI</h2><ul><li>项目官网：<a href="https://github.com/vishnubob/python-midi/tree/feature/python3" target="_blank" rel="noopener">https://github.com/vishnubob/python-midi/tree/feature/python3</a></li></ul><p>很多库的前置库，安装时记得选择其Python 3的branch。</p><h1 id="乐谱生成"><a href="#乐谱生成" class="headerlink" title="乐谱生成"></a>乐谱生成</h1><p> 相关的知乎问题：<a href="https://www.zhihu.com/question/23003386" target="_blank" rel="noopener">链接</a></p><h2 id="LilyPond"><a href="#LilyPond" class="headerlink" title="LilyPond"></a>LilyPond</h2><ul><li>项目官网：<a href="http://lilypond.org/index.html" target="_blank" rel="noopener">http://lilypond.org/index.html</a></li></ul><p>LilyPond是GNU Project的一部分，是免费的乐谱生成软件。生成的乐谱质量很高，支持从xml/mxl以及从midi转化成lilypond格式源代码（.ly），并且可以使用类LaTeX语言编写。</p><p>可以用两个方法通过Python代码调用LilyPond库：</p><ol><li>Python-ly库，其GitHub地址在<a href="https://github.com/wbsoft/python-ly" target="_blank" rel="noopener">这里</a></li><li>mingus的lilypond模块，GitHub地址在<a href="https://github.com/bspaans/python-mingus" target="_blank" rel="noopener">这里</a></li></ol><h2 id="MuseScore"><a href="#MuseScore" class="headerlink" title="MuseScore"></a>MuseScore</h2><ul><li>项目官网：<a href="https://musescore.org" target="_blank" rel="noopener">https://musescore.org</a></li><li>社区和乐谱集：<a href="https://musescore.org" target="_blank" rel="noopener">https://musescore.org</a></li></ul><p>MuseScore的特点在于其有充分活跃的社区。这个软件和LilyPond都可以通过XML和MIDI进行数据传送，所以在接口方面很灵活。</p><h1 id="音乐的符号化表示"><a href="#音乐的符号化表示" class="headerlink" title="音乐的符号化表示"></a>音乐的符号化表示</h1><h2 id="music21"><a href="#music21" class="headerlink" title="music21"></a>music21</h2><ul><li>项目官网：<a href="http://web.mit.edu/music21/" target="_blank" rel="noopener">http://web.mit.edu/music21/</a></li></ul><p>对Python3的支持很好，也是目前为止最完善的一个Python库、武器库。包括音乐的乐谱显示、音乐合成、矩阵分析、语料库、符号表达等。</p><h2 id="ABC格式"><a href="#ABC格式" class="headerlink" title="ABC格式"></a>ABC格式</h2><ul><li>项目官网：<a href="http://abcnotation.com/" target="_blank" rel="noopener">http://abcnotation.com/</a></li></ul><p>ABC格式是一种复杂而古早的模式，很多早期的数据集使用了这种标注。ABC格式可以转化为music21，或者被其他方法解析。</p><h2 id="Pretty-MIDI表示的Piano-Roll"><a href="#Pretty-MIDI表示的Piano-Roll" class="headerlink" title="Pretty_MIDI表示的Piano Roll"></a>Pretty_MIDI表示的Piano Roll</h2><ul><li>项目官网：<a href="https://github.com/craffel/pretty-midi" target="_blank" rel="noopener">https://github.com/craffel/pretty-midi</a></li><li>我以前写的文章：<a href="https://www.cnblogs.com/ldzhangyx/p/7789939.html" target="_blank" rel="noopener">https://www.cnblogs.com/ldzhangyx/p/7789939.html</a></li></ul><p>一个方便将MIDI文件转化为Piano Roll的库。这个库被用在Google Magenta里，我自己的项目也要使用它。master分支现在已经支持了Python3。</p><h2 id="mingus"><a href="#mingus" class="headerlink" title="mingus"></a>mingus</h2><ul><li>项目官网：<a href="https://bspaans.github.io/python-mingus/" target="_blank" rel="noopener">https://bspaans.github.io/python-mingus/</a></li><li>Python 3版本：<a href="https://github.com/edudobay/python-mingus" target="_blank" rel="noopener">https://github.com/edudobay/python-mingus</a></li></ul><p>作者很久没有维护过了，但是从快速上手来看是一个不错的Python库。Python 3版本已经给出。</p><h1 id="音乐合成"><a href="#音乐合成" class="headerlink" title="音乐合成"></a>音乐合成</h1><h2 id="PySynth"><a href="#PySynth" class="headerlink" title="PySynth"></a>PySynth</h2><ul><li>项目官网：<a href="https://mdoege.github.io/PySynth/" target="_blank" rel="noopener">https://mdoege.github.io/PySynth/</a></li></ul><p>主要用处就是将midi或ABC格式的文件转化为wav文件，有多种合成方法可选，十分方便好用。</p><h2 id="pyfluidsynth3"><a href="#pyfluidsynth3" class="headerlink" title="pyfluidsynth3"></a>pyfluidsynth3</h2><ul><li>项目官网：<a href="https://github.com/tea2code/pyfluidsynth3" target="_blank" rel="noopener">https://github.com/tea2code/pyfluidsynth3</a></li></ul><p>Pretty-MIDI的前置库，用于合成音乐。这里推荐的是Python 3的重写版本。</p><ul><li>项目官网</li></ul><h1 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h1><h2 id="Nottingham-Dataset"><a href="#Nottingham-Dataset" class="headerlink" title="Nottingham Dataset"></a>Nottingham Dataset</h2><ul><li>地址：<a href="https://github.com/jukedeck/nottingham-dataset" target="_blank" rel="noopener">https://github.com/jukedeck/nottingham-dataset</a></li></ul><p>特别有名的数据集，音乐界的MNIST。ABC格式可以被上面一些库直接解析，MIDI格式也可以转换。数据预处理的办法一般是shift到12个大调或者小调，速度调整至120bpm。</p><h2 id="Lakh-Dataset"><a href="#Lakh-Dataset" class="headerlink" title="Lakh Dataset"></a>Lakh Dataset</h2><ul><li>地址：<a href="https://colinraffel.com/projects/lmd/" target="_blank" rel="noopener">https://colinraffel.com/projects/lmd/</a></li></ul><p>Lakh MIDI数据集是176,581个独特MIDI文件的集合，其中45,129个已匹配并与Million Song Dataset中的条目对齐。</p><h2 id="Lakh-Pianoroll"><a href="#Lakh-Pianoroll" class="headerlink" title="Lakh Pianoroll"></a>Lakh Pianoroll</h2><ul><li>地址：<a href="https://salu133445.github.io/lakh-pianoroll-dataset/" target="_blank" rel="noopener">https://salu133445.github.io/lakh-pianoroll-dataset/</a></li></ul><p>上面数据集的转化版本，但是表示使用了Pianoroll。</p><h2 id="Million-Songs"><a href="#Million-Songs" class="headerlink" title="Million Songs"></a>Million Songs</h2><ul><li>地址：<a href="https://labrosa.ee.columbia.edu/millionsong/" target="_blank" rel="noopener">https://labrosa.ee.columbia.edu/millionsong/</a></li></ul><p>不是midi文件而是波形文件，特点在于数据量极大。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是之后系列文章的汇总集中目录。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://ldzhangyx.github.io/tags/Python/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>《The Effect of Explicit Structure Encoding of Deep Neural Networks for Symbolic Music Generation》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/12/26/structure-encoding-music/"/>
    <id>http://ldzhangyx.github.io/2018/12/26/structure-encoding-music/</id>
    <published>2018-12-26T06:17:37.000Z</published>
    <updated>2018-12-27T09:00:48.754Z</updated>
    
    <content type="html"><![CDATA[<p>翻夏光宇老师论文列表时翻到的一篇文章，觉得里面的分析方法很有特点。</p><a id="more"></a><h1 id="论文要点"><a href="#论文要点" class="headerlink" title="论文要点"></a>论文要点</h1><p>计算创造（Computational Creativity）是一个很好玩的领域。有一个会议名为ICCC，专门为这样的生成模型做展示。生成模型也有很多种，但是音乐生成因为其作曲<strong>结构</strong>十分复杂，使得任务相对难做一些。给定和弦行进，在约束下进行音乐生成，是这篇论文的任务。当然这个任务也可以组合进别的复杂任务中。</p><p>论文探索了一件事，那就是对音乐结构显式地进行了encoding，通过两个模型：LSTM和WaveNet，并且对效果做了对比。从Encoding的角度来看，这篇文章与XiaoIce Band的编码其实有着类似的想法，但是具体方法不一样。</p><p>这篇文章对结构的分析对我很有启发。它们使用了名为Variable Markov Oracle的分析方法分析音乐结构，这个方法能将音乐行进转换为有限状态自动机，使用Markov链描述音乐。</p><p>分析之后的结论：使用堆叠的空洞卷积曾能提高结构编码的表现，以及将和弦行进进行全局编码（global encoding）也能更好地提升表现。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>论文首先聊了一下自动音乐生成的历史。神经网络给音乐生成带来了新进展，但是结构性上仍有所不足。比较成功的作曲局限于巴赫，因为其结构局部化明显，易于察觉。以前的研究提出过结构约束的技术，但是这样的约束与生成的过程有冲突，并且需要细致调参。相比之下直接将结构进行Encoding更加有意义。</p><p>论文选定了给定和弦行进的音乐生成任务。这个任务中，论文对两个结构做了系统比较：LSTM和WaveNet（空洞时序CNN，dilated temporal-CNN）。值得一提的是WaveNet在这篇论文中才第一次被用于象征域（symbolic domain）的音乐生成。就隐藏变量（hidden virables）的依赖性来看，两个结构是相似的——相对于ARMA模型和MA模型（都是统计学领域的模型）。据了解，这篇论文也是第一个进行两个结构系统性比较的论文。</p><p>象征域的音乐生成任务，有着信息量丰富的优点。在任务中，论文提出了一种创新的编码方式，将和弦与旋律进行交错表示（如图1）。这样能够高效组合不同时间尺度的和弦与旋律，使得模型能够同时学习到“过去的旋律”和“接下来两个小节的旋律”之间的时间延迟的依赖性,同时也可以学到两个小节内的和弦与旋律的同时关系。这样操作的优点在于，人们很少完全做即兴创作，几乎总是依赖于一个提前定义好的蕴含了高层音乐结构的信息（比如figured bass）。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>对长度为$T$的音乐，给定直到时间点$t$的旋律，和全局和弦行进，目标是生成从$t$到$T$的剩余旋律。所以和弦是用作condition，在每一个step给出，最终的输出应保持旋律流，且与和弦有交互。</p><p>回头看图1，其中K是给定的内容，而P是期待输出的内容。</p><p>一个单向模型的条件概率分布可以这么定义：</p><script type="math/tex; mode=display">p(M_{T-t+1:T}|C_{1:T}) = \prod_{i=1}^{T-t}p(m_i|m_1, ..., m_{i-1}, c_1, ..., c_i)</script><p>其中$m_i$表示第$i$个step的旋律，$c_i$是第$i$个step的和弦，而$C_k$表示一直到$k$个step和和弦总体表示。</p><h3 id="数据表示"><a href="#数据表示" class="headerlink" title="数据表示"></a>数据表示</h3><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>在LSTM模型中，将输入表示为向量V，V是两个向量M和C的组合：</p><script type="math/tex; mode=display">V = (M,C)</script><p>拥有1*155的维度，其中0~127代表MIDI的128个音，128代表休止符，129代表holding；130~154代表25个不同的和弦，其中前12个是大调和弦，之后12个是小调和弦，而最后一个代表没有和弦。</p><p>和弦不止24个，那么将之外的和弦映射到对应的和弦，比如C7和弦映射到C和弦，Cm7和弦映射到Cm和弦。</p><p>一首长度为T的乐曲，可以分别用五线谱和矩阵表示为下图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>可以看出来数据处理以八分音符为最小的分辨率（实际模型可以使用更高的分辨率），四分音符就可以表示为八分音符，然后holding住一个八分音符时长（在双向的模型里有所区别）。我个人认为这是一个有效的表示法。其他类似的表示法是music21提供的包，也可以对音乐进行编码。</p><p>特别要指出一点，在原来的论文里没有提及，是我对作者进行询问之后得到的结论。</p><p>在单向模型里，这一个表示没有问题，但是在双向模型里，这样的表示会让输入有一些奇怪之处，比如说holding究竟是在holding哪个音。对于这个问题，作者实际处理的时候使用了回文序列（在双向模型中），即将一个这样的序列：[64,129,129,129]改成了[64,129,129,64]，以保证模型不会出现理解障碍。这样就有另一个问题了，如果这个模型短到holding不存在，即“单音序列”，这该怎么表示呢？作者使用了0.03125秒作为最小分辨率，而八分音符的时长为0.125秒，所以0.03125秒应该是32分音符的时长。作者去除了这些单音序列，保证这样的数据表示正常工作（其实我也没有完全明白，因为单音序列看起来并不会带来任何问题）。</p><h4 id="WaveNet"><a href="#WaveNet" class="headerlink" title="WaveNet"></a>WaveNet</h4><p>表示与上述相同，但是M只有128维，0代表rest，其他代表音高。这里不区分连续多音，相连的两个同样音高视为延音（为什么？）。而和弦仍然是25维。</p><h3 id="LSTM结构"><a href="#LSTM结构" class="headerlink" title="LSTM结构"></a>LSTM结构</h3><p>两个模型，单向和双向。其中双向编码的特别之处在于将和弦完全编码进来了。编码的方式如之前所说是交叉编码，所以在i时刻之后的旋律部分置空，只留和弦部分，将这个矩阵进行编码。</p><p>LSTM结构的大致结构如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="WaveNet结构"><a href="#WaveNet结构" class="headerlink" title="WaveNet结构"></a>WaveNet结构</h3><p>WaveNet将旋律与和弦分开编码，所以也需要分开输入。没有condition的模型公式如下（k代表第k层）：</p><script type="math/tex; mode=display">z = \tanh(W_{f,k} * m) \odot \sigma(W_{g,k} * m)</script><p>其中*符号代表了空洞卷积。</p><p>而加入了condition的模型公式改成了这个样子：</p><script type="math/tex; mode=display">z = \tanh(W_{f,k} * m + V_{f,k} * c) \odot \sigma(W_{g,k} * m + V_{f,k} * c)</script><p>上述公式的第一个*符号都代表了空洞卷积操作，而涉及到c的第二个*符号代表了一个1*1的卷积层。</p><p>空洞卷积的结构如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>WaveNet的结构如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>训练的时候使用交叉熵，将其对比原始的音乐序列（loss函数一直是NLP生成模型的痛，因为原有的label不一定是唯一解，所以在NLP领域也出现了一些别的方法来解决生成模型多样性的问题）。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>Nottingham数据集，平移到12个大/小调，进行数据增强；速度调整为120bpm，所以4分音符对应0.5秒，而最小分辨率0.03125秒对应16分音符。</p><h3 id="人类评估的调查问卷"><a href="#人类评估的调查问卷" class="headerlink" title="人类评估的调查问卷"></a>人类评估的调查问卷</h3><p>评分制，只要考察：旋律与和弦的交互性，旋律复杂性，旋律结构性。</p><p>验证姐u哦采用了ANOVA方法做假设检验，以及对不同的模型做了两两的t检验。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>模型性能的确有着统计学显著的差异。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="8.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="VMO的模式发现（Pattern-Discovery）"><a href="#VMO的模式发现（Pattern-Discovery）" class="headerlink" title="VMO的模式发现（Pattern Discovery）"></a>VMO的模式发现（Pattern Discovery）</h3><p>VMO，全称Variable Markov Oracle，用于分析时间序列的音乐模式。VMO可以捕捉三种连接：前向连接（forward link）、后缀连接（suffix link）、反向后缀连接（reverse suffix link）。每个时间片段t的后缀连接是给定序列的最长后缀的起始点。</p><p>相关的论文可以看《Music Pattern Discovery with Variable Markov Oracle: A Unified Approach to Symbolic and Audio Representations》。一个典型的VMO分析图可以看下图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="9.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>作者将生成的midi文件转化成了波形用于对比。采样频率44.1kHz，并且实现了short-time Frourier transform得到频谱数据。下图将频谱根据energy折成了12类音高得到的图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="10.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>频谱中前10秒是给定的开头。</p><p>作者还计算了信息率（Information Rate）得到距离阈值$\theta$。如果两个时序的距离如果小于$\theta$的话，将会被认为是同一个时间序列。所以过大或者过小的阈值都会影响到模式识别的结果。作者通过寻找最大的IR值来确定$\theta$的值，如图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="VMO"><a href="#VMO" class="headerlink" title="VMO"></a>VMO</h3><p>VMO分析能够寻找音乐的主题片段。由于VMO分析中，阈值并不完全相同，所以显示出来的结果也会略有不同。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="LSTM-1"><a href="#LSTM-1" class="headerlink" title="LSTM"></a>LSTM</h3><p>我们发现LSTM模型生成的音乐在重复生成模式方面具有很大的潜力。图10显示单向LSTM模型出现几次重复（蓝色框），而双向LSTM模型在时间范围内肯定会有更多的模式重复（红色框和黄色框表示）。由于它所拥有的短期记忆和我们处理的直接输入，LSTM模型揭示了捕获数据集中固有结构的自然和主动性。此外，我们注意到，与单向LSTM相比，双向LSTM产生的音乐对于变化的和弦更稳定且更敏感。下图显示了一个示例，其中顶部系统表示双向LSTM模型，底部系统表示单向LSTM模型。我们看到两种模型都可以在当前和弦之后生成音符片段（度量），单向模型无法作为整体进行和弦进行。对于双向LSTM，生成的音符序列趋向于更不稳定，平滑和音乐。这可能是因为生成过程包括即将到来的和弦作为全局结构限制。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="WaveNet-1"><a href="#WaveNet-1" class="headerlink" title="WaveNet"></a>WaveNet</h3><p>WaveNet能够学到一些有趣的节奏模式，下图可以看出切分被学习到了，而这个模式因为分辨率只有16分音符，所以需要长期结构才能捕捉到。这表示空洞卷积的堆叠恰好与音乐结构一致。从节奏生成的角度来看，WaveNet更适合象征域的音乐生成（对比音频），因为音频的层次结构较少。<br>但，WaveNet同时丢失了一些结构特征。所以双向WaveNet是一个很好的idea。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>结果表明，两个关键因素在很大程度上改善了模型性能：</p><ol><li>一组扩张的卷积层，它们明确地编码了旋律序列的结构依赖性。</li><li>并入chordprogress作为全局结构约束。</li></ol><p>期待作者能研究出高效可用的双向WaveNet。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;翻夏光宇老师论文列表时翻到的一篇文章，觉得里面的分析方法很有特点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
      <category term="WaveNet" scheme="http://ldzhangyx.github.io/tags/WaveNet/"/>
    
      <category term="VMO" scheme="http://ldzhangyx.github.io/tags/VMO/"/>
    
      <category term="deep learning" scheme="http://ldzhangyx.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>最近想读的一些论文</title>
    <link href="http://ldzhangyx.github.io/2018/12/25/recent/"/>
    <id>http://ldzhangyx.github.io/2018/12/25/recent/</id>
    <published>2018-12-25T09:36:17.000Z</published>
    <updated>2018-12-25T10:25:56.681Z</updated>
    
    <content type="html"><![CDATA[<p>一篇很随便的随记。</p><a id="more"></a><h1 id="音乐生成"><a href="#音乐生成" class="headerlink" title="音乐生成"></a>音乐生成</h1><p>首先说一句，音乐生成这个方向我现在正在写一篇论文笔记。这篇论文不怎么出名，但是可以表明一个方向。伴随这篇论文笔记，我还会重新系统阐述一些Python与音乐生成方向的常用处理方法，比如pretty-midi，music21等这样的库，以及数据增强和预处理的一般方法套路。</p><h1 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h1><p>这个方向我的论文阅读比较自由，因为最近实习做的内容其实与NLP没有特别大的关系。</p><p>第一篇是关于图网络的综述。《Deep Learning on Graphs: A Survey》，对图网络的发展做了一个比较详细的阐述。</p><p>第二篇是《From Recognition to Cognition: Visual Commonsense Reasoning》，提出了一个常识推理数据集。相关方向我不打算深入，仅仅是去做个了解，明白常识推理在NLP各个领域的现状。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一篇很随便的随记。&lt;/p&gt;
    
    </summary>
    
      <category term="随记" scheme="http://ldzhangyx.github.io/categories/%E9%9A%8F%E8%AE%B0/"/>
    
    
      <category term="心得体会" scheme="http://ldzhangyx.github.io/tags/%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
  </entry>
  
  <entry>
    <title>我将UESTC加入到了CSRankings的列表里</title>
    <link href="http://ldzhangyx.github.io/2018/12/18/csrankings-uestc/"/>
    <id>http://ldzhangyx.github.io/2018/12/18/csrankings-uestc/</id>
    <published>2018-12-18T05:54:38.000Z</published>
    <updated>2018-12-18T06:57:53.470Z</updated>
    
    <content type="html"><![CDATA[<p>如同字面意思，我之前添加UESTC到CSRankings的Pull Request已经被Merge到主分支了。<a href="http://csrankings.org" target="_blank" rel="noopener">http://csrankings.org</a></p><a id="more"></a><h1 id="我为什么要这么做"><a href="#我为什么要这么做" class="headerlink" title="我为什么要这么做"></a>我为什么要这么做</h1><p>CSRankings本身就是一个非常流行的科研排名。我很高兴能在这样一个排名里做出贡献，帮助国内和国外的同行对这个行业的科研水准有更准确的认识。这包括，对中国科研整体水准的认识；对我的母校电子科大的科研水准有更高的估计。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>虽然现在CSRankings的评价基准是各大顶会的论文数量，固然因为人海因素导致院校之间的平均水准得不到有效的评估；但另一方面，这个排名又非常纯粹，并且详细到了每一个教授。这样的排名是客观的、不掺杂主观因素的清流榜单。</p><p>这个排名上，我国的很多高校都没有出现，如果想和我一样补完这个榜单的话，可以看下一节。</p><h1 id="我做了什么"><a href="#我做了什么" class="headerlink" title="我做了什么"></a>我做了什么</h1><p>首先，我找到了他们的<a href="https://github.com/emeryberger/CSrankings" target="_blank" rel="noopener">GitHub页面</a>。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>然后我提交了一个pull resuest，将UESTC加入了机构名单；同时将计算机学院的大部分博导加入了名单。根据要求，这个排名里只能收录博导。</p><p>最后等待大约一周，即可被merge。这期间会有修改要求，只要按照建议修改即可。</p><h1 id="我之后会怎么做"><a href="#我之后会怎么做" class="headerlink" title="我之后会怎么做"></a>我之后会怎么做</h1><p>我写了这篇文章，希望能被更多人看到，然后意识到CSRankings的价值，以及增添PR上自己学校的动力。我之后会持续增加、补全计算机学院的教授们。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如同字面意思，我之前添加UESTC到CSRankings的Pull Request已经被Merge到主分支了。&lt;a href=&quot;http://csrankings.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://csrankings.org&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="http://ldzhangyx.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
  </entry>
  
  <entry>
    <title>使用statsmodels库做线性回归的指南</title>
    <link href="http://ldzhangyx.github.io/2018/12/18/ols/"/>
    <id>http://ldzhangyx.github.io/2018/12/18/ols/</id>
    <published>2018-12-18T03:16:59.000Z</published>
    <updated>2018-12-19T02:48:08.464Z</updated>
    
    <content type="html"><![CDATA[<p>本文是一个简化的实践指南。阅读完这篇文章之后，你将可以快速使用statsmodels库进行最小二乘回归。</p><a id="more"></a><h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><p>对于一元线性回归，其回归模型是：</p><script type="math/tex; mode=display">Y = \beta_0 + \beta_1X_1</script><p>其回归系数应为：</p><p>$\beta_0, \beta_1$</p><p>对于多元线性回归，其回归模型是：</p><script type="math/tex; mode=display">Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n</script><p>其回归系数应为：</p><p>$\beta_0, \beta_1, \beta_2, …, \beta_n$</p><p>其中$\beta_0$为常数项。</p><h1 id="代码实现和解释"><a href="#代码实现和解释" class="headerlink" title="代码实现和解释"></a>代码实现和解释</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">statsmodels</span>.<span class="title">regression</span>.<span class="title">linear_model</span>.<span class="title">OLS</span><span class="params">(endog, exog=None, missing=<span class="string">'none'</span>, hasconst=None, **kwargs)</span></span></span><br></pre></td></tr></table></figure><p>我们使用statsmodels的OLS类进行初始化，其重要参数是前两个。</p><p>endog —— 因变量<br>exog —— 自变量</p><p>注意如果要实现多元线性回归的话，那么其输入应该是一个这样的矩阵：</p><script type="math/tex; mode=display">  \begin{matrix}   1 & x_{11} & x_{m1} \\   ... & ... & ... \\   1 & x_{1n} & x_{mn}  \end{matrix}</script><p>这里要注意两点：</p><ol><li>OLS类默认不提供常数项，所以需要手动添加一列常数项。statsmodels提供了方便地添加常数项的方法。</li><li>每一列都是一个自变量。所以可以使用这样的方法进行拼接：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设数据以list的方法输入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自变量</span></span><br><span class="line">x1 = list()</span><br><span class="line">x2 = list()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因变量</span></span><br><span class="line">y = list()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将列表转为横向量，转置成纵向量，然后拼接（concatenate）起来</span></span><br><span class="line">X = np.concatenate(np.array(x1).reshape(<span class="number">-1</span>,<span class="number">1</span>), </span><br><span class="line">                np.array(x1).reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在自变量矩阵X的最左侧一列添加一列常数1</span></span><br><span class="line">X = sm.add_constant(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化OLS类，得到一个model对象。</span></span><br><span class="line">model = sm.OLS(y, X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用类方法，使model对象进行拟合，然后返回结果。</span></span><br><span class="line">results = model.fit()</span><br><span class="line"></span><br><span class="line">print(results.params)</span><br><span class="line"><span class="comment"># 返回值将会是这样的形式</span></span><br><span class="line"><span class="comment"># [1.04510666, 9.97239799 2.23455534]</span></span><br><span class="line"><span class="comment"># 按照顺序分别是常数项系数、x1系数，x2系数，etc.</span></span><br><span class="line"></span><br><span class="line">print(results.summary())</span><br><span class="line"><span class="comment"># 如果调用这个方法，则会返回一个详尽的表格，其中coef就是回归系数。</span></span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://zhuanlan.zhihu.com/p/22692029" target="_blank" rel="noopener">Python Statsmodels 统计包之 OLS 回归</a></li><li><a href="http://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html#statsmodels.regression.linear_model.OLS" target="_blank" rel="noopener">statsmodels.regression.linear_model.OLS</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是一个简化的实践指南。阅读完这篇文章之后，你将可以快速使用statsmodels库进行最小二乘回归。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://ldzhangyx.github.io/tags/Python/"/>
    
      <category term="统计" scheme="http://ldzhangyx.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
      <category term="statsmodels" scheme="http://ldzhangyx.github.io/tags/statsmodels/"/>
    
  </entry>
  
  <entry>
    <title>用Python做统计的轻量化库：statistics</title>
    <link href="http://ldzhangyx.github.io/2018/12/11/stats/"/>
    <id>http://ldzhangyx.github.io/2018/12/11/stats/</id>
    <published>2018-12-11T06:08:13.000Z</published>
    <updated>2018-12-11T06:57:07.118Z</updated>
    
    <content type="html"><![CDATA[<p>Python用于统计计算的时候，有一个标准库：statistics，可以做一下基本的统计计算。</p><a id="more"></a><p>从使用的角度来看，之所以称其为轻量库，是因为其没有引入额外的数据结构，并且使用简单方便。对于样本来说，需要提供的只是序列或者迭代器，譬如可以<strong>使用一个list</strong>作为函数输入。</p><h1 id="中心位置的计算"><a href="#中心位置的计算" class="headerlink" title="中心位置的计算"></a>中心位置的计算</h1><h2 id="平均值"><a href="#平均值" class="headerlink" title="平均值"></a>平均值</h2><p>mean(), 算术平均值。<br>harmonic_mean(), 谐波平均值。</p><h2 id="中位数"><a href="#中位数" class="headerlink" title="中位数"></a>中位数</h2><p>median(), 中位数。若数据量为偶数时，会取一个平均值作为插值中位数。<br>median_low(), 中位数，但数据为偶数时，会取较小的那个。<br>median_high(), 取较大的那个。<br>median_grouped( , interval = 1), 这个功能比较迷，将输入视为连续数据，并通过优先使用提供的间隔宽度找到中值范围，然后使用落在该范围内的数据集中的实际值的位置在该范围内插值来计算 50％ 百分位中值。interval取不同值是结果会改变。</p><h2 id="众数"><a href="#众数" class="headerlink" title="众数"></a>众数</h2><p>mode(), 要是数据不存在众数会报错，嗯……</p><h1 id="偏离值计算"><a href="#偏离值计算" class="headerlink" title="偏离值计算"></a>偏离值计算</h1><p>pstdev( , mu = None), 计算<strong>总体(population)</strong>标准差。<br>pvariance( , mu = None), 计算<strong>总体</strong>方差，当然就是标准差的平方。第二个参数mu可选，你可以先验地给一个总体平均值，也可以让算法自己算样本平均值。<br>对于这个函数，当其对整体调用的时候，自然可以计算出总体方差$\sigma$，但输入是样本的话，其返回应该是$S^2$，即有偏的样本方差（有N个自由度）。此时样本方差与总体方差可能不一致。所以当我们知道总体平均值mu时可以提供给函数，如果数据点有代表性，结果应该是无偏估计。</p><p>stdev(, xbar = None), 计算样本标准差。<br>variance( , xbar = None), 计算样本方差。<br>这个函数的样本方差使用了贝塞尔矫正（只有N-1个自由度）。结果应该是真实总体方差的无偏估计。</p><p>贝塞尔矫正：</p><p><script type="math/tex">s=\sqrt{\frac{^{\Sigma (x-\bar{x})^2}}{n-1}}</script> 表示样本方差。</p><p>因为高斯分布的边沿抽取的数据很少，那么预测方差一定小于大数据集的方差。通过分母减一，手动提高方差的数值。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python用于统计计算的时候，有一个标准库：statistics，可以做一下基本的统计计算。&lt;/p&gt;
    
    </summary>
    
      <category term="数据挖掘" scheme="http://ldzhangyx.github.io/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="Python" scheme="http://ldzhangyx.github.io/tags/Python/"/>
    
      <category term="统计" scheme="http://ldzhangyx.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
      <category term="数据挖掘" scheme="http://ldzhangyx.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="时序分析" scheme="http://ldzhangyx.github.io/tags/%E6%97%B6%E5%BA%8F%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch第三方库：PyTorch-NLP</title>
    <link href="http://ldzhangyx.github.io/2018/11/22/pytorch-nlp/"/>
    <id>http://ldzhangyx.github.io/2018/11/22/pytorch-nlp/</id>
    <published>2018-11-22T11:41:40.000Z</published>
    <updated>2018-11-22T15:12:05.524Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch-NLP在功能上与torchtext很像，但是其仍然有颇具特色的地方。</p><a id="more"></a><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>选择pip安装。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pytorch-nlp</span><br></pre></td></tr></table></figure><p>安装过程中你可能会遇到如下的坑：</p><blockquote><p>fatal error C1083: Cannot open include file: ‘io.h’: No such file or directory</p></blockquote><p>解决办法是安装Visual Studio一些相关的工具(<a href="https://stackoverflow.com/questions/40018405/cython-cannot-open-include-file-io-h-no-such-file-or-directory/40810172)。" target="_blank" rel="noopener">https://stackoverflow.com/questions/40018405/cython-cannot-open-include-file-io-h-no-such-file-or-directory/40810172)。</a></p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>基本的使用方法在<a href="https://pytorchnlp.readthedocs.io/en/latest/" target="_blank" rel="noopener">官方文档</a>里说得比较清楚，主要分为这几个用处。</p><h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><p>PyTorch-NLP实现了自己的Dataset的子类，直接调用即可。这些类已经好好地实现了<strong>len</strong>()和<strong>getitem</strong>()功能，我们只需要在自己的程序里写一个DataLoader用就好了。以WMT数据集举例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torchnlp.datasets <span class="keyword">import</span> wmt_dataset</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>train = wmt_dataset(train=<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>train[:<span class="number">2</span>]</span><br><span class="line">[&#123;</span><br><span class="line">  <span class="string">'en'</span>: <span class="string">'Res@@ um@@ ption of the session'</span>,</span><br><span class="line">  <span class="string">'de'</span>: <span class="string">'Wiederaufnahme der Sitzungsperiode'</span></span><br><span class="line">&#125;, &#123;</span><br><span class="line">  <span class="string">'en'</span>: <span class="string">'I declare resumed the session of the European Parliament ad@@ jour@@ ned on...'</span></span><br><span class="line">  <span class="string">'de'</span>: <span class="string">'Ich erklär@@ e die am Freitag , dem 17. Dezember unterbro@@ ch@@ ene...'</span></span><br><span class="line">&#125;]</span><br></pre></td></tr></table></figure><h2 id="使用预训练好的词向量"><a href="#使用预训练好的词向量" class="headerlink" title="使用预训练好的词向量"></a>使用预训练好的词向量</h2><p>在BERT出来之前最好的词向量应该是FastText+ELMo的组合。如果想偷懒先快速实现模型的话，这个API可以帮你免去写词表、写Embedding层的过程，直接返回词向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torchnlp.word_to_vector <span class="keyword">import</span> FastText</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vectors = FastText()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vectors[<span class="string">'hello'</span>]</span><br><span class="line"><span class="number">-1.7494</span></span><br><span class="line"><span class="number">0.6242</span></span><br><span class="line">...</span><br><span class="line"><span class="number">-0.6202</span></span><br><span class="line"><span class="number">2.0928</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">100</span>]</span><br></pre></td></tr></table></figure><h2 id="使用丰富的神经网络层"><a href="#使用丰富的神经网络层" class="headerlink" title="使用丰富的神经网络层"></a>使用丰富的神经网络层</h2><p>我觉得比较有名的大概是SRU层和Attention层。SRU层号称将RNN训练得和CNN一样快，而Attention的实现使用了IBM的类似方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>attention = Attention(<span class="number">256</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>query = torch.randn(<span class="number">5</span>, <span class="number">1</span>, <span class="number">256</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>context = torch.randn(<span class="number">5</span>, <span class="number">5</span>, <span class="number">256</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, weights = attention(query, context)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.size()</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">1</span>, <span class="number">256</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>weights.size()</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">1</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>在这里关于attention的使用，可以看<a href="https://ibm.github.io/pytorch-seq2seq/public/models.html#module-seq2seq.models.attention" target="_blank" rel="noopener">IBM的文档</a>。简单解释一下呢，就是context是encoder的hidden states，其与output做了相似度计算，softmax之后参与decode过程，能理解self-attention的对这个肯定能理解。</p><h2 id="Text-Encoder"><a href="#Text-Encoder" class="headerlink" title="Text Encoder"></a>Text Encoder</h2><p>是Tensor2Tensor一部分功能的迁移实现，TextEncoder主要功能就是构建词汇表、实现符号到id的映射。Tensor2Tensor的GitHub可以点<a href="https://github.com/tensorflow/tensor2tensor/blob/master/README.md" target="_blank" rel="noopener">这里</a>。</p><p>实话说，Google这个项目做得挺漂亮的，外行人可以用一用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchnlp.text_encoders <span class="keyword">import</span> WhitespaceEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a `WhitespaceEncoder` with a corpus of text</span></span><br><span class="line">encoder = WhitespaceEncoder([<span class="string">"now this ain't funny"</span>, <span class="string">"so don't you dare laugh"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode and decode phrases</span></span><br><span class="line">encoder.encode(<span class="string">"this ain't funny."</span>) <span class="comment"># RETURNS: torch.LongTensor([6, 7, 1])</span></span><br><span class="line">encoder.decode(encoder.encode(<span class="string">"This ain't funny."</span>)) <span class="comment"># RETURNS: "this ain't funny."</span></span><br></pre></td></tr></table></figure><h2 id="数据采样"><a href="#数据采样" class="headerlink" title="数据采样"></a>数据采样</h2><p>sampler可以从数据集中进行采样，只需提供数据集对象，制定顺序即可。</p><p>我目前没有用到过这个API，但是它应该是一个很好用的采样工具，可以用于做验证。</p><h2 id="进行评测"><a href="#进行评测" class="headerlink" title="进行评测"></a>进行评测</h2><p>令人遗憾的是这里只实现了两个评测：精确度和BLEU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torchnlp.metrics <span class="keyword">import</span> get_accuracy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>targets = torch.LongTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>outputs = torch.LongTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>accuracy, n_correct, n_total = get_accuracy(targets, outputs, ignore_index=<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>accuracy</span><br><span class="line"><span class="number">0.8</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>n_correct</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>n_total</span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure><p>其中ignore_index表示忽略的index。</p><p>至于BLEU就更简单了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>hypotheses = [</span><br><span class="line">    <span class="string">"The brown fox jumps over the dog 笑"</span>,</span><br><span class="line">    <span class="string">"The brown fox jumps over the dog 2 笑"</span></span><br><span class="line">]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>references = [</span><br><span class="line">    <span class="string">"The quick brown fox jumps over the lazy dog 笑"</span>,</span><br><span class="line">    <span class="string">"The quick brown fox jumps over the lazy dog 笑"</span></span><br><span class="line">]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>get_moses_multi_bleu(hypotheses, references, lowercase=<span class="keyword">True</span>)</span><br><span class="line"><span class="number">46.51</span></span><br></pre></td></tr></table></figure><p>两个list传进去，自动给出结果。还是挺方便的。</p><h2 id="工具包"><a href="#工具包" class="headerlink" title="工具包"></a>工具包</h2><p>有一些很有意思的工具可以用：</p><ul><li>对一个batch做padding</li><li>对一个tensor做padding</li><li>分割数据集</li><li>shuffle</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch-NLP在功能上与torchtext很像，但是其仍然有颇具特色的地方。&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
      <category term="NLP" scheme="http://ldzhangyx.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>几类VAE的简要整理</title>
    <link href="http://ldzhangyx.github.io/2018/11/20/vae/"/>
    <id>http://ldzhangyx.github.io/2018/11/20/vae/</id>
    <published>2018-11-20T05:25:32.000Z</published>
    <updated>2018-11-20T11:12:16.315Z</updated>
    
    <content type="html"><![CDATA[<p>VAE在我心里，是NLP领域生成模型的一把好手。在很多Task上，VAE的表现其实不如RNN，但是其出色的地方在于足够灵活，能生成出很多不一样的符合条件的结果。这么几年下来，VAE也有着各种各样的变体，也会顺便提一下。</p><a id="more"></a><h2 id="VAE-amp-Conditional-VAE"><a href="#VAE-amp-Conditional-VAE" class="headerlink" title="VAE &amp; Conditional VAE"></a>VAE &amp; Conditional VAE</h2><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>首先来看VAE，其比较详细的解释可以看<a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">这里</a>。VAE的模型结构如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其Encoder提取输入的特征，并将其map到一个正态分布上。用神经网络输出其均值和方差，得到一个正态分布。这个正态分布体现了输入的特征。编码器不能自由地使用整个潜在空间，而是必须限制产生的z，使其可能服从先验分布。</p><p>对正态分布进行采样，采样数据通过Decoder进行输出。采样可以通过重参数技巧进行代换，保证了整个过程可导，绕开了采样这个不可导的动作。</p><p>为了保证模型具有一定的生成能力，输出的方差需要进行控制，所以要求这个正态分布向标准正态分布看齐。</p><p>使用KL散度衡量这个分布与标准正态分布的距离，其Loss可以推导为下列式子：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>LOSS = KL散度 + 生成图片与原图片的交叉熵</p><p>之后ELBO的提出都是为了方便计算。VAE 的变分下界，是直接基于 KL 散度就得到的。所以直接承认了 KL 散度的话，就没有变分的什么事了<a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">参考</a>。，大体模型就是这个样子。</p><h3 id="CVAE"><a href="#CVAE" class="headerlink" title="CVAE"></a>CVAE</h3><p>CVAE是一类模型，通过在Encoder与Decoder中添加标签来控制生成结果的类别。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其实现方法是，每一类label，其正态分布存在均值，于是Encoder编码形成的正态分布，不仅方差要接近标准正态分布，而且均值要接近label的均值，可以通过修改KL散度实现：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="AAE"><a href="#AAE" class="headerlink" title="AAE"></a>AAE</h2><p>AAE，对抗自编码器，在2015年于《Adversial Autoencoders》提出来。其详细介绍在<a href="http://kissg.me/2017/12/17/papernotes03/" target="_blank" rel="noopener">这里</a>可以查看。</p><p>其与VAE最显著的特征是将KL散度惩罚替换成了对抗学习的过程。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;VAE在我心里，是NLP领域生成模型的一把好手。在很多Task上，VAE的表现其实不如RNN，但是其出色的地方在于足够灵活，能生成出很多不一样的符合条件的结果。这么几年下来，VAE也有着各种各样的变体，也会顺便提一下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="变分自编码器" scheme="http://ldzhangyx.github.io/tags/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
      <category term="GAN" scheme="http://ldzhangyx.github.io/tags/GAN/"/>
    
      <category term="VAE" scheme="http://ldzhangyx.github.io/tags/VAE/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch代码编写的日常(11.19)</title>
    <link href="http://ldzhangyx.github.io/2018/11/19/pytorch-2/"/>
    <id>http://ldzhangyx.github.io/2018/11/19/pytorch-2/</id>
    <published>2018-11-19T07:23:51.000Z</published>
    <updated>2018-11-20T04:48:59.869Z</updated>
    
    <content type="html"><![CDATA[<p>这个系列主要讨论使用PyTorch我个人探寻到的零散知识点，包括API和编写范式。</p><a id="more"></a><h1 id="k折交叉验证"><a href="#k折交叉验证" class="headerlink" title="k折交叉验证"></a>k折交叉验证</h1><p>我找了一圈，发现PyTorch在对数据集的处理上，并没有设置方便进行交叉验证的API。在编程实践中可以这么完成k折交叉验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    数据集类继承PyTorch的Dataset类，进行处理。</span></span><br><span class="line"><span class="string">    数据集以csv读取为例。</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, state = <span class="string">'Train'</span>, k = <span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">'''初始化对象。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            root: 存放数据的目录。</span></span><br><span class="line"><span class="string">            state: 控制要读取训练集、验证集、测试集。</span></span><br><span class="line"><span class="string">            k: 用于控制k折交叉验证。</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.root = root </span><br><span class="line">        self.state = state</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">'Train'</span>:</span><br><span class="line">            path = root + <span class="string">'train.csv'</span></span><br><span class="line">            self.train_data = pd.read_csv(path)</span><br><span class="line">            self.data = pd.concat(</span><br><span class="line">                [self.train_data[:int((k % <span class="number">10</span>) * len(self.train_data) / <span class="number">10</span>)], </span><br><span class="line">                 self.train_data[int((k % <span class="number">10</span> + <span class="number">1</span>) * len(self.train_data) / <span class="number">10</span>):]])</span><br><span class="line">            self.data = self.data.reset_index(drop = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">'Valid'</span>:</span><br><span class="line">            path = root + <span class="string">'train.csv'</span></span><br><span class="line">            self.train_data = pd.read_csv(path)</span><br><span class="line">            self.data = self.train_data[int((k % <span class="number">10</span>) * len(self.train_data) /<span class="number">10</span>):</span><br><span class="line">                                        int((k % <span class="number">10</span> + <span class="number">1</span>) * len(self.train_data) /<span class="number">10</span>)]</span><br><span class="line">            self.data = self.data.reset_index(drop = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.state == <span class="string">'test'</span>:</span><br><span class="line">            path = root + <span class="string">'test.csv'</span></span><br><span class="line">            self.data = pd.read_csv(path)</span><br></pre></td></tr></table></figure><p>这样通过切片的方式，将训练集和验证集分成了k份，训练集拥有k-1份数据。</p><h1 id="loss的设计"><a href="#loss的设计" class="headerlink" title="loss的设计"></a>loss的设计</h1><p>回归问题一般来说使用MSE，但是PyTorch也提供了Huber Loss，在API里命名为SmoothL1Loss。</p><p>使用loss时遵循这样的方法使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.SmoothL1Loss()</span><br><span class="line">logits = ...</span><br><span class="line">labels = ...</span><br><span class="line">output = loss(logits, labels)</span><br></pre></td></tr></table></figure><h1 id="loss与optim的联动关系"><a href="#loss与optim的联动关系" class="headerlink" title="loss与optim的联动关系"></a>loss与optim的联动关系</h1><p>在optimizer定义的时候，需要将网络参数输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer = t.optim.Adam(model.learnable_parameters(), </span><br><span class="line">                        parameters.learning_rate,</span><br><span class="line">                        weight_decay=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>learnable_parameters()是我手动定义的一个类方法，根据requires_grad标志获取所有需要学习的参数。</p><p>此时optimizer已经存储了所有参数的储存地址（也就是指针）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line">output = loss(logits, label)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">output.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p>optimizer.zero_grad()将梯度清零，防止梯度错误累加。<br>output.backward()将所有参数的更新梯度一次性求出来。<br>optimizer.step()根据求得的梯度自动进行更新，这个函数无需传参，因为loss与optim被设计为能很好联动的机制。</p><h1 id="model-train-与model-eval"><a href="#model-train-与model-eval" class="headerlink" title="model.train()与model.eval()"></a>model.train()与model.eval()</h1><p>这个机制用来控制dropout和Batch Normalize层。BN层的主要用途是进行批量正规化，减少不当初始化对学习效果的负面影响。</p><p>在训练代码前加入model.train()，在验证和测试代码前加入model.eval()，model为模型对象。</p><h1 id="requires-grad的默认值"><a href="#requires-grad的默认值" class="headerlink" title="requires_grad的默认值"></a>requires_grad的默认值</h1><p>PyTorch 0.4版本取消了Tensor的参数Volatile，现在requires_grad用于控制参数是否能被学习，默认为False。</p><p>但是重要的是，在模型的<strong>init</strong>方法中被定义的层，<strong>这个参数默认为True</strong>，不需要再改一遍了。</p><h1 id="trainer的编写范式"><a href="#trainer的编写范式" class="headerlink" title="trainer的编写范式"></a>trainer的编写范式</h1><p>trainer作为网络模型的类方法出现，其定义和调用方法如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainer</span><span class="params">(self, optimizer)</span>:</span> <span class="comment"># 如果自己定义了batch_loader也可以在这里用。这里主要定义一些不会变的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(iteration, batch_size, use_cuda, input, label)</span>:</span></span><br><span class="line">        logits = self(input) <span class="comment"># 这里调用的是forward类方法，保持参数一致即可</span></span><br><span class="line">        logits = logits.view(<span class="number">-1</span>,<span class="number">1</span>) <span class="comment"># 可能需要做一次转置</span></span><br><span class="line">        loss = nn.MSELoss()</span><br><span class="line">        mse = loss(logits, label)</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> mse</span><br><span class="line">    <span class="keyword">return</span> train <span class="comment"># 返回一个函数对象</span></span><br></pre></td></tr></table></figure><p>调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> iteration, [input, label] <span class="keyword">in</span> enumerate(data_loader): <span class="comment"># data_loader是自己写好的，是DataLoader类</span></span><br><span class="line">    train_step = model.trainer(optimizer)</span><br><span class="line"></span><br><span class="line">    output = step(iteration, batch_size, cuda, input, label) <span class="comment"># 这是目的</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个系列主要讨论使用PyTorch我个人探寻到的零散知识点，包括API和编写范式。&lt;/p&gt;
    
    </summary>
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/categories/PyTorch/"/>
    
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch模型加载与保存的最佳实践</title>
    <link href="http://ldzhangyx.github.io/2018/11/19/pytorch-1119/"/>
    <id>http://ldzhangyx.github.io/2018/11/19/pytorch-1119/</id>
    <published>2018-11-18T16:57:03.000Z</published>
    <updated>2018-11-19T05:38:10.161Z</updated>
    
    <content type="html"><![CDATA[<p>一般来说PyTorch有两种保存和读取模型参数的方法。但这篇文章我记录了一种最佳实践，可以在加载模型时避免掉一些问题。</p><a id="more"></a><h1 id="传统方案："><a href="#传统方案：" class="headerlink" title="传统方案："></a>传统方案：</h1><p>第一种方案是保存整个模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model_object, <span class="string">'model.pth'</span>)</span><br></pre></td></tr></table></figure><p>第二种方法是保存模型网络参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model_object.state_dict(), <span class="string">'params.pth'</span>)</span><br></pre></td></tr></table></figure><p>加载的时候分别这样加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(<span class="string">'model.pth'</span>)</span><br></pre></td></tr></table></figure><p>以及：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_object.load_state_dict(torch.load(<span class="string">'params.pth'</span>))</span><br></pre></td></tr></table></figure><h1 id="改进的方案"><a href="#改进的方案" class="headerlink" title="改进的方案"></a>改进的方案</h1><p>注意到这个方案是因为模型在加载之后，loss会飙升之后再慢慢降回来。查阅有关分析之后，判定是优化器optimizer的问题。</p><p>如果模型的保存是<strong>为了恢复训练状态</strong>，那么可以考虑同时保存优化器optimizer的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">state = &#123;</span><br><span class="line">    <span class="string">'epoch'</span>: epoch,</span><br><span class="line">    <span class="string">'net'</span>: model.state_dict(),</span><br><span class="line">    <span class="string">'optimizer'</span>: optimizer.state_dict(),</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">torch.save(state, filepath)</span><br></pre></td></tr></table></figure><p>然后这样加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = torch.load(model_path)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">'net'</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">'optimizer'</span>])</span><br><span class="line">start_epoch =  checkpoint[<span class="string">'epoch'</span>] + <span class="number">1</span></span><br></pre></td></tr></table></figure><p>如果模型的保存是为了<strong>方便以后进行validation和test</strong>，可以在加载完之后制定model.eval()固定dropout和BN层。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般来说PyTorch有两种保存和读取模型参数的方法。但这篇文章我记录了一种最佳实践，可以在加载模型时避免掉一些问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>《Joint Learning of Character and Word Embeddings》论文笔记，以及Word Embedding闲聊</title>
    <link href="http://ldzhangyx.github.io/2018/11/17/character-embedding/"/>
    <id>http://ldzhangyx.github.io/2018/11/17/character-embedding/</id>
    <published>2018-11-17T15:44:16.000Z</published>
    <updated>2018-11-18T15:09:04.967Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章出自刘知远老师的组，其核心思想是利用词语丰富的内部含义，提升词语进行embedding的效果。文章是在2015年发表的，比较久远了。在这篇文章里我想同时聊一下最近Word Embedding的有趣的地方。</p><a id="more"></a><h1 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h1><p>先聊聊Character Embedding的前置模型——CBOW。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>CBOW在训练的时候，是利用一个单词的context去预测这个具体的词，而Skip-Gram模型根据中心词预测context。如下图所示，我们设每个单词的one-hot编码拥有$V$个维度，其上下文窗口大小有$C$。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>这样上下文的输入可以用一个$[C* V]$的矩阵表示。</p><p>之后，我们使用一个共享的权重矩阵$W_{V*N}$，将上下文映射成一个$[C*N]$维的矩阵。接下来对这个矩阵求平均得到$[1*N]$维的向量，作为隐层向量。</p><p>我们使用另一个输出权重矩阵$W’_{N*V}$，将这个$[1*N]$维的向量映射为$[1*V]$维的向量。将结果进行softmax处理，得到概率输出。训练目标是将这个输出与单词的one-hot编码尽量接近。</p><p>训练完成之后，我们得到了一个副产物：矩阵$W$。这个矩阵被称为look up table，任何一个单词的one-hot表示乘以这个矩阵都将得到自己的word embedding。</p><h1 id="Character-Embedding"><a href="#Character-Embedding" class="headerlink" title="Character Embedding"></a>Character Embedding</h1><p>这篇文章试图利用中文词语的内部结构（从直觉上来看，中文字单个字的意思和拉丁词的词根词缀有着一样的感觉），来丰富词语的语义。有一篇后续论文《Enriching Word Vectors with Subword Information》，试图在英语词汇上进行类似的研究。那篇文章试图将单词以词袋的方式联合表示，如：where被表示为&lt;wh,whe,her,ere,re,where&gt;。使用这种方式，单词的内在结构可以被很好地利用起来，同时可以强化那些词频低下的词的表示准确度。</p><p>中文embedding中也有利用更细粒度的嵌入，比如《Component-Enhanced Chinese Character Embeddings》提到使用部首表示更加丰富的信息。</p><h2 id="大致模型"><a href="#大致模型" class="headerlink" title="大致模型"></a>大致模型</h2><p>CWE（CCharacter-enhanced Word Embedding）的大致模型如下图所示。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>有几种方式去进行Character Embedding和Word Embedding的组合。总体公式如下：</p><script type="math/tex; mode=display">x_i = w_j \oplus \frac{1}{N_j}\sum_{k=1}^{N_j}c_k</script><p>第一种加和的方法是concatation，第二种加和的方法是将两个向量直接相加。实验表明直接相加的速度会快于前者。</p><p>限于一些词语（如巧克力），其单字并不能帮助词语丰富语义，所以作者决定将这类词语排除在外，只使用其word embedding。</p><p>为了保证这类词与加和了character embedding的词语有相似的长度，作者将上面得到的向量乘以1/2，进行处理。</p><script type="math/tex; mode=display">x_i = \frac{1}{2}(w_j + \frac{1}{N_j}\sum_{k=1}^{N_j}c_k)</script><h2 id="两个困难"><a href="#两个困难" class="headerlink" title="两个困难"></a>两个困难</h2><p>第一个困难是：与词相比, 汉字拥有的意思更多. 一个汉字在不同词中, 可能有不同的语义. 因此, 一个汉字对应一个 character embedding 是不够的。</p><p>作者的对策是：为一个汉字分配多个 character embedding, 以表示它的不同意思。</p><p>第二个困难是：不是所有词都能用汉字意思的语义结合来表意（如巧克力）。</p><p>作者的对策是：将这类词单独取出，不做character-embedding。</p><h2 id="解决一词多义现象"><a href="#解决一词多义现象" class="headerlink" title="解决一词多义现象"></a>解决一词多义现象</h2><p>第一个方案：针对character在word中的位置为其分配向量，分为3种情况：词头、词中和词尾。具体地说，<strong>能力</strong>和<strong>智能</strong>中的能用不同的向量表示。这种方法被称为<strong>Position-based Character Embeddings</strong>。</p><p>第二个方案：使用聚类。对character出现的情况做聚类，根据聚类结果创建不同的向量，称为模式向量。这个方法被称为<strong>cluster-based character embeddings</strong>。这个方法能与第一个结合起来，position-based character embeddings再做聚类。</p><p>第三个方案：无参聚类。针对第二种方法的聚类方法，第三个方案所得到的类别不具有固定的类数，而在训练中灵活地习得。</p><h1 id="其他模型（等待后续更新）"><a href="#其他模型（等待后续更新）" class="headerlink" title="其他模型（等待后续更新）"></a>其他模型（等待后续更新）</h1><h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>最近被BERT屠榜，word embedding要进入大开大合的时代了吗？我没有专门去看模型的实现，只知道它使用了海量参数，运用Transformer进行学习；但是看<a href="https://www.zhihu.com/question/298203515/answer/509470502" target="_blank" rel="noopener">吴俣的知乎回答</a>表示无法复现，所以我也暂时没有考虑深入思考这个方向。</p><p>但是说到word embedding，有一些很好的先验知识能让我们更容易学到精确的词向量。比如说，基于模式向量进行word embedding在文言文翻译任务上也许能提高任务结果；中文字的结构和内部含义的确是有用的信息；也许Analogical Reasoning这个衡量词向量质量的方面，传统的符号推理作为先验，也能帮助我们更好地学到词向量。</p><p>无论如何，在大多数任务上，只要挑一个自己觉得ok的词向量，我认为就可以了。如果之后要对词向量进行单独的训练，可以慢慢调整。这个词向量现在是GloVe_300d + ELMo，以后是BERT，这个广阔的领域等我以后有机会再深入学习吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章出自刘知远老师的组，其核心思想是利用词语丰富的内部含义，提升词语进行embedding的效果。文章是在2015年发表的，比较久远了。在这篇文章里我想同时聊一下最近Word Embedding的有趣的地方。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Word Embedding" scheme="http://ldzhangyx.github.io/tags/Word-Embedding/"/>
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="中文NLP" scheme="http://ldzhangyx.github.io/tags/%E4%B8%AD%E6%96%87NLP/"/>
    
  </entry>
  
  <entry>
    <title>Focal Loss以及其在NLP领域运用的思考</title>
    <link href="http://ldzhangyx.github.io/2018/11/16/focal-loss/"/>
    <id>http://ldzhangyx.github.io/2018/11/16/focal-loss/</id>
    <published>2018-11-15T16:06:53.000Z</published>
    <updated>2018-11-18T10:52:01.713Z</updated>
    
    <content type="html"><![CDATA[<p>Kaiming He的一篇《Focal Loss for Dense Object Detection》是我一直想读的一篇文章。这篇文章将Focal Loss用于目标检测，然而其在NLP中也能得到运用。</p><a id="more"></a><h1 id="Focal-Loss的概念和公式"><a href="#Focal-Loss的概念和公式" class="headerlink" title="Focal Loss的概念和公式"></a>Focal Loss的概念和公式</h1><h2 id="为什么Focal-Loss要出现"><a href="#为什么Focal-Loss要出现" class="headerlink" title="为什么Focal Loss要出现"></a>为什么Focal Loss要出现</h2><p>Focal Loss的出现是为了解决训练集正负样本极度不平衡的情况。作者认为更少的部分学习起来更难，于是在Focal Loss上设计得更加倾向于这一类样本。</p><p>作者提出通过reshape标准交叉熵损失解决类别不均衡（Class Imbalance）,这样它就能降低容易分类的样例的比重（Well-classified Examples）。这个方法专注训练在Hard Examples的稀疏集合上，能够防止大量的Easy Negatives在训练中压倒训练器。</p><p>以前的一些思路集中在采样样本，使更少（等同于更难训练）的那类样本在数据中所占的比重更大，Focal Loss并没有进行采样，而是对容易的那类进行了降权处理，结果上有很好的效果。</p><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>首先将分类问题常用的交叉熵公式摆出来。</p><script type="math/tex; mode=display">CE(p, y) = \begin {cases}& -\log(p)  & \text{if} y = 1 \\ & -\log(1-p) &  \text{otherwise}\\\end {cases}</script><p>公式里的CE指的是交叉熵，$p$指的是期望输出，也就是预测样本属于1的概率。$y$表示label，取值为${±1}$。于是在这个二分类问题里，我们可以自然地将其合并为一个公式：</p><script type="math/tex; mode=display">CE(p,y) = -\frac{1}{n}\sum_{x}[y\log(p)+(1-y)\log(1-p)]</script><p>其中n为训练输入的个数，当然这里$y \in {1,0}$。对交叉熵的基础细节有模糊之处可以看<a href="https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s1.html" target="_blank" rel="noopener">这篇博客</a>。</p><p>定义变量$p_t$，使得：</p><script type="math/tex; mode=display">p_t = \begin {cases}& p  & \text{if} y = 1 \\ & p &  \text{otherwise}\\\end {cases}</script><p>所以$CE(p,y)$可以被统一为一个式子：$-\log(p_t)$，这样$p_t$实际上就是反映了$p$与$y$的接近程度。$p_t$越大，说明分类越好。</p><p>之后文章提出了平衡的交叉熵，使用一个$\alpha_t$来平衡交叉熵。这个因子的作用是手动平衡不同样本的交叉熵，控制正负样本对总loss的权重。</p><p>训练时遇到很大的类别不平衡会主导交叉熵损失。易分负样本在梯度和损失中占据主导地位。而$\alpha$平衡了正负样本的重要性，它不会区别易分样本和难分样本。与之不同，作者将损失函数变形降低易分样本的权重，专注于训练难分负样本。</p><p>现在在交叉熵上加入另一个因子，用于动态调节权重：</p><script type="math/tex; mode=display">FL(p_t) = -(1-p_t)^\gamma\log(p_t)</script><p>FL即为Focal Loss，$\gamma$是一个可以调节的参数，当这个参数不同时，对loss的影响如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其中参数为0的时候，Focal Loss退化为交叉熵CE。从图中我们可以很直观地看到，$p_t$越大，$FL(p_t)$越小，其对总体loss所做的贡献就越小；反过来说，$p_t$越小（小于0.5的情况也就是被误分类），越能反映在总体loss上。实验发现$\gamma=2$时效果最好，在这种情况下，一个样本被分类的$P_t=0.9$的损失比CE小1000多倍。这样就增加了那些误分类的重要性。</p><p>作者最后将$\alpha$-balanced loss也加入了公式，最后公式为：</p><script type="math/tex; mode=display">FL(p_t) = -\alpha_t(1-p_t)^\gamma\log(p_t)</script><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>Focal Loss完全是一个通用性的Loss，面对样本不平衡的情况不失为一个好选择。</p><p>在文本分类上，我认为Focal Loss可以成为一个自然的选择。苏剑林在<a href="https://kexue.fm/archives/4293" target="_blank" rel="noopener">他的文章</a>中提到了关注于模棱两可的样本，而少关注已经分类得很好的样本，从结果上看，其应对<strong>更难分类的样本</strong>的能力的确提升了。</p><p>Focal Loss在原论文中用于提升目标检测的性能，因为目标检测的正样本数量的确远小于负样本。在自然语言处理方面也有一些任务面临着相似的问题。比如说抽取式文本摘要。一篇上千字的文本摘要，需要按照要求抽取出其中几句话，这里正负样本明显是很不平衡的；在生成式摘要中，Pointer Network试图在原文中找寻词语进行生成，这里并不能算一个严格的分类问题，但是交叉熵的损失函数仍然是可以研究一下的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kaiming He的一篇《Focal Loss for Dense Object Detection》是我一直想读的一篇文章。这篇文章将Focal Loss用于目标检测，然而其在NLP中也能得到运用。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="focal loss" scheme="http://ldzhangyx.github.io/tags/focal-loss/"/>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch工程中，提升代码体验的几点想法</title>
    <link href="http://ldzhangyx.github.io/2018/11/15/python-init/"/>
    <id>http://ldzhangyx.github.io/2018/11/15/python-init/</id>
    <published>2018-11-15T09:20:34.192Z</published>
    <updated>2018-11-16T05:36:17.636Z</updated>
    
    <content type="html"><![CDATA[<p>提及PyTorch的代码写作，我没有找到一个大家约定俗成的规约。我自己遵循了一些能让Python代码更具有可读性和持续开发的规则，现记录如下。</p><a id="more"></a><h2 id="保持工程的模块化"><a href="#保持工程的模块化" class="headerlink" title="保持工程的模块化"></a>保持工程的模块化</h2><p>一个我认为整洁的项目应该遵循这样的结构，现在我以一个Encoder-Decoder框架为例进行描述。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''The structure of my project.</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">+--data</span><br><span class="line">|--+log</span><br><span class="line">|--+dataset</span><br><span class="line">|  |  +--train.txt</span><br><span class="line">|  |  +--test.txt</span><br><span class="line">+--model</span><br><span class="line">|  +--__init__.py</span><br><span class="line">|  +--encoder.py</span><br><span class="line">|  +--decoder.py</span><br><span class="line">|  +--(model_name).py</span><br><span class="line">|  +--highway.py</span><br><span class="line">|  ...</span><br><span class="line">+--utils</span><br><span class="line">|  +--__init__.py</span><br><span class="line">|  +--batch_loader.py</span><br><span class="line">|  +--parameters.py</span><br><span class="line">|  +--functional.py</span><br><span class="line">|  ...</span><br><span class="line">+--README.md</span><br><span class="line">+--LICENSE</span><br><span class="line">+--test.py</span><br><span class="line">+--train.py</span><br></pre></td></tr></table></figure><p>在data文件夹里，放置数据集和模型，以及log输出；<br>在model文件夹里，将模型尽可能模块化，并用一个顶层模块将子模块组合起来。这个顶层模块要实现训练与测试的类方法。<br>在utils里放置参数列表文件，小工具，以及对数据预处理的文件。（我曾见过有用Jupyter Notebook进行可视化的数据清理的做法）<br>如果要开源，请加入README.md和LICENSE，一般我会选择GPL v3的License。<br>在test.py和train.py里实例化模型，进行训练和测试。</p><p>一个可以参照的GitHub开源项目可以点这里：<a href="https://github.com/kefirski/pytorch_RVAE" target="_blank" rel="noopener">https://github.com/kefirski/pytorch_RVAE</a></p><h2 id="写一个好的注释"><a href="#写一个好的注释" class="headerlink" title="写一个好的注释"></a>写一个好的注释</h2><p>Google曾经发布过一个<a href="https://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/" target="_blank" rel="noopener">代码风格指南</a>，里面详细地介绍了Python代码应当遵循的有用的规范。在里面的函数一节里，就提及了注释的规范。</p><p>对于文档注释，文档字符串是包, 模块, 类或函数里的第一个语句。这些字符串可以通过对象的<strong>doc</strong>成员被自动提取, 并且被pydoc所用。根据<a href="https://www.python.org/dev/peps/pep-0257/" target="_blank" rel="noopener">PEP-257</a>(这也是一个非常有名的指南)，文档字符串使用的注释风格应当是成对的三个双引号。</p><p>格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Add</span><span class="params">(arg_1, arg_2)</span>:</span></span><br><span class="line">    <span class="string">"""Add two numbers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    There should be some details if needed.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        arg_1: The first number to be added.</span></span><br><span class="line"><span class="string">        arg_2: The second number to be added.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A number. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">        ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>如果是书写一个类，那么在类声明的下一行应该对其<strong>所有的类成员Attributes</strong>做出解释。</p><h2 id="在容易混淆的函数声明中使用类型约束"><a href="#在容易混淆的函数声明中使用类型约束" class="headerlink" title="在容易混淆的函数声明中使用类型约束"></a>在容易混淆的函数声明中使用类型约束</h2><p>这是一点个人的看法。<a href="https://www.python.org/dev/peps/pep-0484/" target="_blank" rel="noopener">PEP-484</a>文档对类型约束提出了规范。在必要的时候使用类型约束能让我在运行之前就能发现一些问题。</p><p>不加类型约束：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greetings</span><span class="params">(name_list)</span>:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><p>加入类型约束：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greetings</span><span class="params">(name_list: List<span class="params">(str)</span>)</span> -&gt; str:</span></span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><h2 id="使用数据类装饰器（Python-3-7特性）"><a href="#使用数据类装饰器（Python-3-7特性）" class="headerlink" title="使用数据类装饰器（Python 3.7特性）"></a>使用数据类装饰器（Python 3.7特性）</h2><p>在使用数据类的时候（比如我的parameters.py中的Parameters类）（根据我的个人习惯，我会将文件名全部小写，但是类会大写首字母），遵循这一条会让可读性有一定提升。<a href="https://www.python.org/dev/peps/pep-0557/" target="_blank" rel="noopener">PEP-557</a>规定了相关规则。</p><p>顺带一提，建议工程中所有的类都<strong>显式继承object类</strong>或者<strong>nn.Module类</strong>。</p><p>原版代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, a, b)</span>:</span></span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br></pre></td></tr></table></figure></p><p>使用类装饰器：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(object)</span>:</span></span><br><span class="line">    a: str</span><br><span class="line">    b: str</span><br></pre></td></tr></table></figure></p><p>这个类越是数据类，好处越明显。而且这个装饰器还会自动地帮助你实现一些魔术方法。</p><h2 id="使用新的Super方法调用"><a href="#使用新的Super方法调用" class="headerlink" title="使用新的Super方法调用"></a>使用新的Super方法调用</h2><p>在我们初始化一个神经网络模块的时候，这样的代码在开源代码中十分常见：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>但是这个超类方法可以被下列写法完全替代：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><p>推荐使用。</p><h2 id="使用-init-py文件"><a href="#使用-init-py文件" class="headerlink" title="使用__init__.py文件"></a>使用__init__.py文件</h2><p>这个文件的作用是将文件夹变为一个Python模块。Python 中的每个模块的包中，都有__init__.py 文件。</p><p>通常__init__.py 文件为空，但是我们还可以为它增加其他的功能。我们在导入一个包时，实际上是导入了它的__init__.py文件。这样我们可以在__init__.py文件中批量导入我们所需要的模块，而不再需要一个一个的导入。</p><p>这样做在写代码时有个额外的好处：在Visual Studio Code等编辑器中，编辑器能够进行<strong>智能提示和自动补全</strong>。</p><h2 id="在必要的时候使用assert"><a href="#在必要的时候使用assert" class="headerlink" title="在必要的时候使用assert"></a>在必要的时候使用assert</h2><p>assert是一个不怎么被提起的功能，然而我在实际使用中感受到assert可以帮助我们进行debug。一般来说，我进行debug最常用的方法是进行单步调试，在<strong>关键的地方打断点，观察变量列表</strong>；然而断点的机制决定了它并不是持久化的debug策略，我们在与其他人协作写代码的时候，也无法提醒其重点关注和检查哪些问题。</p><p>用于持久化进行测试的常见方法一般是将需要观察的变量进行print输出，然后<strong>观察output控制台</strong>。然而print函数在debug过程中可是要去掉的。所以assert函数可以<strong>在关键的地方对变量进行检查</strong>。通过这种检查，函数的功能得以保证，且无需添加if，避免了过深的隐蔽bug；此外，assert语句可以提醒其他的阅读这段代码的人：这个地方的变量应当满足什么要求，是怎么样的一个变量。</p><h2 id="持续更新中"><a href="#持续更新中" class="headerlink" title="持续更新中"></a>持续更新中</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;提及PyTorch的代码写作，我没有找到一个大家约定俗成的规约。我自己遵循了一些能让Python代码更具有可读性和持续开发的规则，现记录如下。&lt;/p&gt;
    
    </summary>
    
      <category term="软件工程" scheme="http://ldzhangyx.github.io/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="Python" scheme="http://ldzhangyx.github.io/tags/Python/"/>
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
      <category term="软件工程" scheme="http://ldzhangyx.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>self-attention与attention简要梳理</title>
    <link href="http://ldzhangyx.github.io/2018/10/14/self-attention/"/>
    <id>http://ldzhangyx.github.io/2018/10/14/self-attention/</id>
    <published>2018-10-14T15:31:02.000Z</published>
    <updated>2018-10-15T14:47:38.351Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention的计算机制"><a href="#Attention的计算机制" class="headerlink" title="Attention的计算机制"></a>Attention的计算机制</h1><h2 id="总说"><a href="#总说" class="headerlink" title="总说"></a>总说</h2><p>Attention机制主要作用在Decode阶段。在这一阶段，$s_t$的运算受到了$s_{t-1}$，$y_{t-1}$和$c_t$的影响。根据目前比较统一的观点，Attention值的计算是相似度比较的结果。Decoder端的Query与Encoder端的各个Key进行相似度比较得到不同的权重，最后对各个Key对应的Value进行归一化的加权累加，得到Attention Value。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>在实际应用中，Key就是Value，即K=V；而K与Q（Query）实际上也常用Hidden State来计算。</p><h2 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h2><p>相似度计算有几种方式。下面公式中$t$代表Decoder第$t$时刻，$s$代表Encoder中第$s$个位置的hidden state。</p><p>点积注意力：<script type="math/tex">s(h_t, h_s) = h_t^Th_s</script></p><p>一般的注意力：<script type="math/tex">s(h_t, h_s) = h_t^TW_ah_s</script></p><p>连接(concat)注意力：<script type="math/tex">s(h_t, h_s) = V_a^T\tanh(W_a[h_t;h_s])</script></p><p>加性注意力：<script type="math/tex">s(h_t, h_s) = V_a^T\tanh(Wh_t+Uh_s)</script></p><h2 id="之后的计算"><a href="#之后的计算" class="headerlink" title="之后的计算"></a>之后的计算</h2><p>我们将$s(h_t, h_i)$进行相似度计算的结果（记为$e_{ti}$）进行softmax归一化，再权值相加，得到最终的attention值：</p><script type="math/tex; mode=display">\alpha_{t,i}^{txt} = \frac{\exp(e_{t,i}^{txt})}{\sum_{j=1}^N\exp(e_{t,j}^{txt})}</script><script type="math/tex; mode=display">c_{txt} = \sum_{i=1}^N\alpha_{t,i}^{txt}h_i</script><p>从统一的角度来看，如果将Decoder端的Query视为整体Q，Key视为Encoder端的整体K，Value在自然语言处理中一般直接使用K，记为V。所以，</p><script type="math/tex; mode=display">Q \in \mathbb{R}^{n*d_k}, K \in \mathbb{R}^{m*d_k}, V \in \mathbb{R}^{m*d_v}</script><p>所以上述Attention可以简要的抽象成下列公式：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其实质是将序列Q编码成了一个新的$n*d_v$的序列。如果将Query的逐个向量分开，那么公式也可以这么表达：</p><script type="math/tex; mode=display">Attention(Q_t, K, V) = \sum_{s=1}^m\frac{1}{z}\exp(\frac{<q_t, k_s>}{\sqrt{d_k}})v_s</script><p>其中Z是归一化因子。事实上q,k,v分别是 query,key,value 的简写，K,V是一一对应的，它们就像是 key-value 的关系，那么上式的意思就是通过$q_t$这个 query，通过与各个$k_s$内积的softmax方式，来得到$q_t$与各个$v_s$的相似度，然后加权求和，得到一个$d_v$维的向量。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>这篇微信文章是《Attention is All You Need》的论文笔记，需要看的时候可以看一下。</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247486960&amp;idx=1&amp;sn=1b4b9d7ec7a9f40fa8a9df6b6f53bbfb&amp;chksm=96e9d270a19e5b668875392da1d1aaa28ffd0af17d44f7ee81c2754c78cc35edf2e35be2c6a1&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">文章链接</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Attention的计算机制&quot;&gt;&lt;a href=&quot;#Attention的计算机制&quot; class=&quot;headerlink&quot; title=&quot;Attention的计算机制&quot;&gt;&lt;/a&gt;Attention的计算机制&lt;/h1&gt;&lt;h2 id=&quot;总说&quot;&gt;&lt;a href=&quot;#总说&quot;
      
    
    </summary>
    
      <category term="术语梳理" scheme="http://ldzhangyx.github.io/categories/%E6%9C%AF%E8%AF%AD%E6%A2%B3%E7%90%86/"/>
    
    
      <category term="attention" scheme="http://ldzhangyx.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>《A Deep Generative Framework for Paraphrase Generation》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/26/deep-para-generation/"/>
    <id>http://ldzhangyx.github.io/2018/09/26/deep-para-generation/</id>
    <published>2018-09-26T08:44:14.000Z</published>
    <updated>2018-09-28T21:10:12.212Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章发表在2018年的AAAI上，继承了VAE在自然语言处理上的应用，将其应用于句子复述上，用于生成相似的句子；同时因为RNN可以作为语言模型使用，语句的语法正确性也有一定提升。<br>论文地址：<a href="https://arxiv.org/pdf/1709.05074.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1709.05074.pdf</a></p><a id="more"></a><h1 id="背景材料"><a href="#背景材料" class="headerlink" title="背景材料"></a>背景材料</h1><p>关于变分自编码器在NLP领域的相关介绍，可以看<a href="http://rsarxiv.github.io/2017/03/02/PaperWeekly%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%B8%83%E6%9C%9F/" target="_blank" rel="noopener">这篇讨论</a>。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>关于文本生成的背景研究，可以看<a href="https://www.msra.cn/zh-cn/news/features/ruihua-song-20161226" target="_blank" rel="noopener">这篇文章</a>。</p><p>关于VAE的原理（流形学习）和工程实现，可以看<a href="https://blog.csdn.net/JackyTintin/article/details/53641885" target="_blank" rel="noopener">这篇博文</a>。这篇博文详细介绍了VAE的训练过程，Encoder部分（识别模型）和Decoder部分（生成模型）的结构和各个参数的含义。这篇文章同时也提及了reparemetriazation trick的原理。</p><p>对于VAE结构的理解和讨论，可以看<a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">这篇文章</a>。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>Keras实现<a href="https://github.com/paulx3/keras_generative_pg" target="_blank" rel="noopener">在这里</a>。</p><p>PyTorch实现<a href="https://github.com/ale3otik/paraphrases-generator" target="_blank" rel="noopener">在这里</a>。</p><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>复述生成是NLP一个重要的课题，本文的模型将VAE和LSTM结合起来生成复述。传统的VAE结合RNN时，生成的句子不符合复述的要求，所以我们将模型的Encoder和Decoder都将原始的句子作为condition进行干涉，这样就达到了复述的效果。这个模型简单、模块化，可以生成不同的多种复述。在量化评测里，本模型明显优于state-of-the-art模型；分析发现模型语法良好；在新数据集上进行了测试，提供了一个baseline。</p><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>Introduction这部分，作者介绍了Paraphrase在QA等方面的应用，以及之前的paraphrase方法，认为其多制约于规则，而深度学习的生成模型更加是数据驱动。<br>区别于VAE的在句子生成上的其他应用，本文的复述模型需要捕获原本句子的特征，所以unconditional的句子生成模型并不适用于此任务。conndition的机制在过去曾被应用到CV领域，然而CV的应用仅仅是用有限的class label作为condition，以及不需要任何的intermediate representation。本文的方法在Encoder和Decoder方面都进行了condition，并且通过LSTM得到intermediate representation。</p><p>本文的生成框架对比seq2seq模型，尽管后者可以使用beam search，但不能同时产生多个效果很好的结果，因为beam search的后面结果总是比最顶部的差。在Quora数据集上，本文的模型表现很好。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="VAE结构"><a href="#VAE结构" class="headerlink" title="VAE结构"></a>VAE结构</h3><p>VAE是一个深度生成式隐变量模型，可以从高维输入学习到丰富、非线性的表示。<br>编码器方面，相比AE使用了一个确切的编码器函数$q_{\Phi}$，VAE使用了后验分布$q_{\Phi}(\mathbf{z}|\mathbf{x})$（或者说识别模型）。这个后验分布通常被假设为高斯分部，所以参数$\Phi$只有均值和方差。VAE使后验分布$q_{\Phi}(\mathbf{z}|\mathbf{x})$尽可能地接近先验分布$p(\mathbf{z})$，通常也被视为高斯分布。<br>VAE的解码器模型，使用了另一个分布$p_{\theta}(\mathbf{x}|\mathbf{z})$，也就是输入隐变量$\mathbf(z)$，还原出$\mathbf{x}$。其参数$\theta$使用另一个前馈神经网络的输出值。<br>将观测数据$x^{(i)}$的对数似然可以写作：</p><script type="math/tex; mode=display">\log p_\theta(x^{(i)} = KL(q_\varPhi(z|x^{(i)})||p_\theta(z|x^{(i)})) + L(\theta, \varPhi; x^{(i)}))</script><p>将$ELBO$记为$L$，然后通过优化$L$，来间接优化似然。<br>$L$可以写作：</p><script type="math/tex; mode=display">L(\theta, \varPhi; x^{(i)})) = -KL(q_\varPhi(z|x^{(i)})||p_\theta(z)) + E_{q_\varPhi(z|x)}[\log p_\theta(x^{(i)}|z)]</script><p>优化目标变为后面两项。</p><p>具体的推导可以参考<a href="https://arxiv.org/pdf/1606.05908.pdf" target="_blank" rel="noopener">这个教程</a>。</p><p>更多地，在建模文字数据的时候，也可以使用KL-term annealing以及dropout of inputs of the decoder等训练技巧，避免一些问题。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>我们的训练集使用了N对句子，表示为${\mathbf{s}_n^{(o), \mathbf{s}_n^{(p)}}^N_{n=1}$，其中原始句子用$o$标注，复述句子用$p$标注。句子的向量表示标记为$x^{(o)}$与$x^{(p)}$，</p><p>去掉LSTM后，本文的宏观模型如图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>区别于传统的VAE，本文构建的模型将$x^{o}$作为了condition加在了Encoder和Decoder之上。</p><p>更加细节的结构如图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>实际上，本文的模型包括了三个LSTM encoder和一个LSTM decoder，总共有4个LSTM，如上图所示。</p><p>在VAE的Encoder方面，两个LSTM encoder被使用，第一个转换原始句子$s^{(o)}$到其向量表示$x^{o}$，与复述句子$s^{(p)}$送入第二个LSTM。将两个句子都编码进了LSTM之后，使用一个前馈网络输出$\Phi$，也就是向量表示的均值和方差，送入VAE。</p><p>在VAE的Decoder方面，VAE输出隐变量$z$，第三个LSTM编码原始句子，然后将原始句子的向量表示$x^{(o)}$与$z$一同送入第四个LSTM，产生复述的句子。</p><p>将LSTM抽象化，可以得到一个这样的模型：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>这个模型的variational lower-bound，也就是$ELBO$如下。最小化KL散度的过程等同于最大化$ELBO$的过程。</p><script type="math/tex; mode=display">L(\theta, \Phi; x^{(p)}, x^{(o)})) = E_{q_\Phi(z|x^{(p)}, x^{(o)})}[\log p_\theta(x^{(p)}|z, x^{(o)})] -KL(q_\Phi(z|x^{(p)}, x^{(o)})||p(z))</script><p>模型的训练方法与<a href="https://aclweb.org/anthology/K/K16/K16-1002.pdf" target="_blank" rel="noopener">《Generating sentences from a continuous space》</a>中的相同。这篇论文深深地影响了后续论文的思路。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>本文使用了两个数据集进行评测。</p><h4 id="MSCOCO数据集"><a href="#MSCOCO数据集" class="headerlink" title="MSCOCO数据集"></a>MSCOCO数据集</h4><p>这个数据集，也曾用于评测句子复述的方法，对120K张图片分别有人工标注的五句描述。构建数据集的时候，本文随机忽略一个描述，然后将剩下的描述两两配对组成数据集，同时限制长度在15个单词以内，以减少模型复杂度。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Quora数据集"><a href="#Quora数据集" class="headerlink" title="Quora数据集"></a>Quora数据集</h3><p>Quora在2017年发布了一个数据集，包含400K行潜在的重复问题对。借助这个数据集，本文将重复的问题看作是复述的句子进行训练。重复的问题对大约有155K对，使用50K，100K，150K对进行训练，4K对作为测试。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>实验的主要参数设置参考了《Generating sentences from a continuous space》的<a href="https://github.com/kefirski/pytorch_RVAE" target="_blank" rel="noopener">实现代码</a>，未对任何数据集做fine-tuning。本文模型中，作者没有使用预先embedding好的词向量，而是自己进行了训练。embedding dimension设置为300，encoder和decoder的dimension是600，隐变量dimension是1100。Encoder只有一层而Decoder有两层。模型使用SGD进行训练，学习率固定为$5*10^{-5}$，dropout为0.3，batch size为32。</p><h2 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h2><p>本文使用了BLEU，METEOR和TER进行评测。因为这个过程类似翻译任务，所以借用翻译的指标效果会比较好。评测结果显示优于其他模型。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>评测包含了对实验结果的分析，可以直接阅读原文。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了一个深度生成框架，基于VAE，增加了seq2seq模型，用于生成释义。与传统的VAE和无条件句子生成模型不同，这个模型在输入句子上调节VAE的encoder和decoder侧，因此可以为给定句子生成多个释义。在一般复述生成数据集上评估所提出的方法，并且表明它在很大程度上优于现有技术，没有任何超参数调整。本文还评估了对最近发布的问题复述数据集的方法，并展示了其卓越的性能。生成的释义不仅在语义上与原始输入句子相似，而且还能够捕获与原始句子相关的新概念。</p><h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><p><strong>这篇文章是我阅读的第一篇关于VAE文本生成的论文</strong>，我将在本周使用PyTorch完成这个模型的复现工作。在学习VAE背后深层原理的过程中，我遇到了一些麻烦，只能说是初步理解。然而工程上VAE是一个相比GAN更加自然地适用于NLP的模型，其噪声与重建的对抗思路让我对文本生成有了全新的认识。</p><p><strong>对于VAE结构，我自己还需要做更多的推导、学习，反复阅读笔记最前面的几篇文章，加深理解。如果不能完整理解VAE的构建过程，就无从提起改进。</strong></p><h2 id="文章亮点"><a href="#文章亮点" class="headerlink" title="文章亮点"></a>文章亮点</h2><p>文章提出了一个基于VAE的模型，但这个模型并不是单纯的文本生成，而是使用了condition，那么这个condition对VAE公式要做一些改动，这个思维的推理过程结合了一定理论成分。<br>在句子复述的领域中，这个模型生成的句子在可读性（METEOR，人类评估）上有着一定优势，能不能用于语法改错？虽然语法改错上我猜测基于规则的模型会更加精确全面，但一定程度上自动改善语法问题，应该有一定帮助。</p><h2 id="想法-1"><a href="#想法-1" class="headerlink" title="想法"></a>想法</h2><p>这篇文章使用VAE来做句子复述，那么是否可以将condition的概念拓展到另外的领域？</p><p>在2016年的那篇VAE做NLP的开创性论文中，提及了这样一个问题，在实际的训练过程中，KL散度可能会出现collapse，导致后验等于先验。由于 RNN-based 的 decoder 有着非常强的 modeling power，直接导致即使依赖很少的 history 信息也可以让 reconstruction errors 降得很低，换句话说，decoder 不依赖 encoder 提供的 z 了，VAE-LSTM模型因此退化成RNN-LM语言模型。</p><p>在那篇论文里，使用的缓解办法就是KL cost annealing 和 Word dropout。目前对VAE结构的改进研究也很多，我认为改变VAE结构，进行微调，其效果有可能陷入GAN的一堆改进结构的泥淖里。</p><p>如果将VAE引入其他模型会怎么样呢？哈佛NLP组提出过<a href="https://zhuanlan.zhihu.com/p/40621695" target="_blank" rel="noopener">注意力模型的解决方案</a>。我觉得这方面的思考非常有吸引力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章发表在2018年的AAAI上，继承了VAE在自然语言处理上的应用，将其应用于句子复述上，用于生成相似的句子；同时因为RNN可以作为语言模型使用，语句的语法正确性也有一定提升。&lt;br&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1709.05074.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1709.05074.pdf&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="变分自编码器" scheme="http://ldzhangyx.github.io/tags/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
      <category term="循环神经网络" scheme="http://ldzhangyx.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自动问答" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E5%8A%A8%E9%97%AE%E7%AD%94/"/>
    
      <category term="句子复述" scheme="http://ldzhangyx.github.io/tags/%E5%8F%A5%E5%AD%90%E5%A4%8D%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>《XiaoIce Band: A Melody and Arrangement Generation Framework for Pop Music》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/26/xiaoice-band/"/>
    <id>http://ldzhangyx.github.io/2018/09/26/xiaoice-band/</id>
    <published>2018-09-25T18:47:41.000Z</published>
    <updated>2018-10-02T07:47:29.997Z</updated>
    
    <content type="html"><![CDATA[<p>小冰乐队是我一直想做笔记的一篇论文，做音乐生成的人不多，这篇论文思路和我的想法又有些相似。这篇由中科大和微软合作撰写的论文，发表在KDD’18上。<br>论文地址：<a href="http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music" target="_blank" rel="noopener">http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music</a></p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>以前的模型在歌曲生成中有限制，因为歌曲对旋律和联系都有要求。除此之外，一些关系到歌曲质量的因子，例如和弦行进和节奏模式上做的不好。而且多轨音乐如何保证其和谐程度也是个未探索的问题。</p><p>本文提出了一个着力在流行音乐生成的端到端模型，“小冰乐队”，歌曲通过几种乐器进行演奏。对于旋律和和弦行进，本文设计了Chord based Rhythm and Melody CrossGeneration Model (CRMCG)架构；对于多轨音乐的组织，本文设计了Multi-Instrument Co-Arrangement Model (MICA)架构。</p><p>模型在一个真实世界数据集上做了拓展实验，证明了小冰乐队的有效性。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文章首先介绍了一下音乐生成的历史。2003年Conklin从真实音乐中采样生成音乐；之后N-gram和Markov模型也应用于生成上。这些方法需要手动选取特征。Magenta使用DRNN从MIDI中学习生成音乐，然而只是单轨。</p><p>现有的方法并没有很好的解决一些重要挑战。特别在这几个方向：</p><ul><li><p>和弦行进在流行音乐里通常是存在的，其指导了旋律的走向。如果能将和弦行进作为输入进行音乐生成，这是有好处的；流行音乐有几个固定的节奏模式（pattern），让歌曲更加结构化、暂停恰当。然而现有的逐个音符的生成模型无法考虑节奏模式，另一方面，这些模型尽管使用了和弦，但只是用单和弦作为输入的一个feature，而不考虑和弦的整体行进。</p></li><li><p>完整的歌曲考虑了多轨的和弦、鼓点、节奏模式的组织，以及其他乐器共同演奏的背景伴奏，但这些模型没能组织起多轨架构。</p></li><li><p>不同轨道和乐器有其自己的特点，然而它们也应该和谐演奏。极少现有工作研究了多轨音乐生成，但没有模型考虑到多轨之间的和谐性。</p></li></ul><p>本文推出了CRMCG和MICA模型。前者用来生成音乐，后者用来保证和谐。Attention和MLP用于捕捉其它task的有效信息。本文主要贡献如下：</p><ul><li><p>提出了端到端的多轨歌曲生成模型，包括旋律和组织。</p></li><li><p>基于音乐知识，作者利用和弦行进指导旋律进行和节奏模式，学习歌曲结构；然后使用旋律和节奏交叉生成的方法进行音乐生成。</p></li><li><p>作者发展了多任务的联合生成网络，在decoder层的每一个step里使用了其他的task states，以保证多轨音乐的和谐性。</p></li><li><p>大量的实验表明，模型优于baseline。</p></li></ul><h2 id="Ralated-Work"><a href="#Ralated-Work" class="headerlink" title="Ralated Work"></a>Ralated Work</h2><h3 id="音乐生成"><a href="#音乐生成" class="headerlink" title="音乐生成"></a>音乐生成</h3><p>在音乐生成上一开始有数据驱动的传统统计学方法，但是需要大量人力和领域知识。</p><p>深度学习上的方法有RNN，VAE和GAN几种方法。对于RNN来说，有一项工作利用分层RNN也生成了搭配的和弦和鼓点（看了一下引用列表，因为这个领域不大，除了VAE那篇以外我都是好好读过的）。</p><p>然而没有单一的研究考虑了音乐的特殊性（比如流行音乐里的和弦行进）。下图所示是各个模型的特点对比：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>多任务学习经常被用于在相关的任务间共享特征，因为一个任务中学到的特征可能对其他任务有帮助。多任务学习在NLP和CV里都广泛应用。</p><h2 id="背景知识-名词定义"><a href="#背景知识-名词定义" class="headerlink" title="背景知识/名词定义"></a>背景知识/名词定义</h2><h3 id="和弦的行进"><a href="#和弦的行进" class="headerlink" title="和弦的行进"></a>和弦的行进</h3><p>这段话是我自己总结的：</p><p>和弦的行进是人们在创作音乐时挖掘的套路。随着共同声部的保留和和弦顺/逆级进，一个成功的和弦走向能让人感到完善和谐。一般流行音乐中，在主和弦和结束式中插入调内和弦可以拓展歌曲长度，同时让歌曲流畅发展。</p><p>更多知识可以这个<a href="https://www.zhihu.com/question/46536599" target="_blank" rel="noopener">知乎问题</a>。</p><h3 id="节奏模式"><a href="#节奏模式" class="headerlink" title="节奏模式"></a>节奏模式</h3><p>流行音乐中会出现一些相同重复的结构，使得流行音乐有着结构化特点。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="乐器特点"><a href="#乐器特点" class="headerlink" title="乐器特点"></a>乐器特点</h3><p>很多流行歌是多轨的，而钢琴一般负责主旋律，其他乐器负责伴奏。</p><h2 id="问题陈述-amp-模型结构"><a href="#问题陈述-amp-模型结构" class="headerlink" title="问题陈述 &amp; 模型结构"></a>问题陈述 &amp; 模型结构</h2><p>表中为使用的数学符号含义。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="问题陈述"><a href="#问题陈述" class="headerlink" title="问题陈述"></a>问题陈述</h3><p>在音乐生成的任务上，给定输入为和弦进行式</p><script type="math/tex; mode=display">C = \{c_1, c_2, ..., c_{l_c}\}</script><p>其中$c_i$为和弦的one-hot编码，$l_c$是序列长度。</p><p>要生成的节奏</p><script type="math/tex; mode=display">R_i = \{r_{i1}, r_{i2}, ..., r_{il_r}\}</script><p>和旋律</p><script type="math/tex; mode=display">M_i = \{r_{i1}, r_{i2}, ..., r_{il_m}\}</script><p>。</p><p>框架分为四个部分（如图）：</p><ol><li>数据处理</li><li>CRMCG单音轨旋律生成</li><li>MICA多音轨组织生成</li><li>展示</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="CRMCG"><a href="#CRMCG" class="headerlink" title="CRMCG"></a>CRMCG</h3><p>CRMCG的模型框架如图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图中我用Surface Pen做了一些不影响阅读的标记。</p><p>给定一个长度为N的和弦行进，我们要生成的当然是N个音乐片段。我们将这N个音乐片段标记为${p_1, p_2, …, p_N}$，每一个片段里（假设片段$i$），有着节奏$R_i$和旋律$M_i$。</p><p>和弦的one-hot编码需要降维。通过一个Embedding矩阵输出和弦的低维表示：</p><script type="math/tex; mode=display">\overline{C} = E_cC, E_c \in \mathbb{R}^{V_c * d}</script><p>然后使用一个GRU来处理和弦。然后使用这些hidden states来帮助生成节奏和旋律。</p><script type="math/tex; mode=display">\overline{h}^c_{i,0} = GRU(\overline{c}_i), i= 1,2,...,l_c</script><h4 id="节奏生成"><a href="#节奏生成" class="headerlink" title="节奏生成"></a>节奏生成</h4><p>生成的节奏需要与音乐已经存在的部分保持和谐。所以，要把之前的音乐部分考虑进去。</p><p>首先我们将之前生成的节奏Embedding回来：</p><script type="math/tex; mode=display">\overline{R}_{t-1} = E_rR_{t-1}, E_r \in \mathbb{R}^{V_r * d}</script><script type="math/tex; mode=display">\overline{M}_{t-1} = E_mM_{t-1}, E_m \in \mathbb{R}^{V_m * d}</script><p>其中$V_m$与$V_r$是音符和鼓点的字典尺寸。然后输入GRU Encoder：</p><script type="math/tex; mode=display">\overline{h}^m_{t-1,i} = GRU(\overline{m}_{t-1,i}), i= 1,2,...,l_m</script><script type="math/tex; mode=display">\overline{h}^r_{t-1,i} = GRU(\overline{r}_{t-1,i}), i= 1,2,...,l_r</script><p>将两个Encoder最后的hidden states进行concat拼接，然后线性变换作为Decoder的初始state，依次进行decode。</p><script type="math/tex; mode=display">s^r_0 = g(W[\overline{h}^m_{t-1,l_m}, \overline{h}^r_{t-1,l_r}]+b), W \in \mathbb{R}^{b*b}</script><script type="math/tex; mode=display">s^r_i = GRU(u^r_{i-1}, s^r_{i-1}), i>0</script><script type="math/tex; mode=display">y^r_i = softmax(s^r_i)</script><h4 id="旋律生成"><a href="#旋律生成" class="headerlink" title="旋律生成"></a>旋律生成</h4><p>不同于节奏生成，旋律的生成i要考虑到和弦的因素。与节奏部分类似地，将三个hidden state进行拼接，然后进行Decode。</p><script type="math/tex; mode=display">s^m_0 = g(W[\overline{h}^m_{t-1,l_m}, \overline{h}^r_{t,l_r}, \overline{h}^c_t]+b), W \in \mathbb{R}^{b*b}</script><script type="math/tex; mode=display">s^m_i = GRU(u^m_{i-1}, s^m_{i-1}), i>0</script><script type="math/tex; mode=display">y^m_i = softmax(s^m_i)</script><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>对于两个部分，分别设置一个损失函数，形式都为softmax cross-entropy函数。在节奏模块，只更新与节奏损失$L^r$有关的参数；在旋律模块，更新所有参数。</p><h3 id="MICA"><a href="#MICA" class="headerlink" title="MICA"></a>MICA</h3><h4 id="MICA-1"><a href="#MICA-1" class="headerlink" title="MICA"></a>MICA</h4><p>设计了一个One-to-Many Sequence Generation任务（OSMG）。不同于常规的多序列学习，OSMG生成的序列有着紧密的联系。</p><p>如同下图所示，Decoder的hidden state包含了序列信息。所以，在生成一个音轨中的音符的时候，自然地引入了其他音轨的hidden state。作者设计了两个合作的cell在decoder的隐层之间进行hidden states的整合。</p><h4 id="Attention-Cell"><a href="#Attention-Cell" class="headerlink" title="Attention Cell"></a>Attention Cell</h4><p>作者设计了一个attention cell，捕获其它task的state供current states使用，结构如下图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>attention机制如下列公式：</p><script type="math/tex; mode=display">a^i_{t,k} = \sum^T_{j=1}\alpha_{t,ij}h^j_{t,k-1},</script><script type="math/tex; mode=display">e_{t,ij} = v^T\tanh(Wh^i_{t,k-1}+Uh^j_{t,k-1}), W,U \in \mathbb{R}^{b*b},</script><script type="math/tex; mode=display">\alpha_{t,ij} = \frac{exp(e_{t,ij})}{\sum^T_{s=1}exp(e_{t,is})}</script><p>其中$a^i_{t.k}$代表task $i$在step $k$，period $t$的合作向量；$h^j_{t,k-1}$代表了第$j$个任务在step $k-1$，period $t$的hidden state。</p><p>之后，修改GRU单元，允许其他音轨的信息能够充分影响到当前音轨的形成。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="8.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="MLP-Cell"><a href="#MLP-Cell" class="headerlink" title="MLP Cell"></a>MLP Cell</h4><p>为了同时考虑到每种乐器的重要程度，通过其重要程度对hidden states进行整合，作者设计了这样一个模块：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="9.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其公式为：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="10.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p>受上述公式的启发，作者优化了以相同Encoder生成的表示为条件的几个条件概率项的总和。</p><script type="math/tex; mode=display">L(\theta) = {argmax}_\theta(\sum_{T_k}(frac{1}{N_p}\sum^{N_p}_ilogp(Y_i^{T_k}|X_i^{T_k};\theta)))</script><p>其中$\theta = {\theta_{src}, \theta_{trgT_k},T_k = 1,2,…, T_m}$，$m$是任务的总数。$\theta_{src}$是源encoder的参数集，$\theta_{trgT_k}$是第$T_k$个目标音轨的参数集，$N_p$是第$p$个序列对，平行training corpus的大小。</p><h4 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h4><p>假设已经有了旋律序列$M_i = {m_{i1}, m|_{i2), …,m_{il_m}}$，下一个步骤是生成其他的乐器音轨，去配合这个旋律。使用GRU处理序列，得到Decoder的初始state：</p><script type="math/tex; mode=display">\overline{M} = E_mM, Em \in \mathbb{R}^{V_m * d}</script><script type="math/tex; mode=display">s^m_0 = GRU(\overline{m}_i, l_m)</script><p>多序列decoder的输出，与其他乐器音轨联系，所以考虑旋律和其他的伴奏音轨：</p><script type="math/tex; mode=display">s^i_t = AttentionCell(y^i_{t-1}, s^i_{t-1}), t>0, or</script><script type="math/tex; mode=display">s^i_t = MLPCell(u^i_{t-1}, s^i_{t-1})， t>0</script><script type="math/tex; mode=display">y^i_t = softmax(s^i_t)</script><p>其中，$s^i_t$是第$i$个任务在第$t$个step的hidden state。作者使用softmax层，利用这个state得到第$i$个乐器的序列。Attention Cell和MLP Cell，用来得到合作的state，包含了本身的乐器state和其他乐器的state，保持所有乐器和谐。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在两个数据集上做了实验，包括旋律生成和组织生成。</p><h3 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h3><p>做实验的数据集叫做真实世界数据集（a real-world dataset），包含了超过50000个MIDI文件。去除掉没有区分音轨的文件，最后14,077个文件得以保留。每个MIDI文件包含了几种不同的音轨，如：旋律、鼓、贝斯、弦乐。</p><p>为保证实验结果可靠性，作者将所有的MIDI文件移到了C大调或者A小调，保证音乐在同样的调上。然后作者设置速度为60bpm，将一个period片段设置为两个小节长。数据集统计如下表：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>采用9855个文件作为训练集，2855个用于调整参数，最后1407个文件作为训练集。</p><p>每个GRU的hidden unit设为256，Attention Cell和MLP Cell的参数维度为256。模型使用SGD进行训练，batch size为64，验证集使用交叉熵损失函数。</p><h3 id="旋律生成-1"><a href="#旋律生成-1" class="headerlink" title="旋律生成"></a>旋律生成</h3><p>只使用MIDI的旋律音轨。</p><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><ul><li>Magenta，Google的RNN项目</li><li>GANMidi，Yi-Hsuan Yang的MidiNet</li></ul><p>设计了三种不同的方法进行评测：</p><ul><li>完整版</li><li>移除了和弦信息</li><li>分别基于两个损失函数进行交叉训练。</li></ul><h4 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h4><p>使用8个志愿者打分。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>考虑下列因素：</p><ul><li>节奏</li><li>旋律</li><li>整合性/整体结构</li><li>适合唱的程度</li></ul><h4 id="和弦进行的分析"><a href="#和弦进行的分析" class="headerlink" title="和弦进行的分析"></a>和弦进行的分析</h4><p>作者定义了和弦精确度，来度量生成的旋律是否符合输入的和弦序列：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其中$P$是period片段的数量，$y_i$是第$i$个生成旋律，检测出来的和弦，而$\widetilde{y_i}$是输入的和弦。</p><p>准确度达到了82.25%，同时发现随着和弦精确度提升，旋律质量得到了保证。</p><h4 id="休止符分析"><a href="#休止符分析" class="headerlink" title="休止符分析"></a>休止符分析</h4><p>作者通过对比生成音乐片段的最小、最大、平均长度，对比数据集音乐，分布如果接近，证明模型的休止是恰当的。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="组织生成"><a href="#组织生成" class="headerlink" title="组织生成"></a>组织生成</h3><h4 id="Baseline-1"><a href="#Baseline-1" class="headerlink" title="Baseline"></a>Baseline</h4><ul><li>HRNN，《Song From Pi》的模型</li></ul><p>对比下面两种方法：</p><ul><li>Attention Cell</li><li>MLP Cell</li></ul><h4 id="整体Performance"><a href="#整体Performance" class="headerlink" title="整体Performance"></a>整体Performance</h4><ul><li>性能优于HRNN</li><li>多音轨得分高于单音轨（但是评测更倾向于整体氛围）</li><li>MLP比Attention得分更高</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="15.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="和谐程度分析"><a href="#和谐程度分析" class="headerlink" title="和谐程度分析"></a>和谐程度分析</h4><p>考虑：如果两个音轨有相似的和弦进行，我们认为它们和谐。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>实验发现音轨少，和谐度会更高。因此对于多音轨的生成，和谐度有着更高的要求。</p><h4 id="组织分析"><a href="#组织分析" class="headerlink" title="组织分析"></a>组织分析</h4><ul><li>音符准确度</li><li>莱文斯坦相似度</li><li>音符分布的均方误差</li><li>空白结果</li></ul><p>具体的分析在这里略过。</p><h1 id="见解和感想"><a href="#见解和感想" class="headerlink" title="见解和感想"></a>见解和感想</h1><p>这篇论文的亮点是显而易见的。</p><p>首先在单音轨的音乐生成里，在流行音乐的框架中捕捉了和弦行进式，将其作为输入，进行训练。和弦行进式很好的把握了音乐的主要节奏，是一个很好的突破口。</p><p>我大二刚接触音乐生成的时候，想的思路就是通过和弦行进式进行音乐生成。但是那个时候限于我编码能力太弱，想法未能落地，也接触不到相关的老师来指点我，完全是自己摸索的。总而言之，和弦行进式这个总体思路我非常赞同。</p><p>在单音轨的音乐训练时，数据的预处理非常重要。现在Kaggle有很多音乐相关的数据集，这是我以前没有注意到的。也许有非常合适的数据集来做相关的研究，但是就我目前所知，自带和弦行进式的数据集非常难找。<strong>也就是说数据预处理本身就是一项有挑战性的工作</strong>。对我来说，这个数据的来源非常有趣，我以后也会去Kaggle里找找有趣的数据集。</p><p>音乐相关的工具也是音乐生成的难点之一。怎么表示音乐？这篇文章提到的处理方法是将音符和节奏的one-hot编码embedding处理，作为输入。我认为这一点处理得比较模糊。音符的时长怎么办？一些问题在论文里没有讲述清楚。目前通用的编码是music21编码，<strong>我认为在工程实现上使用music21去处理移调、变速等问题是一个比较现实的做法</strong>。</p><p>那么多音轨音乐呢？这是我认为本文最出彩的一点，可以说是多音轨生成开先河的一个方法。我阅读论文的大部分时间都在理解多音轨音乐的结构上。Attention Cell和MLP Cell我认为非常有创意——如果是我自己来做多音轨，我可能会想到利用Attention，但无法以明确的形式将这个方法落地。</p><p>本文提出了一些分析方法，我认为是具有其独到之处的。音乐生成和文本生成一样，都有着没法量化的困难，而本文所做的分析恰恰是对Evaluation一个很好的补充。本文结合了和弦行进式，提出了一个Chord Accuracy，我在文中翻译为和弦精确度。这是一个直接的分析方法，又没有像BLEU那样放在音符的层次，而是放在了和弦的层次。我在这里好奇一点，作者是怎么识别生成音乐所属的和弦的？如果有一个自动的办法，<strong>可不可以将其加入Loss函数</strong>？</p><p>对于音符分布的分析方法对我来说也是一个启发，因为音乐生成的休止也一直是悬而未决的难题，或者说没有根本理论来解决。现在通过分布来衡量，那么对于休止就有了一个漂亮的量化过程。下面是我的想法：<strong>如果我们训练一个VAE，类似句子复述的问题一样，将生成的音乐进行微调，使两个分布尽量接近，会怎么样呢</strong></p><p>总而言之，这篇文章在多音轨音乐的生成上做了贡献，是一篇非常棒的文章。美中不足的是，作者在细节上面没有完全说明；没有放出音乐的sample导致无法客观评估其生成水准，以及最终的效果理应还有调整与改进的途径。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;小冰乐队是我一直想做笔记的一篇论文，做音乐生成的人不多，这篇论文思路和我的想法又有些相似。这篇由中科大和微软合作撰写的论文，发表在KDD’18上。&lt;br&gt;论文地址：&lt;a href=&quot;http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>《Multi-modal Sentence Summarization with Modality Attention and Image Filtering》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/25/Multi-modal%20Sentence%20Summarization%20with%20Modality%20Attention%20and%20Image%20Filtering/"/>
    <id>http://ldzhangyx.github.io/2018/09/25/Multi-modal Sentence Summarization with Modality Attention and Image Filtering/</id>
    <published>2018-09-25T06:07:31.000Z</published>
    <updated>2018-09-26T07:07:45.350Z</updated>
    
    <content type="html"><![CDATA[<p>宗成庆老师的这篇文章发表于<a href="https://acl2018.org/programme/papers/" target="_blank" rel="noopener">ACL’18</a>，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。<br>原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf</p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文介绍了一种多模态句子摘要任务，输入为一张图片和一对句子，输出为摘要。作者称其为MMSS任务。任务的难点在于怎么将视觉信息输入到框架里，以及怎么减小噪音。针对这两个问题，作者分别提出了两个方法：同时施加不同的注意力到文本和图片上；使用图像过滤Image Filter来降噪。</p><p>介绍主要是讲了文本摘要的历史，以及多模态方法最早应用在翻译领域，表现特别好，但是作者认为在摘要上表现得应该更好。</p><p>在解决MMSS任务的时候，作者准备使用分层注意力机制，底层分别关注图片和文本的内部，而上层对两个模态进行平衡。因为图片不能表现很多抽象内容，所以图片特征需要过滤去噪；为了解决生成句子结巴的问题，使用了coverage方法。</p><p>顺便他们做了一个数据集，真是让人肝疼的工作量。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图片展示了多模态模型的实际效果要好于文本模型。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>模型可见由句子编码器，图片编码器，摘要解码器和图片过滤器四个部分组成。</p><h3 id="模型图"><a href="#模型图" class="headerlink" title="模型图"></a>模型图</h3><p>模型简图如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="句子Encoder"><a href="#句子Encoder" class="headerlink" title="句子Encoder"></a>句子Encoder</h3><p>模型的句子Encoder使用了Bi-GRU。</p><script type="math/tex; mode=display">\overrightarrow{h}_i =  GRU(E[x_i], \overrightarrow{h}_{i-1})</script><script type="math/tex; mode=display">\overleftarrow{h}_i =  GRU(E[x_i], \overleftarrow{h}_{i-1})</script><h3 id="图片Encoder"><a href="#图片Encoder" class="headerlink" title="图片Encoder"></a>图片Encoder</h3><p>图片Encoder使用了VGG19，抽取了两种特征：7x7x512的局部特征（flatten之后成为了49x512），和4096维的全局特征。其中局部特征表示为：</p><script type="math/tex; mode=display">A = (a_1, ..., a_L), L=49 \\a_l \in \mathbb{R}^{512}</script><h3 id="摘要Decoder"><a href="#摘要Decoder" class="headerlink" title="摘要Decoder"></a>摘要Decoder</h3><p>Decoder使用了单向GRU：</p><script type="math/tex; mode=display">s_t = GRU(s_{t-1}, y_{t-1}, c_t)</script><p>其中，初始状态$s_0$，作者提出了几种初始化策略：</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h2}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{q}q)</script><p>利用了全局特征；</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_i)</script><p>与</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_ia_i)</script><p>利用了局部特征；</p><p>其中，$q$为全局特征，$a$为各个局部特征，如上文所说。</p><h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><p>在Decoder使用的context向量$c_t$，使用一个顶层的Attention，对图片context和文本context进行平衡统一：</p><script type="math/tex; mode=display">c_t = \beta_t^{txt}\mathbf{V}_Tc_{txt} + \beta_t^{img}\mathbf{V}_Ic_{img}</script><p>其中，</p><script type="math/tex; mode=display">\beta_t^{txt} = \sigma(\mathbf{U}_as_{t-1}+\mathbf{W}_ac_{txt})</script><script type="math/tex; mode=display">\beta_t^{img} = \sigma(\mathbf{U}_bs_{t-1}+\mathbf{W}_bc_{img})</script><p>对于文字的attention，使用普通的attention：</p><script type="math/tex; mode=display">c_{txt} = \sum_{i=1}^N\alpha_{t,i}^{txt}h_i</script><p>其中$N$为输入的文字序列长度，也就是embedding过后的向量总数量，$h_{i}$是Encoder的第$i$个Hidden State，这个公式将其加权求和。其中$\alpha_{t,i}^{txt}$与Decoder上一个时态的状态$s_{t-1}$与第$i$个Encoder的Hidden State，$h_{i}$有关。</p><script type="math/tex; mode=display">\alpha_{t,i}^{txt} = \frac{exp(e_{t,i}^{txt})}{\sum_{j=1}^Nexp(e_{t,j}^{txt})}</script><script type="math/tex; mode=display">e_{t,i}^{txt} = f(s_{t-1}, h_i) = v_a^T\tanh(\mathbf{U}_cs_{t-1} + \mathbf{W}_ch_i)</script><p>而对于图片attention，作者将每个feature map作为输入，进行attention处理：</p><script type="math/tex; mode=display">c_{img} = \sum_{i=1}^L\alpha_{t,i}^{img}a_l</script><p>其中L为feature map数量，$a_l$为局部特征。</p><script type="math/tex; mode=display">\alpha_{t,i}^{img} = \frac{exp(e_{t,i}^{img})}{\sum_{j=1}^Nexp(e_{t,j}^{img})}</script><script type="math/tex; mode=display">e_{t,i}^{img} = f(s_{t-1}, h_i) = v_b^T\tanh(\mathbf{U}_ds_{t-1} + \mathbf{W}_da_l)</script><h3 id="Coverage-Model-覆盖率模型"><a href="#Coverage-Model-覆盖率模型" class="headerlink" title="Coverage Model(覆盖率模型)"></a>Coverage Model(覆盖率模型)</h3><p>覆盖率模型可以参考《Get to the point》一文的论文笔记。</p><h3 id="Image-Filter"><a href="#Image-Filter" class="headerlink" title="Image Filter"></a>Image Filter</h3><p>作者设计了两个filter：Image Attention Filter和Image Context Filter。</p><h4 id="Image-Attention-Filter"><a href="#Image-Attention-Filter" class="headerlink" title="Image Attention Filter"></a>Image Attention Filter</h4><p>Image Attention Filter的目的在于“directly applied to change the attention scale between image and text”，即根据图片与文本的相关性进行数值控制。</p><script type="math/tex; mode=display">I_a = \sigma(v_s^Ts_0 + c_q^Tq + v_r^Ts_{t-1})</script><p>然后用这个系数更新：</p><script type="math/tex; mode=display">\beta_t^{img} = I_a \cdot \beta_t^{img}</script><p>其中$s_{0}$是decoder的初始状态，$q$是图片全局特征，这两个参数用来表示图片相关性；$s_{t−1}$是decoder上一个time step的状态，用来表示与下一个单词的联系。</p><h4 id="Image-Context-Filter"><a href="#Image-Context-Filter" class="headerlink" title="Image Context Filter"></a>Image Context Filter</h4><p>对于Image Context Vector，作者解释是脱胎于以前的思路（“Image context filter is partially inspired by gating mechanism which has gained great popularity in neural network models”），但是应用在多模态方法上仍属创新。</p><script type="math/tex; mode=display">I_a = \sigma(\mathbf{W}_ss_0 + \mathbf{W}_qq + \mathbf{W}_rs_{t-1})</script><p>这里的矩阵相乘可以使用Dense Layer实现，但是$I_c$与$I_a$最大的不同在于，$I_c$是一个向量，其元素用于控制特征的选择。</p><script type="math/tex; mode=display">c_{img} = I_{c} \odot c_{img}</script><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>本文贡献了一个MMSS任务使用的数据集。数据集首先从Gigaword Corpus里作为基础，对每个Headline，使用Yahoo!搜索图片，并取出top 5图片。之后删除无关图片（人物，缩略图，广告），雇佣10个研究生选择最匹配的图片（无图片标0），最后得到66,000条可用数据。随机将62,000作为训练集，2,000作为测试集，2,000作为开发集。</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>作者做了广泛的对比：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>Q：对Decoder状态进行的初始化有效吗？<br>A：有效。</p><p>Q：图片有助于提升摘要质量吗？<br>A：是的，可能是因为图片提供了更多信息。</p><p>Q：哪种图片更有效？<br>A：要求3个研究生标注300条数据中图片的匹配程度（匹配从1到3），发现更加匹配的图片所在的那条数据，模型能获得更高的ROUGE分数。</p><p>Q：多模态coverage的有效性？<br>A：通过计算重复词，textual与visual coverage确实能减少重复词的出现。</p><h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><h2 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h2><p>这篇文章的思路与一般的额外信息有一些区别，同是用额外信息干涉指导文本生成，这个模型同时使用了两个attention，并提出了一种加权机制将两个attention组合起来。在我读过的另一篇文章《Diversity driven Attention Model for Query-based Abstractive Summarization》中，作者试图用query的context干涉document的context，而不直接显性参与decoder的过程。</p><p>文中计算权值的时候，充分考虑了各种可能性；在Image Attention Filter那一块，将数个特征非线性组合起来，虽然显得参数有点多，好在不无道理。</p><p>贡献了一个全新的数据集（动用了10个研究生，真有钱），脱胎于Gigawords，对这个领域做出了基础性贡献。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>首先，对于数据集，我认为可以选择替代的数据集，可能是我之前做过中文摘要，我很自然地就想到了LCSTS，可以用同样的方法构建数据集，但是这个很费时间精力，而且并不是什么突出的想法贡献。</p><p>其次，为什么用VGG提取特征？在这个框架里，VGG提取特征取到的效果我持保留态度。即使有了图片特征又能怎么样呢？到底是一个怎样的机制让VGG的图片特征与关键字对上的？我认为这样的attention拼凑框架思路时非常棒的，但是图片特征与文字的多模态映射我始终不明白how it works. 作为替代方法，我很自然地想到了CV里的目标检测，使用选择性搜索，SVM判断图像中的实体，再作为特征送进模型，我认为这是一个更接近直觉的做法。</p><p>再次，Image Filtering这个做法我认为需要改进为更加reasonable的做法。我们完全可以做一个key-word版本的Filter。Filter有两个版本，Image Attention Filter的系数$I_a$是根据图像与文本的相关性来控制图像干预的程度；而Image Context Filter的系数$T_c$是用来突出图像特征的。这个想法理应可以迁移到word的使用上。</p><p>最后，文中用到的小trick，textual coverage mechanism，为了解决结巴问题，我们可以考虑其他的机制，比如将context vector做软正交化处理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;宗成庆老师的这篇文章发表于&lt;a href=&quot;https://acl2018.org/programme/papers/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ACL’18&lt;/a&gt;，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。&lt;br&gt;原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="多模态" scheme="http://ldzhangyx.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
      <category term="生成式摘要" scheme="http://ldzhangyx.github.io/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%91%98%E8%A6%81/"/>
    
  </entry>
  
  <entry>
    <title>【TensorFlow随笔】关于一个矩阵与多个矩阵相乘的问题</title>
    <link href="http://ldzhangyx.github.io/2017/12/21/%E3%80%90TensorFlow%E9%9A%8F%E7%AC%94%E3%80%91%E5%85%B3%E4%BA%8E%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5%E4%B8%8E%E5%A4%9A%E4%B8%AA%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://ldzhangyx.github.io/2017/12/21/【TensorFlow随笔】关于一个矩阵与多个矩阵相乘的问题/</id>
    <published>2017-12-20T17:00:00.000Z</published>
    <updated>2018-11-15T12:39:52.823Z</updated>
    
    <content type="html"><![CDATA[<p>问题描述：</p><p>Specifically, I want to do matmul(A,B) where</p><p>&nbsp;’A’ has shape (m,n)</p><p>&nbsp;’B’ has shape (k,n,p)</p><p>and the result should have shape (k,m,p)</p><hr><p>参考网站：</p><p><a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/4tgsOSxwtkY" target="_blank" rel="noopener">https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/4tgsOSxwtkY</a></p><p><a href="https://stackoverflow.com/questions/38235555/tensorflow-matmul-of-input-matrix-with-batch-data" target="_blank" rel="noopener">https://stackoverflow.com/questions/38235555/tensorflow-matmul-of-input-matrix-with-batch-data</a></p><hr><p>解决办法：</p><p>1，我们知道TensorFlow的matmul已经支持了batch，即：</p><pre><code>A = tf.Variable(tf.random_normal(shape=(a, b, n, m)))B = tf.Variable(tf.random_normal(shape=(a, b, m, k)))tf.matmul(A, B)</code></pre><p>会返回(a,b,n,k)，前面的N个维度会被保留。但是适用情景与题目不符。</p><p>2，所以我们自然想到reshape。</p><p><span class="comment-copy">You can conflate the two dimensions not used in the multiplication using reshape, multiply the two matrices, and then call reshape again to get the desired shape. This is equivalent to doing batch multiplication.</span></p><p><span class="comment-copy">简而言之呢，就是，你可以将乘法中用不到的维度reshape到后面去，比如</span></p><p><span class="comment-copy">(k, m, p) =&gt; (m, p * k)</span></p><p>进行矩阵乘法得到：(n, p * k)</p><p>之后reshape成：(k, n, p)。</p><p>虽然有些麻烦，但这是唯一的解决办法了。</p><p>适用情景：A矩阵只有一个，但是B矩阵有batch_num个，需要逐个进行矩阵乘法的场合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;问题描述：&lt;/p&gt;
&lt;p&gt;Specifically, I want to do matmul(A,B) where&lt;/p&gt;
&lt;p&gt;&amp;nbsp;’A’ has shape (m,n)&lt;/p&gt;
&lt;p&gt;&amp;nbsp;’B’ has shape (k,n,p)&lt;/p&gt;
&lt;p&gt;and 
      
    
    </summary>
    
    
      <category term="TensorFlow" scheme="http://ldzhangyx.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>【Python音乐生成】可能有用的一些Python库</title>
    <link href="http://ldzhangyx.github.io/2017/11/05/%E3%80%90Python%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E3%80%91%E5%8F%AF%E8%83%BD%E6%9C%89%E7%94%A8%E7%9A%84%E4%B8%80%E4%BA%9BPython%E5%BA%93/"/>
    <id>http://ldzhangyx.github.io/2017/11/05/【Python音乐生成】可能有用的一些Python库/</id>
    <published>2017-11-05T14:01:00.000Z</published>
    <updated>2018-11-15T12:36:40.089Z</updated>
    
    <content type="html"><![CDATA[<p>1，Python-MIDI，很多操作库的前置库。作者提供了一个python3的branch。git clone下来之后注意切换到这个branch之后再运行setup.py。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://images2017.cnblogs.com/blog/1037202/201711/1037202-20171105215112310-1907169873.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>实际使用的时候，使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> midi</span><br></pre></td></tr></table></figure><p>进行使用。</p><p>&nbsp;</p><p>2，pretty-midi，非常有用的一个库，将midi进行了二次转换，变成piano-roll。这个库被用在Google Magenta里，我自己的项目也要使用它。master分支现在已经支持了Python3.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://images2017.cnblogs.com/blog/1037202/201711/1037202-20171105215531295-1873892334.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>&nbsp;</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://images2017.cnblogs.com/blog/1037202/201711/1037202-20171105215442873-553231678.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>&nbsp;</p><p>3，<span class="col-11 text-gray-dark mr-2">fluidsynth3，原作者感觉像是跑路了，有大神写出了其Python3版本，真的非常感谢。这个库是上面pretty-midi的前置库。</span></p><p><span class="col-11 text-gray-dark mr-2"><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://images2017.cnblogs.com/blog/1037202/201711/1037202-20171105215705091-1523489110.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure></span></p><p>&nbsp;</p><p>以上就是最常见的几个专门处理音乐的库了。</p><p>当然一些自己用过的库感觉也是可以的。</p><p>4，mingus，第三方推出了其Python3版本。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://images2017.cnblogs.com/blog/1037202/201711/1037202-20171105220027341-70214963.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>&nbsp;</p><p>最后默默祝福自己顺利写出论文来&hellip;&hellip;</p><p>&nbsp;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1，Python-MIDI，很多操作库的前置库。作者提供了一个python3的branch。git clone下来之后注意切换到这个branch之后再运行setup.py。&lt;/p&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                
      
    
    </summary>
    
    
  </entry>
  
</feed>
