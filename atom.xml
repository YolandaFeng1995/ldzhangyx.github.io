<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yixiao Zhang&#39;s Website</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ldzhangyx.github.io/"/>
  <updated>2018-09-26T22:56:57.622Z</updated>
  <id>http://ldzhangyx.github.io/</id>
  
  <author>
    <name>Yixiao Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《Attention is All You Need》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/26/attention-is-all-you-need/"/>
    <id>http://ldzhangyx.github.io/2018/09/26/attention-is-all-you-need/</id>
    <published>2018-09-26T22:50:53.000Z</published>
    <updated>2018-09-26T22:56:57.622Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="attention" scheme="http://ldzhangyx.github.io/tags/attention/"/>
    
      <category term="机器翻译" scheme="http://ldzhangyx.github.io/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>《A Deep Generative Framework for Paraphrase Generation》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/26/deep-para-generation/"/>
    <id>http://ldzhangyx.github.io/2018/09/26/deep-para-generation/</id>
    <published>2018-09-26T22:44:14.000Z</published>
    <updated>2018-09-27T23:50:28.460Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章发表在2018年的AAAI上，继承了VAE在自然语言处理上的应用，将其应用于句子复述上，用于生成相似的句子；同时因为RNN可以作为语言模型使用，语句的语法正确性也有一定提升。<br>论文地址：<a href="https://arxiv.org/pdf/1709.05074.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1709.05074.pdf</a></p><a id="more"></a><h1 id="背景材料"><a href="#背景材料" class="headerlink" title="背景材料"></a>背景材料</h1><p>关于变分自编码器在NLP领域的相关介绍，可以看<a href="http://rsarxiv.github.io/2017/03/02/PaperWeekly%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%B8%83%E6%9C%9F/" target="_blank" rel="noopener">这篇讨论</a>。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>关于文本生成的背景研究，可以看<a href="https://www.msra.cn/zh-cn/news/features/ruihua-song-20161226" target="_blank" rel="noopener">这篇文章</a>。</p><p>关于VAE的原理（流形学习）和工程实现，可以看<a href="https://blog.csdn.net/JackyTintin/article/details/53641885" target="_blank" rel="noopener">这篇博文</a>。这篇博文详细介绍了VAE的训练过程，Encoder部分（识别模型）和Decoder部分（生成模型）的结构和各个参数的含义。这篇文章同时也提及了reparemetriazation trick的原理。</p><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>复述生成是NLP一个重要的课题，本文的模型将VAE和LSTM结合起来生成复述。传统的VAE结合RNN时，生成的句子不符合复述的要求，所以我们将模型的Encoder和Decoder都将原始的句子作为condition进行干涉，这样就达到了复述的效果。这个模型简单、模块化，可以生成不同的多种复述。在量化评测里，本模型明显优于state-of-the-art模型；分析发现模型语法良好；在新数据集上进行了测试，提供了一个baseline。</p><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>Introduction这部分，作者介绍了Paraphrase在QA等方面的应用，以及之前的paraphrase方法，认为其多制约于规则，而深度学习的生成模型更加是数据驱动。<br>区别于VAE的在句子生成上的其他应用，本文的复述模型需要捕获原本句子的特征，所以unconditional的句子生成模型并不适用于此任务。conndition的机制在过去曾被应用到CV领域，然而CV的应用仅仅是用有限的class label作为condition，以及不需要任何的intermediate representation。本文的方法在Encoder和Decoder方面都进行了condition，并且通过LSTM得到intermediate representation。</p><p>本文的生成框架对比seq2seq模型，尽管后者可以使用beam search，但不能同时产生多个效果很好的结果，因为beam search的后面结果总是比最顶部的差。在Quora数据集上，本文的模型表现很好。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="VAE结构"><a href="#VAE结构" class="headerlink" title="VAE结构"></a>VAE结构</h3><p>VAE是一个深度生成式隐变量模型，可以从高维输入学习到丰富、非线性的表示。<br>编码器方面，相比AE使用了一个确切的编码器函数$q<em>{\Phi}$，VAE使用了后验分布$q</em>{\Phi}(\mathbf{z}|\mathbf{x})$（或者说识别模型）。这个后验分布通常被假设为高斯分部，所以参数$\Phi$只有均值和方差。VAE使后验分布$q<em>{\Phi}(\mathbf{z}|\mathbf{x})$尽可能地接近先验分布$p(\mathbf{z})$，通常也被视为高斯分布。<br>VAE的解码器模型，使用了另一个分布$p</em>{\theta}(\mathbf{x}|\mathbf{z})$，也就是输入隐变量$\mathbf(z)$，还原出$\mathbf{x}$。其参数$\theta$使用另一个前馈神经网络的输出值。<br>将观测数据$x^{(i)}$的对数似然可以写作：</p><script type="math/tex; mode=display">\log p_\theta(x^{(i)} = KL(q_\varPhi(z|x^{(i)})||p_\theta(z|x^{(i)})) + L(\theta, \varPhi; x^{(i)}))</script><p>将$ELBO$记为$L$，然后通过优化$L$，来间接优化似然。<br>$L$可以写作：</p><script type="math/tex; mode=display">L(\theta, \varPhi; x^{(i)})) = -KL(q_\varPhi(z|x^{(i)})||p_\theta(z)) + E_{q_\varPhi(z|x)}[\log p_\theta(x^{(i)}|z)]</script><p>优化目标变为后面两项。</p><p>具体的推导可以参考<a href="https://arxiv.org/pdf/1606.05908.pdf" target="_blank" rel="noopener">这个教程</a>。</p><p>更多地，在建模文字数据的时候，也可以使用KL-term annealing以及dropout of inputs of the decoder等训练技巧，避免一些问题。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章发表在2018年的AAAI上，继承了VAE在自然语言处理上的应用，将其应用于句子复述上，用于生成相似的句子；同时因为RNN可以作为语言模型使用，语句的语法正确性也有一定提升。&lt;br&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1709.05074.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1709.05074.pdf&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="循环神经网络" scheme="http://ldzhangyx.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="变分自编码器" scheme="http://ldzhangyx.github.io/tags/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
      <category term="自动问答" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E5%8A%A8%E9%97%AE%E7%AD%94/"/>
    
      <category term="句子复述" scheme="http://ldzhangyx.github.io/tags/%E5%8F%A5%E5%AD%90%E5%A4%8D%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>《XiaoIce Band: A Melody and Arrangement Generation Framework for Pop Music》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/26/xiaoice-band/"/>
    <id>http://ldzhangyx.github.io/2018/09/26/xiaoice-band/</id>
    <published>2018-09-26T08:47:41.000Z</published>
    <updated>2018-09-26T22:50:26.848Z</updated>
    
    <content type="html"><![CDATA[<p>仍在施工中……</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;仍在施工中……&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>《Multi-modal Sentence Summarization with Modality Attention and Image Filtering》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/25/Multi-modal%20Sentence%20Summarization%20with%20Modality%20Attention%20and%20Image%20Filtering/"/>
    <id>http://ldzhangyx.github.io/2018/09/25/Multi-modal Sentence Summarization with Modality Attention and Image Filtering/</id>
    <published>2018-09-25T20:07:31.000Z</published>
    <updated>2018-09-26T07:07:45.350Z</updated>
    
    <content type="html"><![CDATA[<p>宗成庆老师的这篇文章发表于<a href="https://acl2018.org/programme/papers/" target="_blank" rel="noopener">ACL’18</a>，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。<br>原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf</p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文介绍了一种多模态句子摘要任务，输入为一张图片和一对句子，输出为摘要。作者称其为MMSS任务。任务的难点在于怎么将视觉信息输入到框架里，以及怎么减小噪音。针对这两个问题，作者分别提出了两个方法：同时施加不同的注意力到文本和图片上；使用图像过滤Image Filter来降噪。</p><p>介绍主要是讲了文本摘要的历史，以及多模态方法最早应用在翻译领域，表现特别好，但是作者认为在摘要上表现得应该更好。</p><p>在解决MMSS任务的时候，作者准备使用分层注意力机制，底层分别关注图片和文本的内部，而上层对两个模态进行平衡。因为图片不能表现很多抽象内容，所以图片特征需要过滤去噪；为了解决生成句子结巴的问题，使用了coverage方法。</p><p>顺便他们做了一个数据集，真是让人肝疼的工作量。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图片展示了多模态模型的实际效果要好于文本模型。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>模型可见由句子编码器，图片编码器，摘要解码器和图片过滤器四个部分组成。</p><h3 id="模型图"><a href="#模型图" class="headerlink" title="模型图"></a>模型图</h3><p>模型简图如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="句子Encoder"><a href="#句子Encoder" class="headerlink" title="句子Encoder"></a>句子Encoder</h3><p>模型的句子Encoder使用了Bi-GRU。</p><script type="math/tex; mode=display">\overrightarrow{h}_i =  GRU(E[x_i], \overrightarrow{h}_{i-1})</script><script type="math/tex; mode=display">\overleftarrow{h}_i =  GRU(E[x_i], \overleftarrow{h}_{i-1})</script><h3 id="图片Encoder"><a href="#图片Encoder" class="headerlink" title="图片Encoder"></a>图片Encoder</h3><p>图片Encoder使用了VGG19，抽取了两种特征：7x7x512的局部特征（flatten之后成为了49x512），和4096维的全局特征。其中局部特征表示为：</p><script type="math/tex; mode=display">A = (a_1, ..., a_L), L=49 \\a_l \in \mathbb{R}^{512}</script><h3 id="摘要Decoder"><a href="#摘要Decoder" class="headerlink" title="摘要Decoder"></a>摘要Decoder</h3><p>Decoder使用了单向GRU：</p><script type="math/tex; mode=display">s_t = GRU(s_{t-1}, y_{t-1}, c_t)</script><p>其中，初始状态$s_0$，作者提出了几种初始化策略：</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h2}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{q}q)</script><p>利用了全局特征；</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_i)</script><p>与</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_ia_i)</script><p>利用了局部特征；</p><p>其中，$q$为全局特征，$a$为各个局部特征，如上文所说。</p><h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><p>在Decoder使用的context向量$c_t$，使用一个顶层的Attention，对图片context和文本context进行平衡统一：</p><script type="math/tex; mode=display">c_t = \beta_t^{txt}\mathbf{V}_Tc_{txt} + \beta_t^{img}\mathbf{V}_Ic_{img}</script><p>其中，</p><script type="math/tex; mode=display">\beta_t^{txt} = \sigma(\mathbf{U}_as_{t-1}+\mathbf{W}_ac_{txt})</script><script type="math/tex; mode=display">\beta_t^{img} = \sigma(\mathbf{U}_bs_{t-1}+\mathbf{W}_bc_{img})</script><p>对于文字的attention，使用普通的attention：</p><script type="math/tex; mode=display">c_{txt} = \sum_{i=1}^N\alpha_{t,i}^{txt}h_i</script><p>其中$N$为输入的文字序列长度，也就是embedding过后的向量总数量，$h_{i}$是Encoder的第$i$个Hidden State，这个公式将其加权求和。其中$\alpha_{t,i}^{txt}$与Decoder上一个时态的状态$s_{t-1}$与第$i$个Encoder的Hidden State，$h_{i}$有关。</p><script type="math/tex; mode=display">\alpha_{t,i}^{txt} = \frac{exp(e_{t,i}^{txt})}{\sum_{j=1}^Nexp(e_{t,j}^{txt})}</script><script type="math/tex; mode=display">e_{t,i}^{txt} = f(s_{t-1}, h_i) = v_a^T\tanh(\mathbf{U}_cs_{t-1} + \mathbf{W}_ch_i)</script><p>而对于图片attention，作者将每个feature map作为输入，进行attention处理：</p><script type="math/tex; mode=display">c_{img} = \sum_{i=1}^L\alpha_{t,i}^{img}a_l</script><p>其中L为feature map数量，$a_l$为局部特征。</p><script type="math/tex; mode=display">\alpha_{t,i}^{img} = \frac{exp(e_{t,i}^{img})}{\sum_{j=1}^Nexp(e_{t,j}^{img})}</script><script type="math/tex; mode=display">e_{t,i}^{img} = f(s_{t-1}, h_i) = v_b^T\tanh(\mathbf{U}_ds_{t-1} + \mathbf{W}_da_l)</script><h3 id="Coverage-Model-覆盖率模型"><a href="#Coverage-Model-覆盖率模型" class="headerlink" title="Coverage Model(覆盖率模型)"></a>Coverage Model(覆盖率模型)</h3><p>覆盖率模型可以参考《Get to the point》一文的论文笔记。</p><h3 id="Image-Filter"><a href="#Image-Filter" class="headerlink" title="Image Filter"></a>Image Filter</h3><p>作者设计了两个filter：Image Attention Filter和Image Context Filter。</p><h4 id="Image-Attention-Filter"><a href="#Image-Attention-Filter" class="headerlink" title="Image Attention Filter"></a>Image Attention Filter</h4><p>Image Attention Filter的目的在于“directly applied to change the attention scale between image and text”，即根据图片与文本的相关性进行数值控制。</p><script type="math/tex; mode=display">I_a = \sigma(v_s^Ts_0 + c_q^Tq + v_r^Ts_{t-1})</script><p>然后用这个系数更新：</p><script type="math/tex; mode=display">\beta_t^{img} = I_a \cdot \beta_t^{img}</script><p>其中$s_{0}$是decoder的初始状态，$q$是图片全局特征，这两个参数用来表示图片相关性；$s_{t−1}$是decoder上一个time step的状态，用来表示与下一个单词的联系。</p><h4 id="Image-Context-Filter"><a href="#Image-Context-Filter" class="headerlink" title="Image Context Filter"></a>Image Context Filter</h4><p>对于Image Context Vector，作者解释是脱胎于以前的思路（“Image context filter is partially inspired by gating mechanism which has gained great popularity in neural network models”），但是应用在多模态方法上仍属创新。</p><script type="math/tex; mode=display">I_a = \sigma(\mathbf{W}_ss_0 + \mathbf{W}_qq + \mathbf{W}_rs_{t-1})</script><p>这里的矩阵相乘可以使用Dense Layer实现，但是$I_c$与$I_a$最大的不同在于，$I_c$是一个向量，其元素用于控制特征的选择。</p><script type="math/tex; mode=display">c_{img} = I_{c} \odot c_{img}</script><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>本文贡献了一个MMSS任务使用的数据集。数据集首先从Gigaword Corpus里作为基础，对每个Headline，使用Yahoo!搜索图片，并取出top 5图片。之后删除无关图片（人物，缩略图，广告），雇佣10个研究生选择最匹配的图片（无图片标0），最后得到66,000条可用数据。随机将62,000作为训练集，2,000作为测试集，2,000作为开发集。</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>作者做了广泛的对比：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>Q：对Decoder状态进行的初始化有效吗？<br>A：有效。</p><p>Q：图片有助于提升摘要质量吗？<br>A：是的，可能是因为图片提供了更多信息。</p><p>Q：哪种图片更有效？<br>A：要求3个研究生标注300条数据中图片的匹配程度（匹配从1到3），发现更加匹配的图片所在的那条数据，模型能获得更高的ROUGE分数。</p><p>Q：多模态coverage的有效性？<br>A：通过计算重复词，textual与visual coverage确实能减少重复词的出现。</p><h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><h2 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h2><p>这篇文章的思路与一般的额外信息有一些区别，同是用额外信息干涉指导文本生成，这个模型同时使用了两个attention，并提出了一种加权机制将两个attention组合起来。在我读过的另一篇文章《Diversity driven Attention Model for Query-based Abstractive Summarization》中，作者试图用query的context干涉document的context，而不直接显性参与decoder的过程。</p><p>文中计算权值的时候，充分考虑了各种可能性；在Image Attention Filter那一块，将数个特征非线性组合起来，虽然显得参数有点多，好在不无道理。</p><p>贡献了一个全新的数据集（动用了10个研究生，真有钱），脱胎于Gigawords，对这个领域做出了基础性贡献。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>首先，对于数据集，我认为可以选择替代的数据集，可能是我之前做过中文摘要，我很自然地就想到了LCSTS，可以用同样的方法构建数据集，但是这个很费时间精力，而且并不是什么突出的想法贡献。</p><p>其次，为什么用VGG提取特征？在这个框架里，VGG提取特征取到的效果我持保留态度。即使有了图片特征又能怎么样呢？到底是一个怎样的机制让VGG的图片特征与关键字对上的？我认为这样的attention拼凑框架思路时非常棒的，但是图片特征与文字的多模态映射我始终不明白how it works. 作为替代方法，我很自然地想到了CV里的目标检测，使用选择性搜索，SVM判断图像中的实体，再作为特征送进模型，我认为这是一个更接近直觉的做法。</p><p>再次，Image Filtering这个做法我认为需要改进为更加reasonable的做法。我们完全可以做一个key-word版本的Filter。Filter有两个版本，Image Attention Filter的系数$I_a$是根据图像与文本的相关性来控制图像干预的程度；而Image Context Filter的系数$T_c$是用来突出图像特征的。这个想法理应可以迁移到word的使用上。</p><p>最后，文中用到的小trick，textual coverage mechanism，为了解决结巴问题，我们可以考虑其他的机制，比如将context vector做软正交化处理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;宗成庆老师的这篇文章发表于&lt;a href=&quot;https://acl2018.org/programme/papers/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ACL’18&lt;/a&gt;，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。&lt;br&gt;原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="多模态" scheme="http://ldzhangyx.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
      <category term="生成式摘要" scheme="http://ldzhangyx.github.io/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%91%98%E8%A6%81/"/>
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
