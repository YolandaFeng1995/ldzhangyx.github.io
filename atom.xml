<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张逸霄的技术小站</title>
  
  <subtitle>欢迎RSS订阅我的个人主页！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ldzhangyx.github.io/"/>
  <updated>2020-01-07T04:15:58.719Z</updated>
  <id>http://ldzhangyx.github.io/</id>
  
  <author>
    <name>张逸霄</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MuseScore 3：快速使用小记</title>
    <link href="http://ldzhangyx.github.io/2020/01/07/musescore-1/"/>
    <id>http://ldzhangyx.github.io/2020/01/07/musescore-1/</id>
    <published>2020-01-07T04:15:58.000Z</published>
    <updated>2020-01-07T04:15:58.719Z</updated>
    
    <content type="html"><![CDATA[<p>MuseScore 3是一款基本功能免费的软件，主要用处是打谱，以及作为各种音乐文件的阅读和编辑器。这篇文章旨在让你快速上手MuseScore。</p><a id="more"></a><h1 id="新建乐谱后，如何输入音符？"><a href="#新建乐谱后，如何输入音符？" class="headerlink" title="新建乐谱后，如何输入音符？"></a>新建乐谱后，如何输入音符？</h1><p>你可以使用鼠标，midi键盘，普通键盘输入。</p><h2 id="鼠标"><a href="#鼠标" class="headerlink" title="鼠标"></a>鼠标</h2><p>按下左上角的音符“N”按钮，选择工具栏中对应的音符，点击乐谱放置到合理的位置上。</p><h2 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h2><ul><li><p>按下“N”键可以打开输入模式。</p></li><li><p>按下Esc键可以退出输入模式。</p></li><li><p>数字键1-9可以选择不同时长的音符。休止符是0键。</p></li><li><p>输入音名即得到对应音符。如敲击C键可以在乐谱当前位置输入一个C音。</p></li><li><p>方向键上下可以对这个音符做升降。</p></li><li><p>方向键左右可以选择不同的乐谱位置输入。</p></li><li><p>Ctrl + 方向键上下可以对音符移八度。</p></li><li><p>Shift + 音名，可以直接打出和弦。</p></li></ul><h2 id="Midi键盘"><a href="#Midi键盘" class="headerlink" title="Midi键盘"></a>Midi键盘</h2><ul><li><p>可以通过midi键盘选择对应音高，用键盘辅助输入。</p></li><li><p>同时弹下的音会被同时记录。</p></li></ul><h1 id="工具箱"><a href="#工具箱" class="headerlink" title="工具箱"></a>工具箱</h1><p>工具箱中所有的工具可以通过拖拽和双击的方法添加到乐谱中。</p><h1 id="选中音符"><a href="#选中音符" class="headerlink" title="选中音符"></a>选中音符</h1><ul><li><p>单击音符可以选中。</p></li><li><p>按下ctrl可以多选。</p></li><li><p>按下shift可以拉一个框，自动选中框内所有音符。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MuseScore 3是一款基本功能免费的软件，主要用处是打谱，以及作为各种音乐文件的阅读和编辑器。这篇文章旨在让你快速上手MuseScore。&lt;/p&gt;
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music" scheme="http://ldzhangyx.github.io/tags/music/"/>
    
      <category term="musescore" scheme="http://ldzhangyx.github.io/tags/musescore/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 1.2版本中，mask.bernoulli_()的问题记录和解决方案</title>
    <link href="http://ldzhangyx.github.io/2019/12/09/bernoulli-bool/"/>
    <id>http://ldzhangyx.github.io/2019/12/09/bernoulli-bool/</id>
    <published>2019-12-09T14:43:07.000Z</published>
    <updated>2019-12-10T08:03:06.148Z</updated>
    
    <content type="html"><![CDATA[<p>最近coding的时候遇到一个问题。以前都没出现，突然出现了。暂时没法定位是不是PyTorch降级的问题。</p><p>环境：PyTorch 1.2， CUDA 10，cudatoolkit 0.40</p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>在mask函数里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_mask</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.dropout == <span class="number">0.</span>:</span><br><span class="line">        self._weight = self.weight</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask = self.weight.new_empty(</span><br><span class="line">            self.weight.size(),</span><br><span class="line">            dtype=torch.uint8</span><br><span class="line">        )</span><br><span class="line">        mask.bernoulli_(self.dropout)</span><br><span class="line">        self._weight = self.weight.masked_fill(mask, <span class="number">0.</span>)</span><br></pre></td></tr></table></figure><p>mask是一个只有1和0的矩阵，用于dropout实现。在定义mask时，如果将mask定义为<code>torch.uint8</code>类型，在<code>bernoulli()</code>函数里会报如下warning：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.</span><br></pre></td></tr></table></figure><p>提示初始化为bool类型。初始化后，下一步的<code>bernoulli()</code>函数会报另一个错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: &quot;bernoulli_scalar_cuda_&quot; not implemented for &apos;Bool&apos;</span><br></pre></td></tr></table></figure><p>提示<code>bernoulli</code>函数不支持对bool的运算。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>在<code>bernoulli()</code>和<code>bernoulli()</code>中间做一次类型转换：<code>mask.bool()</code></p><p>完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_mask</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.dropout == <span class="number">0.</span>:</span><br><span class="line">        self._weight = self.weight</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask = self.weight.new_empty(</span><br><span class="line">            self.weight.size(),</span><br><span class="line">            dtype=torch.uint8</span><br><span class="line">        )</span><br><span class="line">        mask.bernoulli_(self.dropout)</span><br><span class="line">        mask = mask.bool()</span><br><span class="line">        self._weight = self.weight.masked_fill(mask, <span class="number">0.</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近coding的时候遇到一个问题。以前都没出现，突然出现了。暂时没法定位是不是PyTorch降级的问题。&lt;/p&gt;
&lt;p&gt;环境：PyTorch 1.2， CUDA 10，cudatoolkit 0.40&lt;/p&gt;
&lt;h1 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; c
      
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="python" scheme="http://ldzhangyx.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://ldzhangyx.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 1.3及以上版本关于forward函数@staticmethod装饰器的一个坑和解决方案</title>
    <link href="http://ldzhangyx.github.io/2019/12/04/pytorch-skill-1204/"/>
    <id>http://ldzhangyx.github.io/2019/12/04/pytorch-skill-1204/</id>
    <published>2019-12-03T20:39:41.000Z</published>
    <updated>2019-12-03T20:49:25.820Z</updated>
    
    <content type="html"><![CDATA[<p>使用PyTorch编写代码的时候，我们通常将模型整合进一个类，在init方法里声明模型结构，在forward方法里约定模型里数据的正向流动，然后PyTorch自动生成数据的方向传播backward方法。</p><p>PyTorch在1.3版本及之后，规定forward方法必须是静态方法。违反了该原则的代码将会在运行时报下列错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. &quot;,&quot;Please use new-style autograd function withstatic forward method.&quot;,&quot;(Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)&quot;.</span><br></pre></td></tr></table></figure><p>于是一般的解决办法是，在forward方法声明上一行加入<code>@staticmethod</code>装饰器，即可完成修改。</p><p>但是在一些没那么规范的代码下，这个改进可能会成为我们的绊脚石。如：<a href="https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html" target="_blank" rel="noopener">https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html</a></p><p>这一篇文章里提到的代码，其训练和推断使用了两个不同的路径，使得模型结构比较混乱。我自行尝试后，发现无论如何没有办法解决掉这个问题。</p><p>一个临时的解决办法是：</p><ol><li><p>将PyTorch降级到1.2版本及以下，因为这个改进直到1.3版本才正式生效；</p></li><li><p>手动忽略掉UserWarning：</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用PyTorch编写代码的时候，我们通常将模型整合进一个类，在init方法里声明模型结构，在forward方法里约定模型里数据的正向流动，然后PyTorch自动生成数据的方向传播backward方法。&lt;/p&gt;
&lt;p&gt;PyTorch在1.3版本及之后，规定forward方法
      
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="pytorch" scheme="http://ldzhangyx.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>MuseScore 3：快速使用小记</title>
    <link href="http://ldzhangyx.github.io/2019/11/26/musescore/"/>
    <id>http://ldzhangyx.github.io/2019/11/26/musescore/</id>
    <published>2019-11-26T14:02:17.000Z</published>
    <updated>2019-11-26T14:09:40.812Z</updated>
    
    <content type="html"><![CDATA[<p>MuseScore 3是一款基本功能免费的软件，主要用处是打谱，以及作为各种音乐文件的阅读和编辑器。这篇文章旨在让你快速上手MuseScore。</p><a id="more"></a><h1 id="新建乐谱后，如何输入音符？"><a href="#新建乐谱后，如何输入音符？" class="headerlink" title="新建乐谱后，如何输入音符？"></a>新建乐谱后，如何输入音符？</h1><p>你可以使用鼠标，midi键盘，普通键盘输入。</p><h2 id="鼠标"><a href="#鼠标" class="headerlink" title="鼠标"></a>鼠标</h2><p>按下左上角的音符“N”按钮，选择工具栏中对应的音符，点击乐谱放置到合理的位置上。</p><h2 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h2><ul><li><p>按下“N”键可以打开输入模式。</p></li><li><p>按下Esc键可以退出输入模式。</p></li><li><p>数字键1-9可以选择不同时长的音符。休止符是0键。</p></li><li><p>输入音名即得到对应音符。如敲击C键可以在乐谱当前位置输入一个C音。</p></li><li><p>方向键上下可以对这个音符做升降。</p></li><li><p>方向键左右可以选择不同的乐谱位置输入。</p></li><li><p>Ctrl + 方向键上下可以对音符移八度。</p></li><li><p>Shift + 音名，可以固定住当前位置，这样依次打音名可以打出一个和弦。</p></li></ul><h2 id="Midi键盘"><a href="#Midi键盘" class="headerlink" title="Midi键盘"></a>Midi键盘</h2><ul><li><p>可以通过midi键盘选择对应音高，用键盘辅助输入。</p></li><li><p>同时弹下的音会被同时记录。</p></li></ul><h1 id="工具箱"><a href="#工具箱" class="headerlink" title="工具箱"></a>工具箱</h1><p>工具箱中所有的工具可以通过拖拽和双击的方法添加到乐谱中。</p><h1 id="选中音符"><a href="#选中音符" class="headerlink" title="选中音符"></a>选中音符</h1><ul><li><p>单击音符可以选中。</p></li><li><p>按下ctrl可以多选。</p></li><li><p>按下shift可以拉一个框，自动选中框内所有音符。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MuseScore 3是一款基本功能免费的软件，主要用处是打谱，以及作为各种音乐文件的阅读和编辑器。这篇文章旨在让你快速上手MuseScore。&lt;/p&gt;
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music" scheme="http://ldzhangyx.github.io/tags/music/"/>
    
      <category term="musescore" scheme="http://ldzhangyx.github.io/tags/musescore/"/>
    
  </entry>
  
  <entry>
    <title>《Poly-tree： A Tree Structure towards Better Polyphonic Music Representation Learning》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/10/30/polytree/"/>
    <id>http://ldzhangyx.github.io/2019/10/30/polytree/</id>
    <published>2019-10-30T10:05:10.000Z</published>
    <updated>2019-10-30T10:05:10.618Z</updated>
    
    <content type="html"><![CDATA[<p>组里的最新工作，投到了ICASSP 2020，提出了一种复调音乐的树结构，并利用结构设计了适当的实验，证明其有效性。</p><a id="more"></a><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>复调音乐是一种共时、离散的时间序列新高，通常被表示为1-d的events序列，或者2d的piano roll，缺点是music knowledge不够多，不能体现复调音乐的内在结构。本文提出poly-tree的树结构，包含三个级别：时间序列——音符——音符属性。同时本文进一步提出了一个Encoder-Decoder网络，以学习复调音乐的latent representation（VAE中的<code>z</code>）。在钢琴表征学习任务实验结果显示，poly-tree在<strong>重建准确性</strong>和<strong>模型泛化</strong>方面优于baseline。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对于复调音乐，我们不仅可以从信号处理的角度理解，还可以从音乐本身来理解。复调音乐有metrical structure和pitch order的概念，metrical structure指的是音符之间的时间关系，它们形成了节奏模式；而pitch order与音程有关，可以由此构建harmony（和声）的概念。这两个属性可以一起组成更高级别的音乐结构，例如repetition和sequence。因此，如果数据结构<strong>优先</strong>反映这两个属性，而将其他细节作为次要信息处理。</p><p><img src="1.png" alt=""></p><p>poly-tree的主要结构如下：</p><p><img src="2.png" alt=""></p><p>可以看到poly-tree顶层是time-step sequence，每一个time-step，包含了一个list，容纳<strong>同时弹下的音符</strong>；中层的list里，每一个音符由其基本属性概括（如pitch和duration）。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>Midi-like event使用这样的方式表示：</p><p><img src="3.png" alt=""></p><p>Piano-Roll是用这样的方式表示：</p><p><img src="4.png" alt=""></p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="5.png" alt=""></p><p>上面乐谱被poly-tree以下面的方式表示：</p><p><img src="6.png" alt=""></p><p>时间维度上，音乐以不定长数量的block表示。每一个block搭载了不定长数量的此时onset的音符。音符以(128 + N)长度的向量表示，其中128代表pitch，N的长度取决于音乐片段的总长度。如果音乐片段长度为8拍，分辨率为16分音符，意味着一个音符的duration可能为1（16分）~32（双全），共32种表示，则N=5.</p><p>因为时间维度上的block数量和block里同时出现的音符都是数量不同的，故在两个维度各用一次GRU。第一个GRU将block里的音符做转换，再在时间维度上用GRU将转换后的结果再转换为latent representation。如图：</p><p><img src="7.png" alt=""></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>数据集使用了Musicalion Dataset和POP909 Dataset，前者是古典音乐钢琴数据集，后面的数据集是组里新公开的流行音乐钢琴数据集。</p><h2 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h2><p>对于midi-event表征，使用循环VAE；对于piano-roll表征，训练循环VAE，卷积VAE，全连接VAE，后两者是图像级别的操作。具体参数不表，可以阅读原论文。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="8.png" alt=""></p><p><img src="9.png" alt=""></p><p>可以发现新的表征表现得很好，从主观实验结果也能听出其有更高的质量。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;组里的最新工作，投到了ICASSP 2020，提出了一种复调音乐的树结构，并利用结构设计了适当的实验，证明其有效性。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="MIR" scheme="http://ldzhangyx.github.io/tags/MIR/"/>
    
      <category term="representation learning" scheme="http://ldzhangyx.github.io/tags/representation-learning/"/>
    
      <category term="polyphonic music" scheme="http://ldzhangyx.github.io/tags/polyphonic-music/"/>
    
  </entry>
  
  <entry>
    <title>本站评论系统即日起升级为gitalk组件</title>
    <link href="http://ldzhangyx.github.io/2019/10/23/update-1023/"/>
    <id>http://ldzhangyx.github.io/2019/10/23/update-1023/</id>
    <published>2019-10-23T06:16:12.000Z</published>
    <updated>2019-10-23T08:11:41.175Z</updated>
    
    <content type="html"><![CDATA[<p>本站之前的评论系统使用了valine，评论并不能被即时地阅读。升级了评论系统后，gitalk将访客的评论转为issue添加到GitHub repo中，这样我可以及时收到评论提醒。</p><p>另外，最有效和快速的联系方法是<strong>点击首页头像下的邮箱链接，向我发送邮件</strong>。技术讨论类问题会在一天内进行回复。</p><h1 id="Gitalk配置中踩到的坑"><a href="#Gitalk配置中踩到的坑" class="headerlink" title="Gitalk配置中踩到的坑"></a>Gitalk配置中踩到的坑</h1><ul><li><p>Gitalk.ejs可能获取不到_config.yml的参数，使得OAuth链接的参数为空，导致404 Error问题。可以考虑将gitalk.ejs的参数直接硬编码。</p></li><li><p>配置完成后需要手动访问一次文章页面，完成此文章的评论区初始化。文章不多的话可以考虑全部打开一遍，多的话可以考虑自动脚本。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本站之前的评论系统使用了valine，评论并不能被即时地阅读。升级了评论系统后，gitalk将访客的评论转为issue添加到GitHub repo中，这样我可以及时收到评论提醒。&lt;/p&gt;
&lt;p&gt;另外，最有效和快速的联系方法是&lt;strong&gt;点击首页头像下的邮箱链接，向我发送
      
    
    </summary>
    
    
      <category term="others" scheme="http://ldzhangyx.github.io/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>Python两个代码技巧</title>
    <link href="http://ldzhangyx.github.io/2019/10/16/python-trick-1016/"/>
    <id>http://ldzhangyx.github.io/2019/10/16/python-trick-1016/</id>
    <published>2019-10-16T08:50:16.000Z</published>
    <updated>2019-10-16T08:50:16.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="字符串格式化：F-string（v3-6-）"><a href="#字符串格式化：F-string（v3-6-）" class="headerlink" title="字符串格式化：F-string（v3.6+）"></a>字符串格式化：F-string（v3.6+）</h1><p>之前版本的Python给字符串赋值会使用<code>%</code>和<code>.format()</code>两种方法。F-string提供了第三种方法。</p><p>F-string的使用例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">name = <span class="string">'Bob'</span></span><br><span class="line">age = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">f"My name is <span class="subst">&#123;name&#125;</span>, and my age is <span class="subst">&#123;age&#125;</span>."</span>)</span><br></pre></td></tr></table></figure><p>这种表达在阅读代码的时候能让人更加容易将位置和变量对应起来。</p><p>PEP 498中提到，其规范的格式应当是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f &apos; &lt;text&gt; &#123; &lt;expression&gt; &lt;optional !s, !r, or !a&gt; &lt;optional : format specifier&gt; &#125; &lt;text&gt; ... &apos;</span><br></pre></td></tr></table></figure><h1 id="赋值的表达式（Assignment-Expressions，-v3-8-）"><a href="#赋值的表达式（Assignment-Expressions，-v3-8-）" class="headerlink" title="赋值的表达式（Assignment Expressions， v3.8+）"></a>赋值的表达式（Assignment Expressions， v3.8+）</h1><p>这是Python 3.8加入的新功能。众所周知Python的<code>=</code>赋值运算符不返回任何值。新增的赋值表达式运算符<code>:=</code>可以完成运算，同时将赋值结果返回。</p><p>这使得以下代码成为可能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (result := function()) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><p>而如果用普通的<code>=</code>运算符，只能以额外声明变量实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = function()</span><br><span class="line"><span class="keyword">if</span> result <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><p>或重复计算表达式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> function() <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">print(function())</span><br></pre></td></tr></table></figure><p>新的语法糖改善了此过程。</p><p>PEP 572完整阐述了这个语法糖的使用。给出的其他例子如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    line = fp.readline()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><p>可以简化成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (line := fp.readline()):</span><br><span class="line">    print(line)</span><br></pre></td></tr></table></figure><p>以及列表推导式可以从</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results = [</span><br><span class="line">    f(x) <span class="keyword">for</span> x <span class="keyword">in</span> data</span><br><span class="line">    <span class="keyword">if</span> f(x)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>改进以避免重复运算，到：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results = [</span><br><span class="line">    y <span class="keyword">for</span> x <span class="keyword">in</span> data</span><br><span class="line">    <span class="keyword">if</span> (y := f(x))</span><br><span class="line">]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;字符串格式化：F-string（v3-6-）&quot;&gt;&lt;a href=&quot;#字符串格式化：F-string（v3-6-）&quot; class=&quot;headerlink&quot; title=&quot;字符串格式化：F-string（v3.6+）&quot;&gt;&lt;/a&gt;字符串格式化：F-string（v3.6
      
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="http://ldzhangyx.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>使音乐模型更好地训练的一些技巧（2019.10.16）</title>
    <link href="http://ldzhangyx.github.io/2019/10/16/practice-1016/"/>
    <id>http://ldzhangyx.github.io/2019/10/16/practice-1016/</id>
    <published>2019-10-16T07:17:32.000Z</published>
    <updated>2019-11-11T07:30:47.266Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PyTorch与TensorBoard的交互"><a href="#PyTorch与TensorBoard的交互" class="headerlink" title="PyTorch与TensorBoard的交互"></a>PyTorch与TensorBoard的交互</h1><p>PyTorch最近一直在改进TensorBoard的支持，包括在最新v1.3中，也在一直改进。</p><p>最基本的使用方法是，新建一个<code>SummaryWriter</code>对象，之后在合适的位置进行<code>add_scalar()</code>记录，最后<code>close()</code>即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line">writer.add_scalar(<span class="string">"valid loss"</span>, loss, n_iter) <span class="comment"># 三个参数代表变量，数值，横坐标</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>文件会保存在代码的<code>.run/</code>目录里，访问<code>localhost:6006</code>即可进行观察。</p><h1 id="音乐数据集的移调数据扩增"><a href="#音乐数据集的移调数据扩增" class="headerlink" title="音乐数据集的移调数据扩增"></a>音乐数据集的移调数据扩增</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_X_augment = []</span><br><span class="line">train_Y_augment = []</span><br><span class="line"><span class="keyword">for</span> i, target <span class="keyword">in</span> enumerate(train_Y):</span><br><span class="line">    train_X_augment.append(pad(train_X[i], pad_size))</span><br><span class="line">    train_Y_augment.append(pad(train_Y[i], pad_size))</span><br><span class="line">    <span class="keyword">if</span> augment_data:</span><br><span class="line">        <span class="keyword">for</span> direction <span class="keyword">in</span> [<span class="number">-1</span>, <span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">for</span> shift <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">12</span>):</span><br><span class="line">                train_X_temp = (train_X[i]).clone()</span><br><span class="line">                train_X_temp[:] += direction * shift</span><br><span class="line">                train_X_augment.append(pad(train_X_temp, pad_size))</span><br><span class="line">                train_Y_augment.append(pad(train_Y[i], pad_size))</span><br><span class="line">train_X = torch.stack(train_X_augment)</span><br><span class="line">train_Y= torch.stack(train_Y_augment)</span><br></pre></td></tr></table></figure><h1 id="统计模型总参数量"><a href="#统计模型总参数量" class="headerlink" title="统计模型总参数量"></a>统计模型总参数量</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">params = list(model.parameters())</span><br><span class="line">total_params = sum(x.size()[<span class="number">0</span>] * x.size()[<span class="number">1</span>] <span class="keyword">if</span> len(x.size()) &gt; <span class="number">1</span> <span class="keyword">else</span> x.size()[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> params <span class="keyword">if</span> x.size())</span><br><span class="line">print(<span class="string">'Args:'</span>, args)</span><br><span class="line">print(<span class="string">'Model total parameters:'</span>, total_params)</span><br></pre></td></tr></table></figure><h1 id="CUDA-out-of-memory问题"><a href="#CUDA-out-of-memory问题" class="headerlink" title="CUDA out of memory问题"></a>CUDA out of memory问题</h1><p>模型在做evaluate的时候经常遇到CUDA out of memory的问题。常见的做法是用<code>with torch.no_grad()</code>方法将对应的代码块包起来，使模型不存储梯度。</p><p>注意，<code>model.eval()</code>代码执行的仅为<code>dropout</code>层和<code>batchnorm</code>层固定的功能，并不能达到<code>torch.no_grad()</code>这样的效果。</p><h1 id="int与one-hot的转换"><a href="#int与one-hot的转换" class="headerlink" title="int与one-hot的转换"></a>int与one-hot的转换</h1><p><code>torch.nn.functional.one_hot(tensor, num_classes=-1)</code>可以直接将int转为one-hot。</p><p>转回来可以通过这种办法：</p><p><code>melody1[0].tolist().index(1)</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PyTorch与TensorBoard的交互&quot;&gt;&lt;a href=&quot;#PyTorch与TensorBoard的交互&quot; class=&quot;headerlink&quot; title=&quot;PyTorch与TensorBoard的交互&quot;&gt;&lt;/a&gt;PyTorch与TensorBoard的
      
    
    </summary>
    
      <category term="开发笔记" scheme="http://ldzhangyx.github.io/categories/%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="心得体会" scheme="http://ldzhangyx.github.io/tags/%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>音乐相关会议的deadline一览</title>
    <link href="http://ldzhangyx.github.io/2019/09/12/music-conference-deadline/"/>
    <id>http://ldzhangyx.github.io/2019/09/12/music-conference-deadline/</id>
    <published>2019-09-12T15:05:35.000Z</published>
    <updated>2019-10-16T07:23:54.398Z</updated>
    
    <content type="html"><![CDATA[<p>网站运营的简要介绍。</p><a id="more"></a><p>之前看到aideadlin.es对AI各个领域会议做了一个汇总。音乐领域没有专门的网站做汇集，于是我fork过来重新整理发布了一个网站，挂载在我特意注册的小号上。</p><p>网址是：yixiao-music.github.io，直接点这个网页的侧栏tab也可以直接进去。</p><p><img src="1.png" alt=""></p><p>网页更新采用ISMIR社区的回复更新：<a href="https://groups.google.com/a/ismir.net/forum/#!topic/community/8CnJfljcx0E" target="_blank" rel="noopener">https://groups.google.com/a/ismir.net/forum/#!topic/community/8CnJfljcx0E</a></p><p>我计划手动根据回复对这个网站进行数据更新，以避免我的GitHub小号因不常登陆而收不到PR的情况。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;网站运营的简要介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="其他" scheme="http://ldzhangyx.github.io/categories/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="MIR" scheme="http://ldzhangyx.github.io/tags/MIR/"/>
    
      <category term="community" scheme="http://ldzhangyx.github.io/tags/community/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch中，LSTM的两种不同形式</title>
    <link href="http://ldzhangyx.github.io/2019/09/11/lstm/"/>
    <id>http://ldzhangyx.github.io/2019/09/11/lstm/</id>
    <published>2019-09-11T10:20:56.000Z</published>
    <updated>2019-09-11T10:20:56.885Z</updated>
    
    <content type="html"><![CDATA[<p>最近在LSTM这一块有点扯不清，现在回想一下发现自己将两种LSTM记混了。</p><a id="more"></a><h1 id="LSTM-input与label无关"><a href="#LSTM-input与label无关" class="headerlink" title="LSTM(input与label无关)"></a>LSTM(input与label无关)</h1><p>第一种是这样的：</p><p><img src="1.png" alt=""></p><p>t时刻的input x与t-1时刻的label y没有关系，完成的是一个序列到另一个序列的转换。</p><p>Neural Language Models通过input和output的错位，用一行LSTM完成操作。</p><p>PyTorch中，直接</p><h1 id="LSTM（input与label有关）"><a href="#LSTM（input与label有关）" class="headerlink" title="LSTM（input与label有关）"></a>LSTM（input与label有关）</h1><p>LSTM被广泛地用于构造Encoder-Decoder模型。Encoder部分没什么问题，但是Decoder部分与上面提到的结构不同，因为Decoder的过程是一步步解码的过程，是将t-1时刻应有的输出，传递给t时刻的cell。这意味着在编码的时候不能一步到位。</p><p>TensorFlow的解决方案是包装了一个decoder。</p><p>在训练的时候，PyTorch的官方文档框架（<a href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html）是这样的：" target="_blank" rel="noopener">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html）是这样的：</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">if use_teacher_forcing:</span><br><span class="line">    # Teacher forcing: Feed the target as the next input</span><br><span class="line">    for di in range(target_length):</span><br><span class="line">        decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">            decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">        loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">        decoder_input = target_tensor[di]  # Teacher forcing</span><br><span class="line"></span><br><span class="line">else:</span><br><span class="line">    # Without teacher forcing: use its own predictions as the next input</span><br><span class="line">    for di in range(target_length):</span><br><span class="line">        decoder_output, decoder_hidden, decoder_attention = decoder(</span><br><span class="line">            decoder_input, decoder_hidden, encoder_outputs)</span><br><span class="line">        topv, topi = decoder_output.topk(1)</span><br><span class="line">        decoder_input = topi.squeeze().detach()  # detach from history as input</span><br><span class="line"></span><br><span class="line">        loss += criterion(decoder_output, target_tensor[di])</span><br><span class="line">        if decoder_input.item() == EOS_token:</span><br><span class="line">            break</span><br></pre></td></tr></table></figure><p>可以看到Decoder是通过for循环进行逐步解码的。这和上面那类LSTM模型的运作规律<strong>不一致</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在LSTM这一块有点扯不清，现在回想一下发现自己将两种LSTM记混了。&lt;/p&gt;
    
    </summary>
    
      <category term="基础技巧" scheme="http://ldzhangyx.github.io/categories/%E5%9F%BA%E7%A1%80%E6%8A%80%E5%B7%A7/"/>
    
    
      <category term="PyTorch" scheme="http://ldzhangyx.github.io/tags/PyTorch/"/>
    
      <category term="基础" scheme="http://ldzhangyx.github.io/tags/%E5%9F%BA%E7%A1%80/"/>
    
      <category term="坑" scheme="http://ldzhangyx.github.io/tags/%E5%9D%91/"/>
    
  </entry>
  
  <entry>
    <title>诗歌生成模型“九歌”：《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/08/10/jiuge/"/>
    <id>http://ldzhangyx.github.io/2019/08/10/jiuge/</id>
    <published>2019-08-10T07:22:58.000Z</published>
    <updated>2019-08-10T07:30:27.415Z</updated>
    
    <content type="html"><![CDATA[<p>清华孙茂松老师组的工作，从效果来看，“九歌”的效果相当不错。适逢最近这个模型开源，我希望能梳理一下和这个模型有关的论文，尤其是《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》的工作（发表在EMNLP 2018）。</p><a id="more"></a><h1 id="关于模型开源的相关信息"><a href="#关于模型开源的相关信息" class="headerlink" title="关于模型开源的相关信息"></a>关于模型开源的相关信息</h1><p>今年7月，九歌模型开源。其诗歌生成的模型在这个链接里：<a href="https://github.com/THUNLP-AIPoet/StylisticPoetry" target="_blank" rel="noopener">https://github.com/THUNLP-AIPoet/StylisticPoetry</a></p><p>同时，他们组也放出了相关领域的论文列表：<a href="https://github.com/THUNLP-AIPoet/PaperList" target="_blank" rel="noopener">https://github.com/THUNLP-AIPoet/PaperList</a></p><p>以及诗歌生成的相关数据集：<a href="https://github.com/THUNLP-AIPoet/Datasets" target="_blank" rel="noopener">https://github.com/THUNLP-AIPoet/Datasets</a></p><p>有兴趣的话可以深入了解。</p><h1 id="相关论文梳理"><a href="#相关论文梳理" class="headerlink" title="相关论文梳理"></a>相关论文梳理</h1><p>《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》这篇论文在发表之后，被相关的后续论文引用。</p><ul><li><p>Sentiment-Controllable Chinese Poetry Generation</p></li><li><p>Rhetorically Controlled Encoder-Decoder for Modern Chinese Poetry Generation</p></li><li><p>Jiuge: A Human-Machine Collaborative Chinese Classical Poetry Generation System</p></li><li><p>GPT-based Generation for Classical Chinese Poetry（华为诺亚方舟实验室）</p></li></ul><h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>论文的目标在于无监督生成风格诗歌。之前的诗歌生成更注重于一致性、连续性。本文更关注于同一输入下生成不同风格的诗歌。</p><p>同一意象下，人们可以以不同的风格写出不同的诗歌：</p><p><img src="1.png" alt=""></p><ul><li>诗歌并没有明确的风格label。本文使用了无监督的方法生成不同风格的诗词。</li><li>论文中，我们提出了一个模型，可以从诗歌中解耦出不同的风格，并在给定人工风格输入后生成特定风格的诗歌。模型总体是seq2seq的，同时通过最大化衡量两个随机变量之间依赖的互信息，以强化人工风格输入和生成的特定风格输出的关系。</li><li>实验结果表明模型可以生成不同风格的诗歌而不丢失一致性和连贯性。</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>模型输入：$s_\text{input}$和风格id，$k \in K$，K为风格总数。</p><h3 id="互信息-Mutual-Information"><a href="#互信息-Mutual-Information" class="headerlink" title="互信息(Mutual Information)"></a>互信息(Mutual Information)</h3><p>互信息常被用于无监督生成模型。当我们遇到存在潜在的类别差别而没有标签数据，我们就需要一类能够无监督地辨别出这类潜在标签的数据。互信息在生成模型中被人们注意，最早应该是InfoGAN的提出。</p><p>给定两个随机变量$X$，$Y$，可以将两个随机变量的互信息记为$I(X, Y)$。互信息表示在一个随机变量中包含另一个随机变量的信息的数量。也可以理解为两个变量的相关性。</p><p>互信息可以表示为联合概率$P(X,Y)$与边缘概率的$P(X)P(Y)$的相关性：</p><script type="math/tex; mode=display">I(X, Y)=\int_{Y} \int_{X} p(X, Y) \log \frac{p(X, Y)}{p(X) p(Y)} d X d Y</script><h3 id="风格解耦的Decoder模型"><a href="#风格解耦的Decoder模型" class="headerlink" title="风格解耦的Decoder模型"></a>风格解耦的Decoder模型</h3><p>定义输入句子$X$，输出句子$Y$，字典为$V$，时间步为$T$。</p><p>模型如图：</p><p><img src="2.png" alt=""></p><p>将Encoder最后一步的hidden state与风格的one-hot向量concatenate起来，送Decoder。</p><p>我们无法从理论上证明，Decoder的output能与风格K正确地对应起来。output很可能会忽略掉风格输入，使得输出不受风格影响。因此我们显式添加一个约束，强制风格K与output有强烈的依赖关系。</p><p>假定K的id分布k是一个均匀分布的随机向量，就是这样：</p><script type="math/tex; mode=display">P_r(\text{sty} = k) = \frac{1}{k}, \text{for }k= 1,2,...,K</script><p>目标转化为：<strong>最大化风格分布$P_r(\text{sty})$与输出分布$P_r(Y;X)$之间的互信息。</strong></p><p>互信息这么计算：</p><script type="math/tex; mode=display">\begin{aligned} & I(\operatorname{Pr}(\text {Sty}), \operatorname{Pr}(Y ; X)) \\=& \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \int_{Y | k ; X} \log \frac{\operatorname{Pr}(Y, S t y=k ; X)}{\operatorname{Pr}(\text {Sty}=k) \operatorname{Pr}(Y ; X)} d Y \\=& \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \int_{Y | k ; X} \log \frac{\operatorname{Pr}(Y, S t y=k ; X)}{\operatorname{Pr}(Y ; X)} d Y \\ &-\sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \log \operatorname{Pr}(\text {Sty}=k)\\=& \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k) \int_{Y | k ; X} \log \operatorname{Pr}(\text {Sty}=k | Y) d Y+\log K \\ =&\int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(\text {Sty}=k | Y) \log P(\text {Sty}=k | Y) d Y+\log K\end{aligned}</script><p>因为输入变量$\text{sty}$与$X$相互独立，所以</p><script type="math/tex; mode=display">\operatorname{Pr}(S t y=k | Y ; X)=\operatorname{Pr}(S t y=k | Y)</script><p>由于后验分布$\operatorname{Pr}(S t y=k | Y)$是未知的，我们无法对它计算积分。于是我们使用变分推理最大化的方式，训练一个参数化的函数$Q(\text{Sty}=k|Y)$，这个函数估计了后验分布，最大化方程互信息的下界：</p><script type="math/tex; mode=display">\begin{aligned} & I(\operatorname{Pr}(S t y), \operatorname{Pr}(Y ; X))-\log K \\=& \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log \operatorname{Pr}(S t y=k | Y) d Y \\=& \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log Q(S t y=k | Y) d Y \\ &+\int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log \frac{\operatorname{Pr}(S t y=k | Y)}{Q(S t y=k | Y)} d Y \\=& \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log Q(S t y=k | Y) d Y \\ &+\int_{Y ; X} K L(\operatorname{Pr}(S t y | Y), Q(\operatorname{Sty} | Y)) d Y \\ \geq & \int_{Y ; X} \sum_{k=1}^{K} \operatorname{Pr}(S t y=k | Y) \log Q(S t y=k | Y) d Y \\=& \sum_{k=1}^{K} \operatorname{Pr}(S t y=k) \int_{Y | k ; X} \log Q(S t y=k | Y) d Y \end{aligned}</script><p>我们知道KL散度一定是非负的，故上式的不等式一定成立。</p><p>因此我们只需要设计一个参数化的函数Q，使得Q被模型最大化，那么目的就达到了。</p><h3 id="后验分布估计"><a href="#后验分布估计" class="headerlink" title="后验分布估计"></a>后验分布估计</h3><p>给定输出序列$Y$，函数$Q$估计序列$Y$的风格的概率分布。直接计算序列$Y$的平均embedding，用矩阵$W$映射为一个值：</p><script type="math/tex; mode=display">Q(S t y | Y)=\operatorname{softmax}\left(W \cdot \frac{1}{T} \sum_{i=1}^{T} e\left(y_{i}\right)\right)</script><p>最后要做的是计算$Q$在$Y|k;X$上的积分，然而序列$Y$的搜索空间是字典长度的指数，因此采用枚举的方式计算积分。如果我们采样部分Y，这个结果是不可微的。这里我们使用embedding的期望进行积分的近似。</p><h3 id="期望的词向量embedding"><a href="#期望的词向量embedding" class="headerlink" title="期望的词向量embedding"></a>期望的词向量embedding</h3><p>之前的相关工作是《Semantic parsing with semi-supervised sequential autoencoders》。即，只生成一个期望embedding序列，$Y|k;X$，拥有100%的生成概率。下式表达第i词的分布：</p><script type="math/tex; mode=display">p\left(y_{i} | y_{1}, y_{2}, \ldots y_{i-1}, X\right)=g\left(y_{i}, s_{i}\right)</script><p>所以第i词的期望就是：</p><script type="math/tex; mode=display">\operatorname{expect}(i ; k, X)=\sum_{c \in V} g\left(c | s_{i}\right) e(c)</script><p>之后将其喂给下一步的输出：</p><script type="math/tex; mode=display">s_{i+1}=L S T M_{d e c o d e r}\left(s_{i},\left[e x p e c t(i ; k, X), a_{i+1}\right]\right)</script><p>然后使用这些期望向量近似$Y|k;X$的概率分布。</p><p>所以，上面推导出来的</p><script type="math/tex; mode=display">Q(S t y | Y)=\operatorname{softmax}\left(W \cdot \frac{1}{T} \sum_{i=1}^{T} e\left(y_{i}\right)\right)</script><p>可以被重写为：</p><script type="math/tex; mode=display">\mathcal{L}_{r e g}=\frac{1}{K} \sum_{k=1}^{K} \log \left\{\operatorname{softmax}\left(W * \frac{1}{T} \sum_{i=1}^{T} \operatorname{expect}(i ; k, X)\right)[k]\right\}</script><p>k代表第k个风格。所以上面一直推导的的积分公式就可以表达为上式。</p><p>于是上面那个公式就可以作为正则项加入训练过程，作为一个额外的loss：</p><script type="math/tex; mode=display">\operatorname{Train}(X, Y)=\sum_{i=1}^{T} \log p\left(y_{i} | y_{1} y_{2} \ldots y_{i-1}, X\right)+\lambda \mathcal{L}_{\text { reg }}</script><p>loss的前半部分与风格没有什么关系，保证生成的效果，公式中的X置零；而后半部分保证了结果的风格相关。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>主要通过人工评估。</p><h3 id="数据集与模型细节"><a href="#数据集与模型细节" class="headerlink" title="数据集与模型细节"></a>数据集与模型细节</h3><p>16.8W首古诗，一半五言，一半七言。8:1:1划分。将连续的两个句子作为训练对$(X,Y)$。</p><p>embedding和Encoder的hidden size为512，风格K=10，所以Decoder的hidden state维度为1034维。</p><p>batch size=50，前5W个batch中$\lambda=0$，之后设为1.</p><h4 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h4><ul><li>Seq2seq，主要用于对比风格模块的提升。</li><li>Polish(Yan, 2016)，特点是多次修改生成的句子。</li><li>Memory(Zhang et al., 2017)，将memory融入到诗歌生成，可以被视为一个正则化行为。</li></ul><p>不考虑rule-based或模板模型。</p><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>将第一句话作为评估，依次生成后续三个句子。由于模型需要指定风格，于是我们使用上文的函数Q来估计第一句的风格，在后面三句中套用同样的风格。</p><ul><li>流畅性</li><li>一致性</li><li>意义性</li><li>诗歌性</li></ul><p>上述四个指标被纳入考虑。</p><p>分别生成了20首五言和20首七言，作为评估。邀请了10个中国文学专家，做了两组实验。第一组对比seq2seq，第二组对比别的先进模型。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="3.png" alt=""></p><p>seq2seq生成的是混合风格的模型，更可能产生无意义的诗歌。风格作为统一信息，为一致性提供了帮助，并且缩小了空间，因此在风格内部能学习得更加准确、紧凑。</p><h3 id="习得风格的可解释性"><a href="#习得风格的可解释性" class="headerlink" title="习得风格的可解释性"></a>习得风格的可解释性</h3><p>10种风格的关键词如下：</p><p><img src="4.png" alt=""></p><p>首先输入每种风格各生成5首诗，再由人类专家进行分类。由于是无监督生成的，故模型很可能不会严格对齐人类的风格标注。</p><p><img src="5.png" alt=""></p><p>可以看见，人类可以以更高的概率成功识别出很多风格。观察结果表明，模型的学习风格只有两三个关键词有意义、可识别。除此之外，生成的诗歌需要是多样的，否则他们不能被区分开。</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p><img src="6.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;清华孙茂松老师组的工作，从效果来看，“九歌”的效果相当不错。适逢最近这个模型开源，我希望能梳理一下和这个模型有关的论文，尤其是《Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement》的工作（发表在EMNLP 2018）。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="诗歌生成" scheme="http://ldzhangyx.github.io/tags/%E8%AF%97%E6%AD%8C%E7%94%9F%E6%88%90/"/>
    
      <category term="generation" scheme="http://ldzhangyx.github.io/tags/generation/"/>
    
      <category term="poem generation" scheme="http://ldzhangyx.github.io/tags/poem-generation/"/>
    
      <category term="distanglement" scheme="http://ldzhangyx.github.io/tags/distanglement/"/>
    
  </entry>
  
  <entry>
    <title>如何评估树结构的相似性？也许可以了解一下Tree Editing Distance</title>
    <link href="http://ldzhangyx.github.io/2019/08/07/tree-evaluation/"/>
    <id>http://ldzhangyx.github.io/2019/08/07/tree-evaluation/</id>
    <published>2019-08-07T09:03:45.000Z</published>
    <updated>2019-08-07T09:03:45.412Z</updated>
    
    <content type="html"><![CDATA[<p>这是一篇关于树编辑距离相关算法和相关包的介绍。</p><a id="more"></a><p>如何衡量两个树结构之间有多相似？这个领域有很多相关研究。在这里介绍一个可能的算法：Tree Editing Distance。</p><p>这个词被翻译为树编辑距离。形象地理解，就是从一棵树通过编辑，变为另一棵树的复杂程度。</p><h1 id="树编辑距离的形式化"><a href="#树编辑距离的形式化" class="headerlink" title="树编辑距离的形式化"></a>树编辑距离的形式化</h1><p>形式化地来说，有序标记树之间的树编辑距离，是奖一棵树转换为另一棵树的节点操作的最小成本序列。定义下面三种编辑：</p><ol><li>删除节点，将其子节点连接到父节点上，以保持有序。</li><li>在一个已知的节点，和这个节点的连续子节点的子序列之间插入一个节点。</li><li>更改这个节点的label。</li></ol><p>每编辑一次需要花费一点成本，而问题的目标是找到一个操作序列，使得总成本最小。</p><h1 id="现有算法"><a href="#现有算法" class="headerlink" title="现有算法"></a>现有算法</h1><p>Queen’s University有一份slides，专门用于讲述一些比较早期的算法：<a href="http://research.cs.queensu.ca/TechReports/Reports/1995-372.pdf" target="_blank" rel="noopener">http://research.cs.queensu.ca/TechReports/Reports/1995-372.pdf</a></p><p>这个问题可以被递归地解决，然而其具有指数级的复杂度。Zhang and Shasha（1989年）将这个问题的复杂度降低到了$O(m^2n^2)$级别，而最新的算法（Demaine et al.）将算法的时间复杂度降低到了$O(n^2m(1+\log \frac{m}{n}))$级别。</p><p>具体的论文亮点可以参考这个网址：<a href="http://tree-edit-distance.dbresearch.uni-salzburg.at/#bibliography" target="_blank" rel="noopener">http://tree-edit-distance.dbresearch.uni-salzburg.at/#bibliography</a></p><h1 id="可以使用的包"><a href="#可以使用的包" class="headerlink" title="可以使用的包"></a>可以使用的包</h1><p>GitHub上有人复现了Zhang and Shasha的算法，并且可以直接通过pip方式安装，import调用： <a href="https://github.com/timtadh/zhang-shasha" target="_blank" rel="noopener">https://github.com/timtadh/zhang-shasha</a></p><p>奥地利萨尔茨堡大学公开了一个Java程序，用于实现RTED等复杂度更低的算法：<a href="http://tree-edit-distance.dbresearch.uni-salzburg.at" target="_blank" rel="noopener">http://tree-edit-distance.dbresearch.uni-salzburg.at</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇关于树编辑距离相关算法和相关包的介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="structure" scheme="http://ldzhangyx.github.io/tags/structure/"/>
    
      <category term="algorithm" scheme="http://ldzhangyx.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch使用的日常（2019.8.5）</title>
    <link href="http://ldzhangyx.github.io/2019/08/05/pytorch-0801/"/>
    <id>http://ldzhangyx.github.io/2019/08/05/pytorch-0801/</id>
    <published>2019-08-05T08:23:48.000Z</published>
    <updated>2019-12-30T08:52:50.242Z</updated>
    
    <content type="html"><![CDATA[<h1 id="nn-Embedding"><a href="#nn-Embedding" class="headerlink" title="nn.Embedding"></a>nn.Embedding</h1><p>我一直对nn.Embedding层感到困惑。它在训练中有没有改变过呢？</p><p>查询StackOverflow，我得到了一个答案：</p><ol><li>我可以随机初始化一个词向量，也可以导入一个预训练的词向量；</li><li>我可以选择是否让它参与训练（默认参与了训练）。</li></ol><p>如果要固定某几层不进行训练，需要做两件事：</p><ol><li>层数的requires_grad参数设为False；</li><li>传给optimizer的parameters需要手动过滤。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)</span><br></pre></td></tr></table></figure><h1 id="CUDA-out-of-memory"><a href="#CUDA-out-of-memory" class="headerlink" title="CUDA out of memory"></a>CUDA out of memory</h1><p>遇到了一次这样的情况。考虑optimizer产生的大量中间结果，使得显存爆炸。</p><p>解决方案：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">    ....</span><br></pre></td></tr></table></figure><p>将validation代码放在里面，减少显存占用。实际起到的效果和<code>model.eval()</code>是一样的。</p><h1 id="Learning-Rate-Scheduler"><a href="#Learning-Rate-Scheduler" class="headerlink" title="Learning Rate Scheduler"></a>Learning Rate Scheduler</h1><p>学习率衰减（weight decay）被证明与参数的二阶范数正则化等价。PyTorch使用<code>lr_scheduler</code>进行学习率衰减操作。下面是一类</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters(), lr=0.0005)</span><br><span class="line">scheduler = ExponentialLR(optimizer, gamma=0.95, minimum=1e-5)</span><br></pre></td></tr></table></figure><p>在训练的时候这样调整学习率：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheduler.step()</span><br></pre></td></tr></table></figure><p>将这行代码插入你认为应该调整的地方。下面是一种常见做法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">output = model(input)</span><br><span class="line">loss = loss(output, label)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br><span class="line">schedule.step()</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h1 id="使用GELU激活函数"><a href="#使用GELU激活函数" class="headerlink" title="使用GELU激活函数"></a>使用GELU激活函数</h1><p>一些实验证明在NLP领域，GELU函数的效果要好于RELU。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;nn-Embedding&quot;&gt;&lt;a href=&quot;#nn-Embedding&quot; class=&quot;headerlink&quot; title=&quot;nn.Embedding&quot;&gt;&lt;/a&gt;nn.Embedding&lt;/h1&gt;&lt;p&gt;我一直对nn.Embedding层感到困惑。它在训练中有没有
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《Regularizing and Optimizing LSTM Language Models》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/07/31/awd-lstm/"/>
    <id>http://ldzhangyx.github.io/2019/07/31/awd-lstm/</id>
    <published>2019-07-31T13:43:44.000Z</published>
    <updated>2019-08-01T07:53:53.966Z</updated>
    
    <content type="html"><![CDATA[<p>人称“语言建模的王者”，AWD-LSTM模型。</p><a id="more"></a><p>原文地址：<a href="https://openreview.net/references/pdf?id=rJI9awpBf" target="_blank" rel="noopener">https://openreview.net/references/pdf?id=rJI9awpBf</a></p><h1 id="论文亮点"><a href="#论文亮点" class="headerlink" title="论文亮点"></a>论文亮点</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文为了解决词级别的语言建模问题，研究了正则化、优化LSTM类模型的策略。本文提出了weighted-dropped LSTM，在hidden to hidden weights上使用了DropConnect，作为循环正则化的形式；此外引入了NT-AvSGD，是平均随机梯度方法的非单调触发变体，使用NT条件自动确定平均触发。</p><p>使用这些和其他正则化策略，AWD-LSTM在PTB和WikiTest-2上达到了最优ppl。模型可用于Q-RNN（Quasi-RNN，和SRU目的一致，都是对RNN进行了并行化改进）。</p><h2 id="Weight-Dropped-LSTM"><a href="#Weight-Dropped-LSTM" class="headerlink" title="Weight-Dropped LSTM"></a>Weight-Dropped LSTM</h2><p>首先给出LSTM的公式：</p><script type="math/tex; mode=display">\begin{aligned} i_{t} &=\sigma\left(W^{i} x_{t}+U^{i} h_{t-1}\right) \\ f_{t} &=\sigma\left(W^{f} x_{t}+U^{f} h_{t-1}\right) \\ o_{t} &=\sigma\left(W^{o} x_{t}+U^{o} h_{t-1}\right) \\ \tilde{c}_{t} &=\tanh \left(W^{c} x_{t}+U^{c} h_{t-1}\right) \\ c_{t} &=i_{t} \odot \tilde{c}_{t}+f_{t} \odot+\tilde{c}_{t-1} \\ h_{t} &=o_{t} \odot \tanh \left(c_{t}\right) \end{aligned}</script><p>正则化技术用于防止RNN过度拟合。之前的递归正则化对$h_{t-1}$或$c_{t}$起作用，阻止了黑盒RNN的实现。</p><p>建议使用DropConnect。DropConnect应用于隐藏状态之间的权重矩阵(Ui, Uf, Uo, Uc)，而不是隐藏状态或记忆状态。这一丢弃操作只在前向和反向传播前进行一次，从而最小化对训练速度的影响，并且适用于任何标准的黑盒RNN优化实现。通过丢弃隐藏状态之间的权重矩阵的部分信息，可以防止LSTM循环连接的过拟合。</p><h2 id="NT-ASGD"><a href="#NT-ASGD" class="headerlink" title="NT-ASGD"></a>NT-ASGD</h2><p>在语言建模过程中，不带动量的SGD的表现比其他优化方法更好。 我们调查AvSGD进一步改善训练过程。AvSGD展示了许多惊讶的结果，比如说而渐近二阶收敛。普通SGD更新公式如下：</p><script type="math/tex; mode=display">w_{k+1}=w_{k}-\gamma_{k} \hat{\nabla} f\left(w_{k}\right)</script><p>但AvSGD不使用最后一步的迭代作为解，而是使用</p><script type="math/tex; mode=display">\frac{1}{(K-T+1)} \sum_{i=T}^{K} w_{i}</script><p>其中K是迭代总数，T是用户指定的平均计算触发器。</p><p>ASGD的缺点在于，学习率$\ita_k$和T的调参没有明确的方法论。</p><p>理想情况下，SGD收敛到稳态分布时，需要触发平均。语言建模使用的一种常见策略是指标停滞时降低学习率，而触发也可以参照这种方法。</p><p>NT-ASGD的作法是，</p><ul><li>仅当验证测度在多次循环后没有改善的情况下才触发平均。所以，当验证测度在n次循环（n称为非单调间隔超参数）后没有改善时，算法换用ASGD。论文作者发现n=5这一设置效果良好。</li><li>使用恒定学习率，因此无需进一步调整。</li></ul><p><img src="1.png" alt=""></p><h2 id="其他正则化策略"><a href="#其他正则化策略" class="headerlink" title="其他正则化策略"></a>其他正则化策略</h2><h3 id="变长BPTT"><a href="#变长BPTT" class="headerlink" title="变长BPTT"></a>变长BPTT</h3><p>论文作者指出了固定长度BPTT的低效。假设我们在100个元素上进行长度为10的固定窗口的BPTT。在这一情形下，任何可以被10整除的元素将没有任何元素可以反向传播。固定长度BPTT阻止了1/10的数据以循环的方式改善自身，还有8/10的数据仅仅使用部分BPTT窗口。</p><p>变长BPTT首先选择一个基本序列长度。人工制定一个BPTT，然后基本BPTT长度有p的概率选择BPTT，也有1-p的概率选择BPTT/2。</p><p>然后通过基本BPTT长度计算得到sequence length：</p><script type="math/tex; mode=display">\text{sequence length} = \max(5, l \in N(bptt, \sigma))</script><p>l从正态分布中取样得到结果。这一步是必要的，因为取样任意序列长度的情况下， 使用固定学习率将倾向于短序列。</p><h3 id="变分Dropout"><a href="#变分Dropout" class="headerlink" title="变分Dropout"></a>变分Dropout</h3><p>在标准dropout中，每次调用dropout时取样一个新的dropout掩码。而在变分dropout中，dropout掩码只在第一次调用时取样，接着锁定的dropout掩码重复应用于前向和反向过程中的所有连接。</p><p>尽管RNN的隐藏到隐藏转换中使用了DropConnect，但其他所有dropout操作中使用了变分dropout，特别是在给定的前向和反向传播中，LSTM的所有输入和输出使用同样的dropout掩码。mini-batch内的每个样本使用不同的dropout掩码，而不是在所有样本上使用同一个掩码，以确保元素丢弃的多样性。</p><h3 id="嵌入Dropout"><a href="#嵌入Dropout" class="headerlink" title="嵌入Dropout"></a>嵌入Dropout</h3><p>实际上就是在Embedding Matrix上使用dropout，使得该字在完整的前向、反向传播上都消失了。该技术最早由A Theoretically Grounded Application of Dropout in Recurrent Neural Networks这篇论文提出。</p><h3 id="权重绑定"><a href="#权重绑定" class="headerlink" title="权重绑定"></a>权重绑定</h3><p>权重绑定在embedding和softmax layer上共享了权重，减少了模型中的总参数。该技术具有理论动机（Inan等，2016），并防止模型必须学习输入和输出之间的一对一对应，从而对标准LSTM语言模型进行实质性改进。</p><h3 id="独立embedding-size和hidden-size"><a href="#独立embedding-size和hidden-size" class="headerlink" title="独立embedding size和hidden size"></a>独立embedding size和hidden size</h3><p>在大多数自然语言处理任务中，预训练和训练的单词矢量都具有相对较低的维度 - 通常在100到400维之间。大多数先前的LSTM语言模型将单词向量的维度与LSTM的隐藏状态的维度联系起来。即使减少单词嵌入大小对防止过度拟合也没有好处，语言模型的总参数的最简单减少是减少单词向量大小。</p><p>为了实现这一点，修改第一个和最后一个LSTM层，使得它们的输入和输出维度分别等于减小的嵌入大小。</p><h3 id="激活正则化和时域激活正则化"><a href="#激活正则化和时域激活正则化" class="headerlink" title="激活正则化和时域激活正则化"></a>激活正则化和时域激活正则化</h3><p>L2正则化除用于网络参数上，还可以用在独立单元的激活上，和不同时间步里，RNN的输出上。</p><p>激活正则化惩罚显著过大的激活：</p><script type="math/tex; mode=display">\alpha L_{2}\left(m \odot h_{t}\right)</script><p>时域激活正则化，惩罚过大的hidden state波动：</p><script type="math/tex; mode=display">\beta L_{2}\left(h_{t}-h_{t+1}\right)</script><h2 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h2><p>在PTB和WikiText-2上实验。PTB的词汇量约1W单词，导致大量词汇OOV。WT2的词汇量超过3W。</p><p>LSTM隐单元为三层，1150神经元，Embedding size为400.loss被examples和timesteps平均。embedding被均匀初始化在[-0.1, 0.1]之间，其他权重在$[-\frac{1}{\sqrt{H}}, \frac{1}{\sqrt{H}}]$之间，H为hidden size。</p><p>NT-AvSGD算法训练750 epoches，L相当于1 epoch，n=5. batch size为80（WT2）和40（PTB）。经验表明batch size较大时表现更好。完成后，运行AvSGD，T=0，热启动w0作为finetuning step以进一步改进解。对于这个finetuning步骤，使用算法1的相同的标准终止执行。</p><p>最大范数为0.25的梯度裁剪，初始学习率为30，随机BPTT长度设置为N(70, 5)，p=0.95和N(35, 5)，p=0.05. 用于word vector的dropout、LSTM层间输出，LSTM最上层输出，embedding dropout分别为(0.4, 0.3, 0.4, 0.1)。对WD-LSTM，dropour=0.5用在rurrent weight matrices，而WT2上值增加到0.65，考虑到增加到词汇量。</p><p>对于所以实验，分别使用2和1的AR和TAR值，并将embedding和softmax权重联系起来。所有超参数通过反复实验选择。</p><p><img src="2.png" alt=""></p><p><img src="3.png" alt=""></p><h3 id="指针模型"><a href="#指针模型" class="headerlink" title="指针模型"></a>指针模型</h3><p>在过去的工作中，已经证明基于指针的注意力模型在改进语言建模方面非常有效（Merity等，2016; Grave等，2016）。鉴于对基础神经语言模型的这种实质性改进，关于指针增强的有效性仍然是一个悬而未决的问题，特别是当重量绑定等改进可能以相互排斥的方式起作用时。</p><p>可以以可忽略的成本在预训练的语言模型之上添加神经缓存模型（Grave等，2016）。神经缓存将先前隐藏状态存储在存储器单元中，然后使用由缓存建议的概率分布和用于预测的语言模型的简单凸组合。缓存模型有三个超参数：缓存的内存大小（窗口），组合的系数（确定两个分布如何混合），以及缓存分布的平坦度。一旦获得训练有素的语言模型，所有这些都在验证集上进行调整，并且不需要自己进行培训，使得使用起来非常便宜。这些超参数的调整值分别为PTB（2000,0.1,1.0）和WT2（3785,0.1279,0.662）。</p><p>在表1和表2中，我们表明该模型进一步改善了语言模型的困惑，PTB的6个困惑点和WT2的11个点。<br>虽然这比Grave等报道的增益要小。<br>（2016），使用LSTM没有重量绑定，这仍然是一个实质性的下降。<br>鉴于神经缓存模型的简单性以及缺乏任何受过训练的组件，这些结果表明现有的神经语言模型基本上缺乏，无法捕获长期依赖关系或有效记住最近看到的单词。<br>为了理解指针对模型的影响，特别是验证集的困惑，我们详细说明了每个单词对表3中缓存模型的整体困惑的贡献。<br>我们计算WikiText-2数据集的验证部分中的目标字的LSTM和LSTM与缓存模型之间的损失函数值（即，对数困扰）的总差异的总和。<br>我们提出差异总和的结果而不是均值，因为后者不合适地过分强调了不经常出现的单词，其中高速缓存有助于显着地忽略频繁出现的单词，其中高速缓存提供适度的改进，累积地做出强有力的贡献。<br>最大累积增益在提高的<unk>令牌的处理，虽然这是超过11540个的情况。<br>第二个最好的改进，大约五分之一由<unk>令牌给定的增益，为经，然而，这仅字发生161次。<br>这表明缓存对于相对罕见的单词仍然有显着帮助，丘吉尔，布莱斯或索尼克进一步证明了这一点。<br>当处理频繁的单词类别（例如标点符号或停用单词）时，缓存不是有益的，语言模型很可能适合这些单词类别。<br>这些观察结果激发了缓存框架的设计，该框架更加了解两个模型的相对优势。</unk></unk></p><p><img src="4.png" alt=""></p><h3 id="AWD-QRNN"><a href="#AWD-QRNN" class="headerlink" title="AWD-QRNN"></a>AWD-QRNN</h3><p>概括一下就是模型也适合Q-RNN。</p><p><img src="5.png" alt=""></p><h3 id="模型消融分析"><a href="#模型消融分析" class="headerlink" title="模型消融分析"></a>模型消融分析</h3><p><img src="6.png" alt=""></p><p>详见原论文。 最明显的困惑度提升来自LSTM hidden to hidden的lSTM正则化，也就是DropConnect。</p><h2 id="代码与训练速度"><a href="#代码与训练速度" class="headerlink" title="代码与训练速度"></a>代码与训练速度</h2><p><a href="https://github.com/salesforce/awd-lstm-lm" target="_blank" rel="noopener">https://github.com/salesforce/awd-lstm-lm</a></p><p>代码在上述网址开源，记载了更加详细的实验结果。readme里特意提及了速度的问题。NVIDIA Quadro GP100的速度在PTB上大约是65秒一个epoch。考虑到我们将会使用的HookTheory，这个速度相比WT2的速度更具有参考价值。作者体积K80的速度大约是1/3，而我做实验可以用到一块1080Ti(for each task)。根据NVIDIA提供的compute Capability数值来看：</p><ul><li>Tesla K80：3.7</li><li>Tesla V100：7.0</li><li>Tesla P100：6.0</li><li>Quadro GP100：6.0</li><li>GTX 1080Ti：6.1</li><li>Jetson Nano：5.3</li></ul><p>猜想在词级语言建模任务上，我达到45秒每epoch的速度是正常的。另外这个计算力表其实和我的认知差别有点大。K80这么弱的吗？<br>值得一提的是Jetson Nano，达到5.3的分数意味着可以一用了。过段时间我会调研一下树莓派4和Jetson Nano。</p><h1 id="想法和见解"><a href="#想法和见解" class="headerlink" title="想法和见解"></a>想法和见解</h1><p>还能说什么呢，一年被引用200+次，只能说大佬牛逼，工作量和模型质量都是一流的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;人称“语言建模的王者”，AWD-LSTM模型。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="AWD-LSTM" scheme="http://ldzhangyx.github.io/tags/AWD-LSTM/"/>
    
      <category term="language model" scheme="http://ldzhangyx.github.io/tags/language-model/"/>
    
  </entry>
  
  <entry>
    <title>简洁明快的命令行解析器argparse简明指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/30/argparse/"/>
    <id>http://ldzhangyx.github.io/2019/07/30/argparse/</id>
    <published>2019-07-30T04:22:23.000Z</published>
    <updated>2019-07-30T07:49:14.937Z</updated>
    
    <content type="html"><![CDATA[<p>为了避免Python代码在需要设置的情况下进行hard code，我们有很多种方式将参数指定在一个地方，便于集中、解耦地修改参数。可以使用的解决办法很多，如设立config类，全局变量指明，以及使用argparse。</p><p>本文是argparse的简明指南，指明了argparse模块的快速上手方法。</p><h1 id="指南"><a href="#指南" class="headerlink" title="指南"></a>指南</h1><h2 id="argparse应该放在文件的什么地方？"><a href="#argparse应该放在文件的什么地方？" class="headerlink" title="argparse应该放在文件的什么地方？"></a>argparse应该放在文件的什么地方？</h2><p>argparse可以放在main.py文件的最上方，仅在import语句块下。</p><h2 id="argparse分为几个步骤？"><a href="#argparse分为几个步骤？" class="headerlink" title="argparse分为几个步骤？"></a>argparse分为几个步骤？</h2><p>四个步骤。</p><p>第一步，import argparse。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br></pre></td></tr></table></figure><p>第二步，定义parser。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser(description=&apos;Process some integers.&apos;)</span><br></pre></td></tr></table></figure><p>第三步，增加参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&apos;integers&apos;, metavar=&apos;N&apos;, type=int, nargs=&apos;+&apos;,</span><br><span class="line">                    help=&apos;an integer for the accumulator&apos;)</span><br><span class="line">parser.add_argument(&apos;--sum&apos;, dest=&apos;accumulate&apos;, action=&apos;store_const&apos;,</span><br><span class="line">                    const=sum, default=max,</span><br><span class="line">                    help=&apos;sum the integers (default: find the max)&apos;)</span><br></pre></td></tr></table></figure><p>第四步，解析参数并实例化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><p>在调用main.py时，在后面增添命令行参数，或以default值的方式指定参数。后面的参数值可以以args变量的各个attribute直接使用，如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args.sum</span><br></pre></td></tr></table></figure><h2 id="add-augment接收几种参数？"><a href="#add-augment接收几种参数？" class="headerlink" title="add_augment接收几种参数？"></a>add_augment接收几种参数？</h2><blockquote><p>name or flags - 一个命名或者一个选项字符串的列表，例如 foo 或 -f, —foo。<br>action - 当参数在命令行中出现时使用的动作基本类型。<br>nargs - 命令行参数应当消耗的数目。<br>const - 被一些 action 和 nargs 选择所需求的常数。<br>default - 当参数未在命令行中出现时使用的值。<br>type - 命令行参数应当被转换成的类型。<br>choices - 可用的参数的容器。<br>required - 此命令行选项是否可省略 （仅选项可用）。<br>help - 一个此选项作用的简单描述。<br>metavar - 在使用方法消息中使用的参数值示例。<br>dest - 被添加到 parse_args() 所返回对象上的属性名。</p></blockquote><ul><li><p>name or flags<br>这几乎是必须的。<code>add_augment</code>后需要为参数命名。一般来说，使用<code>--name</code>字段即可。这样调用时写<code>--name=xxx</code>进行参数指定。</p></li><li><p>action<br>默认action是<code>store</code>，代表存储参数的值。</p></li><li><p>nargs<br>关联剁个参数到一个arguments里。<code>N</code>代表之后N个参数会形成一个列表。<code>+</code>代表片段内所有参数被聚集到列表（直到下一个参数）。</p></li><li><p>default<br>指定默认值。最常用。</p></li><li><p>type<br>指定参数类型，指定为<code>str</code>或<code>int</code>最常用。</p></li><li><p>help<br>填写参数的提示。</p></li></ul><p>参数实例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(&apos;--model&apos;, type=str, default=&apos;LSTM&apos;,</span><br><span class="line">                    help=&apos;type of recurrent net (LSTM, QRNN, GRU)&apos;)</span><br><span class="line">parser.add_argument(&apos;--emsize&apos;, type=int, default=100,</span><br><span class="line">                    help=&apos;size of word embeddings&apos;)</span><br><span class="line">parser.add_argument(&apos;--when&apos;, nargs=&quot;+&quot;, type=int, default=[-1],</span><br><span class="line">                    help=&apos;When (which epochs) to divide the learning rate by 10 - accepts multiple&apos;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为了避免Python代码在需要设置的情况下进行hard code，我们有很多种方式将参数指定在一个地方，便于集中、解耦地修改参数。可以使用的解决办法很多，如设立config类，全局变量指明，以及使用argparse。&lt;/p&gt;
&lt;p&gt;本文是argparse的简明指南，指明了a
      
    
    </summary>
    
      <category term="简明指南" scheme="http://ldzhangyx.github.io/categories/%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/"/>
    
    
      <category term="python" scheme="http://ldzhangyx.github.io/tags/python/"/>
    
      <category term="argparse" scheme="http://ldzhangyx.github.io/tags/argparse/"/>
    
      <category term="tutorial" scheme="http://ldzhangyx.github.io/tags/tutorial/"/>
    
  </entry>
  
  <entry>
    <title>MobaXTerm简明指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/29/mobaxterm-tutorial/"/>
    <id>http://ldzhangyx.github.io/2019/07/29/mobaxterm-tutorial/</id>
    <published>2019-07-29T09:24:38.000Z</published>
    <updated>2019-07-29T10:10:12.163Z</updated>
    
    <content type="html"><![CDATA[<p>这个简明指南旨在介绍MobaXTerm，Windows下的一个现代SSH客户端。</p><a id="more"></a><ol><li><p>首先下载MobaXTerm的对应版本。网址如下：<a href="https://mobaxterm.mobatek.net/download.html" target="_blank" rel="noopener">https://mobaxterm.mobatek.net/download.html</a></p></li><li><p>左上角点击Session，新建一个SSH连接。</p></li></ol><p><img src="1.png" alt=""></p><ol><li>配置完成。</li></ol><p>这篇文章主要想说明MobaXTerm一些优点：</p><ol><li><p>可以检测服务器运行情况。Remote Monitoring开启后，服务器的运行情况可以被实时地监控。</p></li><li><p>自带一个SFTP协议的可视化文件树，并且支持拖动上传下载文件。</p></li><li><p>附带了一个文本编辑器，是vi的良好替代。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个简明指南旨在介绍MobaXTerm，Windows下的一个现代SSH客户端。&lt;/p&gt;
    
    </summary>
    
      <category term="简明指南" scheme="http://ldzhangyx.github.io/categories/%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/"/>
    
    
      <category term="tutorial" scheme="http://ldzhangyx.github.io/tags/tutorial/"/>
    
      <category term="software" scheme="http://ldzhangyx.github.io/tags/software/"/>
    
  </entry>
  
  <entry>
    <title>Coding on Server! PyCharm远程调试、文件同步、GitHub版本控制简明指南</title>
    <link href="http://ldzhangyx.github.io/2019/07/29/ssh-for-pycharm/"/>
    <id>http://ldzhangyx.github.io/2019/07/29/ssh-for-pycharm/</id>
    <published>2019-07-29T09:22:55.000Z</published>
    <updated>2019-07-29T10:10:17.026Z</updated>
    
    <content type="html"><![CDATA[<p>这个指南适用于想在PyCharm上debug，同时使用远程服务器上的解释器运行代码的人。</p><a id="more"></a><p>使用PyCharm本地编辑代码，调用远程服务器的Python解释器。这个方案支持PyCharm单步调试等操作。</p><ol><li><p>新建或打开你想运行的项目。</p></li><li><p>Tools -&gt; Deployment -&gt; Configuration，打开界面。</p></li></ol><p><img src="1.png" alt=""></p><ol><li>在界面中配置对服务器的连接。</li></ol><p><img src="2.png" alt=""></p><ol><li>在Mappings里面设置文件映射，映射到服务器的对应目录。</li></ol><p>注意，对应的目录是步骤3的root path + Mappings的Deployment path。</p><p><img src="3.png" alt=""></p><ol><li><p>点击左边侧栏的√，使得服务器名字变为粗体。这代表你启用了这个Deployment。</p></li><li><p>Tools -&gt; Deployment -&gt; Settings，将文件同步设置，更改为仅在ctrl+S时保存。至此，所有的本地代码文件都会被同步到服务器上。</p></li></ol><p><img src="5.png" alt=""></p><ol><li>File -&gt; Settings -&gt; Project -&gt; Project Interpreter，打开界面，右上角选择小齿轮， Add，选择服务器上的python.exe应用程序，链接到远程解释器。</li></ol><p><img src="4.png" alt=""></p><ol><li><p>此时不需要特意设置path mappings，因为会沿用Deployment的mapping。</p></li><li><p>我们开始版本控制。VCS -&gt; Import into Version Control -&gt; Share project on GitHub。</p></li></ol><p><img src="6.png" alt=""></p><p>此时你的每一次保存会被视为一次commit。</p><p>配置完毕。</p><p>以下是你可能遇到的问题：</p><ol><li><p>File Transfer失败。可以检查一下mapping地址是否正确。</p></li><li><p>Error:Python helpers are not copied yet to the remote host. Please wait until remote interpreter initialization finishes. 如果你的服务器名称是字符串，推荐改用IP地址；如果无效，请删除服务器上的.pycharm_helper文件夹，等待重新建立index。</p></li></ol><p>以下是一些附带的常用操作。</p><ol><li>PyCharm在debug的时候，可以在右下角的Console Tab点击Python图标，进入交互式Python，你可以在断点debug时运行各种表达式。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这个指南适用于想在PyCharm上debug，同时使用远程服务器上的解释器运行代码的人。&lt;/p&gt;
    
    </summary>
    
      <category term="简明指南" scheme="http://ldzhangyx.github.io/categories/%E7%AE%80%E6%98%8E%E6%8C%87%E5%8D%97/"/>
    
    
      <category term="python" scheme="http://ldzhangyx.github.io/tags/python/"/>
    
      <category term="tutorial" scheme="http://ldzhangyx.github.io/tags/tutorial/"/>
    
      <category term="software" scheme="http://ldzhangyx.github.io/tags/software/"/>
    
      <category term="PyCharm" scheme="http://ldzhangyx.github.io/tags/PyCharm/"/>
    
      <category term="SSH" scheme="http://ldzhangyx.github.io/tags/SSH/"/>
    
  </entry>
  
  <entry>
    <title>《Deep Music Analogy via Latent Representation Distanglement》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/07/29/ec2vae/"/>
    <id>http://ldzhangyx.github.io/2019/07/29/ec2vae/</id>
    <published>2019-07-29T06:30:57.000Z</published>
    <updated>2019-07-31T13:28:26.633Z</updated>
    
    <content type="html"><![CDATA[<p>组里大佬们的工作。</p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>类比制作（Analogy-making）是使用计算机算法生成自然而有创造力的音乐作品的关键方法。通常来说，将音乐的抽象形式（如，高层表示及这些表示之间的关系）做部分地转移，可以达成类比的目的。</p><p>这个过程要求对音乐表示做解耦（如，保留风格，创作内容不一样的曲子）。对于人类音乐家来说，是轻而易举的事情，但是这对计算机来说却是一个难题。要完成这个目标，我们有三个步骤：</p><ol><li>从观察（observation）中提取潜在的表示（latent representations）；</li><li>对表示进行解耦，使得解耦后每一部分有唯一的的语义解释；</li><li>将潜在表示映射回实际的音乐。</li></ol><p>这篇论文提出了一个具有明确约束（explicitly-constrained）的VAE模型（简称$EC^2-VAE$），作为所有三个子问题的统一解决方案。</p><p>更确切地说，本文专注于对8-beat音乐片段，以和弦（chord）作为condition，解耦音高（pitch）和节奏（rhythm）的表示。这个模型借用了其他音乐片段的表示，帮助我们实现了假设情景：一个片段如果使用别的音高、节奏、和弦将会怎么样？</p><p>最后，本文用客观的测量方法验证了模型的解耦方法，并通过一个主观的研究，评估了类比的实际例子。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>类比是一个生成高质量艺术作品的有效方法。一般来说，如果两个系统有共同的抽象表示、共同的表示间关系，那么这两个系统就是相似的。举个例子，“氢原子就像太阳系”，形式化表示为$A:B::C:D$的形式：“原子核:氢原子::太阳::太阳系”。两个系统共享的抽象就是“更大的部分是一个系统的中心”。对于生成算法来说，解决一个“what if”问题，即已知ABC，D是什么的问题，是生成算法的一个捷径。</p><p><code>A:B::C:D</code></p><p>对于音乐生成来说，如果A是B的抽象，A与B的关系是，A是音乐B的节奏pattern（一个例子）。那么，如果我们有另一个节奏pattern C，那么B搭配上C的pattern，得到的D将会是什么样子呢？</p><p>类比生成的一大优势是能够产生自然、有创意的结果。之所以自然，是因为模型复用了人类作品实例的表示（如image style等），以及这些概念之间的内在关系；之所以有创意，是因为这些表示使用了新颖的方式进行重新组合，进行了不同的表达。</p><p>我们清楚地意识到，类比的实质是对抽象表征的迁移，而不是浅层observation的组合和复读。要进行有意义的类比，需要对高层表征进行解耦，而这对计算机来说是一个难题。</p><p>EC2-VAE模型，就是一个学习解耦的有效工具。具体来说，Encoder从Observation中获取latent representation，语义约束（semantic constraints）解耦表征，使得每个部分具有唯一的解释，然后Decoder将解耦表示映射回实际的音乐，同时保持表示之间的内在关系。</p><p>作为生成模型，我们希望EC2-VAE有三个特性。</p><ol><li>对表示的解耦是明确的。即我们可以指定哪些维度表示哪些语义因素。</li><li>解耦不能对重建造成大量牺牲。</li><li>训练阶段不需要任何类似的例子，但该模型能够在推理阶段进行类比。</li></ol><p>为了评估，我们提出了一个新的指标（metric）进行调查。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="类比生成"><a href="#类比生成" class="headerlink" title="类比生成"></a>类比生成</h3><p>两种类比算法：</p><ol><li>监督学习。需要大量标注，表示学习的需求较少。</li><li>严格的类比算法，不仅需要学习表示，还需要解耦表示，允许模型通过操纵任何解耦的表示来进行domain-free的类比。</li></ol><h3 id="表示学习-amp-解耦"><a href="#表示学习-amp-解耦" class="headerlink" title="表示学习 &amp; 解耦"></a>表示学习 &amp; 解耦</h3><p>（相关研究略）</p><p>本文模型并不直接约束z，而是将loss应用于与latent factor有关的中间输出。间接而显式的约束使得模型能够进一步将表示分解为音调、节奏等语义因素。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>目标是解开时长为8拍的旋律的节奏和音高两个方面。数据均来源于Nottingham数据集。分辨率为1/4拍。</p><h3 id="数据表示"><a href="#数据表示" class="headerlink" title="数据表示"></a>数据表示</h3><p>每个8拍旋律都可以用一个32x130的矩阵表示。32为时间维，130为音高、保持和休止符维。</p><p>额外设计了一个节奏pattern约束网络的中间输出。8拍节奏模式表示为32x3的矩阵，3为one-hot向量，代表：(onset, holding, rest)。</p><p>chord作为条件，表示成32x12的矩阵（色度图），如12维的multi-hot向量。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型基本结构与Vanilla sequence VAE一致。</p><p><img src="1.png" alt=""></p><p>模型的创新点在于，为Decoder的一部分（橙色）设计了一个子任务，通过明确地鼓励$z_r$的中间输出，匹配节点的节奏特征，从而将潜在（latent）节奏表示$z_r$从整体的表示z中解开。</p><p>因此z的另一部分是节奏以外的一切，被解释为音高表示$z_p$。这样明确编码的解耦技术非常灵活，因为可以设计多个子部分，解开z的多个语义可解释的因子。只要可以定义相应潜在因子的中间输出。</p><p>新模型将chord作为condition，优点是使z免于存储和弦信息。</p><p>Decoder中，为了解耦，本文将z分割成了两半：$z_p$和$z_r$，让两边的向量各有128个维度。</p><h3 id="具有解耦功能的ELBO目标的理论正当性"><a href="#具有解耦功能的ELBO目标的理论正当性" class="headerlink" title="具有解耦功能的ELBO目标的理论正当性"></a>具有解耦功能的ELBO目标的理论正当性</h3><p>表征解耦有时会牺牲重建能力。本节主要证明了模型没有遭受大量的解耦-重构悖论，并且模型的likelihood bound接近原始条件的VAE，在一些情况下甚至可能相等。</p><p>ELBO目标函数：</p><script type="math/tex; mode=display">\begin{aligned} \operatorname{ELBO}(\phi, \theta)=& \mathbb{E}_{Q}\left[\log P_{\phi}(x | z, c)\right] \\ &-\mathbb{K} \mathbb{L}\left[Q_{\theta}(z | x, c)| | P_{\phi}(z | c)\right] \leq \log P_{\phi}(x | c) \end{aligned}</script><p>下面用$\mathcal{D}$代替$&amp;-\mathbb{K} \mathbb{L}\left[Q_{\theta}(z | x, c)| | P_{\phi}(z | c)\right]$。如果我们将图1(b)的中间节奏输出看作网络的hidden variable，模型的ELBO只会增加基于原始节奏的节奏重建loss，达到原始ELBO的下限。</p><script type="math/tex; mode=display">\begin{aligned} & \mathrm{ELBO}^{\mathrm{new}}(\phi, \theta) \\=& \mathbb{E}_{Q}\left[\log P_{\phi}(x | z, c)\right]-\mathcal{D}+\mathbb{E}_{Q}\left[\log P_{\phi_{r}}\left(r(x) | z_{r}\right)\right] \\=& \operatorname{ELBO}(\phi, \theta)+\mathbb{E}_{Q}\left[\log P_{\phi_{r}}\left(r(x) | z_{r}\right)\right] \end{aligned}</script><p>其中$\phi_r$代表节奏decoder的参数。新ELBO是原始ELBO的一个下界，因为$\mathbb{E}_{Q}\left[\log P_{\phi_{r}}\left(r(x) | z_{r}\right)\right] \leq 0$。</p><p>此外，如果将global decoder的其余部分采用原始节奏而不是节奏decoder的中间输出作为输入，则目标可以重写为：</p><script type="math/tex; mode=display">\begin{aligned} & \operatorname{ELBO}^{\mathrm{new}}(\phi, \theta) \\=& \mathbb{E}_{Q}\left[\underbrace{\log P_{\phi}\left(x | r(x), z_{p}, c\right)+\log P_{\phi}\left(r(x) | z_{r}, c\right)}_{x \underline{ \|} z_{r} | r(x),c;r(x)\underline{ \|}z_p}\right] -\mathcal{D}\\=& \mathbb{E}_{Q}\left[\log P_{\phi}(x, r(x) | z, c)\right]-\mathcal{D} \\=& \mathbb{E}_{Q}\left[P_{\phi}(x | z, c)+\log P_{\phi}(r(x) | x, z, c)\right]-\mathcal{D} \\=& \operatorname{ELBO}(\phi, \theta) \end{aligned}</script><p>第二个等号表示一个完美的解耦，最后一个等号成立，因为$r(x)$由$x$决定。换而言之，我们展示了在某些假设下，两个ELBO相同。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="客观指标"><a href="#客观指标" class="headerlink" title="客观指标"></a>客观指标</h3><p>一旦成功解耦，音高的任何变化都不应影响潜在的节奏表示。为此设计了两个指标：</p><ol><li>换位后的Δz；</li><li>一个augmantation-based序列的F-score。</li></ol><h4 id="音高转换的Δz可视化"><a href="#音高转换的Δz可视化" class="headerlink" title="音高转换的Δz可视化"></a>音高转换的Δz可视化</h4><p>将$F_i$定义为一个操作，将所有音符转$i$个半音。使用L1范数测量z的变化。</p><p><img src="2.png" alt=""></p><p>增加音高时，音高的潜在表示的变化$z_p$远高于节奏的潜在表示变化$z_r$。这充分证明了解耦的成功。</p><h4 id="F-score"><a href="#F-score" class="headerlink" title="F-score"></a>F-score</h4><p>从IR角度，可以实现新的评估方法。我们将模型结构定义中的pitch-rhythm split作为ground truth，将factor-wise数据增强（保持一个，改变另一个，vice versa），作为query。将因此出现了最大variance的实际上latent dimensions，成为结果集（result set）。用这种方法，可以量化评估模型的P、R和F分数。</p><p><img src="3.png" alt=""></p><p>这是第一个为明确编码的解耦模型定制的测量方法，因此将random作为baseline。</p><p><img src="4.png" alt=""></p><p>后面的case study略。</p><h1 id="想法与见解"><a href="#想法与见解" class="headerlink" title="想法与见解"></a>想法与见解</h1><p>一路看下来感觉很流畅，从音乐的结果来看也是一个非常好的模型，起解耦能力值得肯定。</p><p>值得挑一些刺的地方我认为有两点。</p><ol><li><p>在模型描述上，模型的图画得并不是很清楚。比如说在对z的分割上，原文中提到对z对半分割，而图中的描述并不能体现这一点。</p></li><li><p>F-score的evaluation做得并不是很清晰。这个measurement从实现上来说是衡量解耦效果的，但是与random对比并不是好的选择。我猜测，使用“没有明确解耦”的普通VAE比较，也许是一个更好的选择。这就像是ON-LSTM的论文，也有人质疑，普通的LSTM会不会也有一定的order性呢？</p></li></ol><p>模型可以发展的地方相比缺点来说要多得多。首先是解耦设计的灵活性，几乎可以无成本地在之后进行扩展；其次，解耦类比生成的这个topic本身，也应该是正确的方向。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;组里大佬们的工作。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="VAE" scheme="http://ldzhangyx.github.io/tags/VAE/"/>
    
      <category term="representation learning" scheme="http://ldzhangyx.github.io/tags/representation-learning/"/>
    
      <category term="analogy" scheme="http://ldzhangyx.github.io/tags/analogy/"/>
    
  </entry>
  
  <entry>
    <title>我们来聊聊EsAC和Essen Folk Song Database</title>
    <link href="http://ldzhangyx.github.io/2019/07/26/esac/"/>
    <id>http://ldzhangyx.github.io/2019/07/26/esac/</id>
    <published>2019-07-26T07:01:14.000Z</published>
    <updated>2019-07-27T08:39:22.296Z</updated>
    
    <content type="html"><![CDATA[<p>古董级民歌数据集，对数据格式做了一些整理，在本文进行解释和说明。</p><a id="more"></a><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>EsAC全称是Essen Associative Code，可能是现存最长的音乐编码系统。EsAC的概念可以追溯到200年前，于1982年正式形成。因为其年代过于久远，现在很多音乐文档，尤其是欧洲音乐文档都在使用这种编码格式。</p><p>1980年-1994年，项目在Helmut Schaffrath在Essen领导推动这个项目。1994年Schaffrach意外去世后，他生前所在的实验室继续着他的工作。</p><h2 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h2><p>EsAC项目搜集了世界各地（尤其是欧洲）的民歌和历史上的音乐，编码后用于方便地进行搜索、分析和表示研究。</p><h1 id="格式说明"><a href="#格式说明" class="headerlink" title="格式说明"></a>格式说明</h1><p>EsAC被编码为Database record，在esac-data.org能下载到其txt版本。在这里，一条record对应一首曲调。</p><p>一条常见的记录如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ALTDEU</span><br><span class="line">CUT[Landsknecht als Schlemmer]</span><br><span class="line">REG[Europa, Mitteleuropa, Deutschland]</span><br><span class="line">KEY[A0359  08  G 4/4]</span><br><span class="line">MEL[1_  3b_3b_4_4_  5_5_5_7b_  ^_656_6_  5__0_</span><br><span class="line">    3b_  2_3b_2_2_  1__.+1_  +1_7b_6_6_  5__0_</span><br><span class="line">    5_  +1__+1__  7b__5__  7b__6b__  5__0_</span><br><span class="line">    7b_  5_5_5_3b_  5_5_5_1_  4_3b_2_2_  1__0_</span><br><span class="line">    1_  3b_3b_4_4_  5_5_5_7b_  ^_656_6_  5__0_</span><br><span class="line">    3b_  2_3b_2_2_  1__.+1_  +1_7b_6_6_  5__0_</span><br><span class="line">    5_  +1__+1__  7b__5__  7b__6b__  5__0_</span><br><span class="line">    7b_  5_5_5_3b_  5_5_5_1_  4_3b_2_2_  1__0__</span><br><span class="line">    +1__+1_+1_  7b__6b__  5_5_5__  4_4_4__  3b__3b__</span><br><span class="line">    7b_7b_7b__  5__5_5_  4_4_4__</span><br><span class="line">    3b_3b_3b_3b_  4_3b_2_2_  1__3b__</span><br><span class="line">    4_.41_2_  3b_.45_.6b  7b__.6b_  5_4_3b_2_  1__. //] &gt;&gt;</span><br><span class="line">FCT[Romanze, Ballade, Lied]</span><br><span class="line">CMT[Forster II. 1540 No. 17.]</span><br></pre></td></tr></table></figure><p>其中音乐旋律信息为单音旋律，由简谱格式构成。是的，当时他们采用了简谱格式。旋律包含在MEL字段里。</p><ul><li>CUT：标题</li><li>REG：区域</li><li>TRD：曲调来源，如书籍、录音带</li><li>KEY：签名行</li><li>MEL：旋律线</li><li>FKT：歌曲功能，如舞曲</li><li>BEM：标记</li><li>TXT：歌词</li></ul><h2 id="KEY"><a href="#KEY" class="headerlink" title="KEY"></a>KEY</h2><p>签名行包含了正确解释旋律所需要的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KEY[A0359  08  G 4/4]</span><br></pre></td></tr></table></figure><p>这一行包含了：签名、最小节奏单位、调性、拍子，四个信息。</p><p><img src="1.png" alt=""></p><h2 id="EsAC-syntax"><a href="#EsAC-syntax" class="headerlink" title="EsAC syntax"></a>EsAC syntax</h2><p>EsAC的语法基本与简谱一致，但在细微之处做了很多的补充。<br>MEL中的数字代表首调音高，而2、3等数字代表相对音高。</p><p><img src="2.png" alt=""></p><p>节奏以下划线和点作为标记。</p><p><img src="3.png" alt=""></p><ul><li>我自己在看数据的时候遇到了”^”的标记，这个意味着跨小节延音。</li><li>两个空格代表小节线</li><li>换行代表断句。</li><li>注意，这里的附点一般出现在延音符后，会将延音后总时长*150%。</li><li>注意，延音符连着出现，时长成倍增长。</li></ul><p>i.e. <code>5__</code>的时长是4倍，<code>5__.</code>的时长是6倍，<code>5___</code>的时长是8倍。</p><ul><li>标注中会出现括号，意味着出现了三连音等连音。</li></ul><p>i.e. <code>(-3-71)  3_2_(-5-72)  4_3_</code></p><p>即使经过这样的处理之后，还是会有这等肮脏的例子，可以说是非常恶心了……</p><p>i.e. <code>33  2__(^_1#_2_)(2_3_4_)  5__(^_4#_5_)(5_3_1_)</code></p><p>甚至还有打拍子……：</p><p>i.e. <code>xxxx  xxxx  1_2-5  1_ //] &gt;&gt;</code></p><p>甚至还有神奇的调子（居然还是德国标注法，将B调标为H调是什么鬼辣！）：</p><p>i.e. <code>KEY[U0002  16  H 6/8]</code></p><p>甚至还有不在旋律结束处加//标记的：</p><p>i.e. <code>1_+2+31_+3+51_+5_+5__  1_+5+61_+6++11_++1_++1__] &gt;&gt;</code></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://wiki.ccarh.org/wiki/EsAC" target="_blank" rel="noopener">http://wiki.ccarh.org/wiki/EsAC</a></li><li><a href="http://www.cs.uu.nl/events/dech1999/dahlig/tsld001.htm" target="_blank" rel="noopener">http://www.cs.uu.nl/events/dech1999/dahlig/tsld001.htm</a></li><li><a href="http://www.esac-data.org/" target="_blank" rel="noopener">http://www.esac-data.org/</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;古董级民歌数据集，对数据格式做了一些整理，在本文进行解释和说明。&lt;/p&gt;
    
    </summary>
    
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
      <category term="dataset" scheme="http://ldzhangyx.github.io/tags/dataset/"/>
    
      <category term="music information retrival" scheme="http://ldzhangyx.github.io/tags/music-information-retrival/"/>
    
      <category term="MIR" scheme="http://ldzhangyx.github.io/tags/MIR/"/>
    
      <category term="音乐结构分析" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>《Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2019/07/21/gnn-for-performance/"/>
    <id>http://ldzhangyx.github.io/2019/07/21/gnn-for-performance/</id>
    <published>2019-07-21T08:38:46.000Z</published>
    <updated>2019-07-21T16:15:10.050Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章的亮点在于利用了图网络编码额外信息，来改进其他任务效果的思想。这篇文章写作清晰，想法新颖，值得关注。文章出自韩国KAIST的Juhan Nam老师组（出身Stanford的CCRMA实验室，师承Malcolm Slaney老师）。</p><a id="more"></a><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文使用图神经网络表示乐谱，将其应用于钢琴演奏的渲染过程中。具体地说，本文使用了音符级别的门控图网络（GGNN）和在小节级别的带分层Attention机制（HAN）的bi-LSTM，进行模型设计。为了建模不同的表现风格，本文使用了VAE。试验结果表明，本文提出的模型产生了更像人类的表现。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>如同演员的表现能更好地调动观众情绪一样，音乐的演奏也具有很多细节。使用计算方法表达演奏，从基于规则的方法、高斯过程、卡尔曼滤波、贝叶斯网络、条件随机场、循环神经网络，都有相关的研究。然而，之前的方法仍然在这个task上有缺陷。</p><p>近年来，生成模型在各个领域取得了很多成果，音乐上如自动作曲、音乐转录、声音合成等，都有显著成就。</p><p>将神经网络应用到音乐数据，一个主要问题在于定义其输入结构。一般来说，乐谱根据其音高，转换成一个一维的序列数据(Simon &amp; Oore, 2017; Oore et al., 2018; Jeong et al., 2018)。但是一维的编码可能会丢失音符之间的一些多边关系。</p><p>另一种输入表示是通过采样，得到Piano Roll的2D矩阵。这使得CNN可以使用，但是基于采样的表示需要更高的维度，和随乐曲复杂性而跟着增长的时间分辨率。这种高维可能会阻碍模型学习长期结构。</p><p>为了解决这个问题，我们提出了一种基于GNN的模型，音符作为图中的节点，而乐谱中音符的关系被转化为一个表（后文会详细论述表的构造）。</p><p>如图1所示，我们将GNN结合到轻量的RNN来学习长期结构。此外，我们建议使用迭代循环来使用彼此的结果更新GNN和RNN的输入。</p><p><img src="1.png" alt=""></p><p>与其他生成任务类似，给定condition，生成各种结果是建模有表现力的演奏的重要目标。我们采用VAE训练模型，包含相同条件C但具有不同输出Y的数据。</p><p>系统的范围侧重于为MusicXML格式的乐谱生成MIDI格式的演奏。</p><p>论文两点有两个：</p><ol><li>首次尝试图网络学习乐谱表示；</li><li>HAN+RNN的新方法，对钢琴演奏的模仿是无需数据上的额外注释的。</li></ol><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h2><p>虽然CNN和RNN分别在图像处理和序列建模方面取得了重大进展，但是有各种类型的数据无法用这些网络正确处理，图形就是其中一个例子。利用图形神经网络（GNN）处理图形数据的早期研究由（Gori等人，2005）和（Scarselli等人，2009）引入。（Li et al。，2016）介绍了门控图神经网络（GGNN），它结合了现代RNN实践的门控循环单元和GNN。虽然之前的模型受到收缩映射的限制，但GGNN模型首先克服了这一局限。最近使用GNN的研究在各种任务中取得了最新成果，例如分析引文网络（Kipf＆Welling，2016），分子结构（Jin等，2018），程序代码（Allamanis等，2018）），学习结构化政策（Wang et al。，2018a）。</p><h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><p>我们的模型组合了音符级的GGNN，小节级的HAN（使用LSTM），使用在迭代方法中。</p><h2 id="GGNN"><a href="#GGNN" class="headerlink" title="GGNN"></a>GGNN</h2><p>我们采用了有向多类别边GGNN（其实就是有向图，箭头还分了个类）学习输入乐谱中的隐含表示。图可以表示为边与节点的集合，即$G=(V,E)$，其中V是节点，E是边。在音乐中，E就是相邻音符之间的连接线。</p><p>我们定义了六种边：next，rest，onset，sustain，voice，slur。</p><p>next：将音符连接到接下来的音符上。如接下来的音符在上一个音符结束时恰好开始。<br>rest：将音符连接到休止符后面的音符上。多个休止符会被当做单个处理。<br>onset：连接两个同时开始的音符。<br>sustain：在一个音符的开始和结尾中间出现的音符用sustain连接。<br>voice：是next边集的子集。它们仅用以连接同一个语音内的音符。<br>slur：在同一个圆滑线下的音符通过slur边互相连接。</p><p>（sustain和slur的定义有一些模糊，因为其没有注明是否仅为邻接的）</p><p>除了onset边以外，所有的边都是有向的。所以我们将前向和后向的边视为两种不同的类型。同时，为每一个音符添加自我连接，这样一共得到12种不同的边。每种边共享不同的权重参数。</p><p><img src="2.png" alt=""></p><p>我们使用GGNN，因为它在学习图中的节点级表示方面有优势。</p><p>GGNN的概念介绍如下：</p><p><img src="r1.png" alt=""><br><img src="r2.png" alt=""><br><img src="r3.png" alt=""><br><img src="r4.png" alt=""></p><p>论文中对于GGNN的表述没有超出上面所介绍的范围。</p><h2 id="Hierarchical-Attention-RNN（HAN）"><a href="#Hierarchical-Attention-RNN（HAN）" class="headerlink" title="Hierarchical Attention RNN（HAN）"></a>Hierarchical Attention RNN（HAN）</h2><p>我们之前的工作曾使用过HAN渲染有表现力的钢琴演奏。（对比了HRNN和HMRNN）</p><p>在分层RNN模型中，使用HAN的原因是其直接适用于GNN。事实上，因为HAN使用注意力来总结较低级别的表示，因此它可以直接应用于任何类型的网络。</p><p>我们的系统里，使用了context attention（《Hierarchical attention networks for document classification》里的），将音符表示转为小节向量。attention同时使用了多头注意力机制（multi-head attention）。通过这种方法，每一个小节的内容可以转化到高层的一个节点里。</p><p>作者的公式有一些问题，不过大致上就是attention计算那一套，其中$u_c$是context vector。因为context vector并没有接受其他输入，实际上这是一个trainable的参数。计算细节不在此赘述。</p><p><img src="eq2.png" alt=""></p><h2 id="Iterative-Sequential-Graph-Network"><a href="#Iterative-Sequential-Graph-Network" class="headerlink" title="Iterative Sequential Graph Network"></a>Iterative Sequential Graph Network</h2><p>组合来自不同分层单元的输出的简单方法是将它们连接为单个向量。然而，这种方法具有以下限制：较低级别的层不能以较高层中编码的长期上下文为条件，因为较高级别的输出不会影响较低级别的层。<br>在HRNN和HM-RNN中，较低级别的隐藏状态在分层边界处被馈送到较高级别，反之亦然。然而，在HAN中，隐藏状态传播仅在自下而上的方向上进行。<br>当模型的目标结果是给定顺序输入的单个输出时，例如在最初应用HAN的文档分类中，这种限制并不重要。但是为每个音符学习音乐表示时，我们希望高层的信息也能传到底层去，利用更加扩展的上下文学习。<br>为了克服这个限制，我们提出了GGNN和HAN的组合，称为迭代顺序图网络（ISGN），GGNN和HAN以迭代的方式将它们的结果互相馈送：</p><p><img src="3.png" alt=""></p><p>GGNN不仅接受音符表示，还接受高层的hidden state，以concatenate的方式输送。第一次迭代中，高层信息为0，之后再用HAN高层状态接进来。</p><p>这样的结构有两个优点：</p><ol><li>GGNN可以将HAN输出作为输入，考虑更长时间的context。</li><li>RNN可以自回归推理，可以补偿GGNN中缺乏的自回归机制。</li></ol><p>另外，多次迭代也一定程度上能弥补非自回归模型的缺点。</p><h1 id="Expressive-Performance-Rendering-System"><a href="#Expressive-Performance-Rendering-System" class="headerlink" title="Expressive Performance Rendering System"></a>Expressive Performance Rendering System</h1><p>下图展示了模型结构。</p><p><img src="4.png" alt=""></p><h2 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h2><p>模型使用预定义的乐谱和演奏作为输入和输出。特征提取方案在《 Score and performance features for rendering expressive music performances》有详述。输入feature包括各类音乐信息，比如音高、市场、速度、响度、起始偏差、清晰度、踏板等。</p><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><p>系统包括三个结构：乐谱编码器$E_s$，演奏编码器$E_p$，演奏解码器$D_p$。对于给定输入X，$E_s$推断乐谱条件$C$。模块包括两层GGNN，和一层LSTM。输入X经过三层选链接，第一层GGNN仅更新音符级别特征（固定小节级别的特征）；第二层GGNN更新整个隐状态。输出进行skip connection组成C。</p><p>根据C和对应的演奏特征Y，$E_p$编码出latent vector Z。Encoder的输入使用了全连接，对concatenated data进行了降维。编码器由GGNN和LSTM组成，具体如图。</p><p>Decoder解码器产生演奏特征Y，迭代地推断输入乐谱中每个音符的演奏参数。解码采用了分层解码的概念（《A hierarchical latent vector model for learning long-term structure in music》）。</p><p>解码器的输入是C、小节级别演奏风格向量$z_m$，和初始演奏参数的concatenatation。具体设置可以参考原论文。</p><h1 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h1><p>最近使用深度神经网络进行音乐生成的最值得注意的研究是Music Transformer（Huang et al。，2019）。该研究的目的是通过将作曲和表演结合为一个单一阶段来产生音乐。基于由self-attention组成的原始Transformer模型（Vaswani等，2017），他们提出了相对位置嵌入的音高和时间，并成功地通过超越他们以前的基于LSTM的模型生成具有长期结构的音乐作品 （Oore等，2018）。然而，如上所述，该任务与本文的范围不同。我们的研究侧重于解释和执行给定的乐谱，但Music Transformer更像是制作即兴创作。</p><p>最近，VAE已被用于若干音乐数据生成模型中，例如音乐生成（Roberts等，2018），音乐风格转移（Brunner等，2018）。<br>（Maezawa，2018）正如我们在这项工作中所做的那样，采用有condition的VAE来表现音乐表现。然而，VAE中的潜在向量是在音符级别生成的，而我们的模型使用VAE编码整个演奏，因此单个潜在向量可以表示整个片段的表现风格。<br>已经有关于使用包括神经网络的数据驱动方法自动生成表达演奏任务的研究，其在（Cancino-Chacon等人，’2018）中得到了很好的总结，但是它们实现了有限的演奏要素。<br>例如，他们只推断了速度（Malik＆Ek，2017），忽略了速度变化（Lauly，2010; Giraldo＆Ramirez，2016），假设旋律总是高音（Flossmann等，2013; Kim等 。，2013），或使用标准化的节奏（Grachten＆Cancino Chacon’，2017）。<br>我们的模型旨在实现全面的演奏要素。</p><p>将图连接应用在音乐上，生成有表现力的演奏，也有研究(Moulieras &amp; Pachet, 2016)。但是系统仅限于生成单音旋律，且图连接仅用于捕获单音上的音符特征。这个研究与我们的有基本性的不同。</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>收集了MuseScore、Musicalion（这是公开的吗？）的MusicXML数据，以及Yamaha e-competition的MIDI数据，将其做了对齐（Makamura, 2017）。</p><p>收集到了16位作曲家的227个作品，以及1061个演奏，包含3606930个音符。</p><h2 id="对比模型"><a href="#对比模型" class="headerlink" title="对比模型"></a>对比模型</h2><p>本文使用了基于HAN的VAE模型。HAN模型包括使用HAN的score encoder和演奏decoder。score encoder使用音符和语音LSTM作为字符表示，HAN作为节拍几别和小节级别的表示。Baseline省略了HAN和语音LSTM，仅使用音符级别的LSTM。仅使用GGNN替换语音和音符LSTM的修改版本被称为G-HAN。</p><p>模型对比如下：</p><p><img src="5.png" alt=""></p><h2 id="训练设置"><a href="#训练设置" class="headerlink" title="训练设置"></a>训练设置</h2><p>具体参见原论文。</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="重建误差（Reconstruction-Loss）"><a href="#重建误差（Reconstruction-Loss）" class="headerlink" title="重建误差（Reconstruction Loss）"></a>重建误差（Reconstruction Loss）</h3><p><img src="6.png" alt=""></p><h3 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h3><p><img src="7.png" alt=""></p><h3 id="聆听测试"><a href="#聆听测试" class="headerlink" title="聆听测试"></a>聆听测试</h3><p><img src="8.png" alt=""></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>我们提出了一种迭代序列图网络，它结合了门控图神经网络和分层注意RNN，用于为给定的乐谱建模表现的钢琴演奏。定量和定性评估表明，与以前的模型相比，所提出的模型取得了显着的进步。对于未来的工作，我们将进一步研究如何定量评估表达性能模型。</p><h1 id="想法和见解"><a href="#想法和见解" class="headerlink" title="想法和见解"></a>想法和见解</h1><p>用图网络进行乐谱表示是一个很棒的主意。音乐不同于NLP其他任务的一点在于，它可以是多声部协调进行的；音乐的Piano Roll相比图像的丰富信息来说，也比较稀疏。乐谱上的音符稀疏度，用一个图来表示是相当合理的。规定图网络的连接线属性，对图拓扑结构进行定义和约束，图就能自然地表示出乐谱。</p><p>论文阐述了一个生成任务，VAE的中间结果z是随机取样的。但是从表示学习的角度来看，VAE能解耦出演奏风格，z必然有其意义。如果指定VAE中间的z，而不是随机生成，那么能否做到某种意义上的演奏风格迁移？</p><p>本文的模型设计也颇有意思。实际上，LSTM的表示是GNN的高层表示，这种表示方法是自然且高效的，如果不使用GNN，这种方法难以自然地导出。HAN和GNN的流动机制也是一个很有意思的点。</p><p>评估中，模型的性能并不突出，我个人猜测是因为音乐生成任务较困难，在其他的task上也许能有更好的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章的亮点在于利用了图网络编码额外信息，来改进其他任务效果的思想。这篇文章写作清晰，想法新颖，值得关注。文章出自韩国KAIST的Juhan Nam老师组（出身Stanford的CCRMA实验室，师承Malcolm Slaney老师）。&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="structure" scheme="http://ldzhangyx.github.io/tags/structure/"/>
    
      <category term="music generation" scheme="http://ldzhangyx.github.io/tags/music-generation/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
      <category term="VAE" scheme="http://ldzhangyx.github.io/tags/VAE/"/>
    
      <category term="图网络" scheme="http://ldzhangyx.github.io/tags/%E5%9B%BE%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
