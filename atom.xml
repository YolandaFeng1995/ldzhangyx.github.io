<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yixiao Zhang&#39;s Website</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ldzhangyx.github.io/"/>
  <updated>2018-09-26T05:44:18.286Z</updated>
  <id>http://ldzhangyx.github.io/</id>
  
  <author>
    <name>Yixiao Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《Multi-modal Sentence Summarization with Modality Attention and Image Filtering》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/25/Multi-modal%20Sentence%20Summarization%20with%20Modality%20Attention%20and%20Image%20Filtering/"/>
    <id>http://ldzhangyx.github.io/2018/09/25/Multi-modal Sentence Summarization with Modality Attention and Image Filtering/</id>
    <published>2018-09-25T20:07:31.000Z</published>
    <updated>2018-09-26T05:44:18.286Z</updated>
    
    <content type="html"><![CDATA[<p>宗成庆老师的这篇文章发表于<a href="https://acl2018.org/programme/papers/" target="_blank" rel="noopener">ACL’18</a>，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。<br>原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf</p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文介绍了一种多模态句子摘要任务，输入为一张图片和一对句子，输出为摘要。作者称其为MMSS任务。任务的难点在于怎么将视觉信息输入到框架里，以及怎么减小噪音。针对这两个问题，作者分别提出了两个方法：同时施加不同的注意力到文本和图片上；使用图像过滤Image Filter来降噪。</p><p>介绍主要是讲了文本摘要的历史，以及多模态方法最早应用在翻译领域，表现特别好，但是作者认为在摘要上表现得应该更好。</p><p>在解决MMSS任务的时候，作者准备使用分层注意力机制，底层分别关注图片和文本的内部，而上层对两个模态进行平衡。因为图片不能表现很多抽象内容，所以图片特征需要过滤去噪；为了解决生成句子结巴的问题，使用了coverage方法。</p><p>顺便他们做了一个数据集，真是让人肝疼的工作量。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图片展示了多模态模型的实际效果要好于文本模型。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>模型可见由句子编码器，图片编码器，摘要解码器和图片过滤器四个部分组成。</p><h3 id="模型图"><a href="#模型图" class="headerlink" title="模型图"></a>模型图</h3><p>模型简图如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="句子Encoder"><a href="#句子Encoder" class="headerlink" title="句子Encoder"></a>句子Encoder</h3><p>模型的句子Encoder使用了Bi-GRU。</p><script type="math/tex; mode=display">\overrightarrow{h}_i =  GRU(E[x_i], \overrightarrow{h}_{i-1})</script><script type="math/tex; mode=display">\overleftarrow{h}_i =  GRU(E[x_i], \overleftarrow{h}_{i-1})</script><h3 id="图片Encoder"><a href="#图片Encoder" class="headerlink" title="图片Encoder"></a>图片Encoder</h3><p>图片Encoder使用了VGG19，抽取了两种特征：7x7x512的局部特征（flatten之后成为了49x512），和4096维的全局特征。其中局部特征表示为：</p><script type="math/tex; mode=display">A = (a_1, ..., a_L), L=49 \\a_l \in \mathbb{R}^{512}</script><h3 id="摘要Decoder"><a href="#摘要Decoder" class="headerlink" title="摘要Decoder"></a>摘要Decoder</h3><p>Decoder使用了单向GRU：</p><script type="math/tex; mode=display">s_t = GRU(s_{t-1}, y_{t-1}, c_t)</script><p>其中，初始状态$s_0$，作者提出了几种初始化策略：</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h2}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{q}q)</script><p>利用了全局特征；</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_i)</script><p>与</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_ia_i)</script><p>利用了局部特征；</p><p>其中，$q$为全局特征，$a$为各个局部特征，如上文所说。</p><h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><p>在Decoder使用的context向量$c_t$，使用一个顶层的Attention，对图片context和文本context进行平衡统一：</p><script type="math/tex; mode=display">c_t = \beta_t^{txt}\mathbf{V}_Tc_{txt} + \beta_t^img\mathbf{V}_Ic_{img}其中，$$ \beta_t^{txt} = \sigmoid(\mathbf{U}_as_{t-1}+\mathbf{W}_ac_{txt})</script><script type="math/tex; mode=display">\beta_t^{img} = \sigmoid(\mathbf{U}_bs_{t-1}+\mathbf{W}_bc_{img})</script><p>对于文字的attention，使用普通的attention：</p><script type="math/tex; mode=display">c_txt = \sum_{i=1}^N\alpha_{t,i}^{txt}h_i</script><p>其中$N$为输入的文字序列长度，也就是embedding过后的向量总数量，$h<em>i$是Encoder的第$i$个Hidden State，这个公式将其加权求和。其中$\alpha</em>{t,i}^{txt}$与Decoder上一个时态的状态$s_{t-1}$与第$i$个Encoder的Hidden State，$h_i$有关。</p><script type="math/tex; mode=display">\alpha_{t,i}^{txt} = \frac{exp(e_{t,i}^{txt})}{\sum_{j=1}^Nexp(e_{t,j}^{txt})}</script><script type="math/tex; mode=display">e_{t,i}^{txt} = f(s_{t-1}, h_i) = v_a^T\tanh(\mathbf{U}_cs_{t-1} + \mathbf{W}_ch_i)</script><p>而对于图片attention，作者将每个feature map作为输入，进行attention处理：</p><script type="math/tex; mode=display">c_img = \sum_{i=1}^L\alpha_{t,i}^{img}a_l</script><p>其中L为feature map数量，$a_l$为局部特征。</p><script type="math/tex; mode=display">\alpha_{t,i}^{img} = \frac{exp(e_{t,i}^{img})}{\sum_{j=1}^Nexp(e_{t,j}^{img})}</script><script type="math/tex; mode=display">e_{t,i}^{img} = f(s_{t-1}, h_i) = v_b^T\tanh(\mathbf{U}_ds_{t-1} + \mathbf{W}_da_l)</script><h3 id="Coverage-Model-覆盖率模型"><a href="#Coverage-Model-覆盖率模型" class="headerlink" title="Coverage Model(覆盖率模型)"></a>Coverage Model(覆盖率模型)</h3><p>覆盖率模型可以参考《Get to the point》一文的论文笔记。</p><h3 id="Image-Filter"><a href="#Image-Filter" class="headerlink" title="Image Filter"></a>Image Filter</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;宗成庆老师的这篇文章发表于&lt;a href=&quot;https://acl2018.org/programme/papers/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ACL’18&lt;/a&gt;，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。&lt;br&gt;原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="多模态" scheme="http://ldzhangyx.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
      <category term="生成式摘要" scheme="http://ldzhangyx.github.io/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%91%98%E8%A6%81/"/>
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
