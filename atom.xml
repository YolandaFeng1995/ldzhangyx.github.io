<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yixiao Zhang&#39;s Website</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ldzhangyx.github.io/"/>
  <updated>2018-10-15T14:42:35.549Z</updated>
  <id>http://ldzhangyx.github.io/</id>
  
  <author>
    <name>Yixiao Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>self-attention与attention简要梳理</title>
    <link href="http://ldzhangyx.github.io/2018/10/14/self-attention/"/>
    <id>http://ldzhangyx.github.io/2018/10/14/self-attention/</id>
    <published>2018-10-14T15:31:02.000Z</published>
    <updated>2018-10-15T14:42:35.549Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention的计算机制"><a href="#Attention的计算机制" class="headerlink" title="Attention的计算机制"></a>Attention的计算机制</h1><h2 id="总说"><a href="#总说" class="headerlink" title="总说"></a>总说</h2><p>Attention机制主要作用在Decode阶段。在这一阶段，$s_t$的运算受到了$s_{t-1}$，$y_{t-1}$和$c_t$的影响。根据目前比较统一的观点，Attention值的计算是相似度比较的结果。Decoder端的Query与Encoder端的各个Key进行相似度比较得到不同的权重，最后对各个Key对应的Value进行归一化的加权累加，得到Attention Value。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>在实际应用中，Key就是Value，即K=V；而K与Q（Query）实际上也常用Hidden State来计算。</p><h2 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h2><p>相似度计算有几种方式。下面公式中$t$代表Decoder第$t$时刻，$s$代表Encoder中第$s$个位置的hidden state。</p><p>点积注意力：<script type="math/tex">s(h_t, h_s) = h_t^Th_s</script></p><p>一般的注意力：<script type="math/tex">s(h_t, h_s) = h_t^TW_ah_s</script></p><p>连接(concat)注意力：<script type="math/tex">s(h_t, h_s) = V_a^T\tanh(W_a[h_t;h_s])</script></p><p>加性注意力：<script type="math/tex">s(h_t, h_s) = V_a^T\tanh(Wh_t+Uh_s])</script></p><h2 id="之后的计算"><a href="#之后的计算" class="headerlink" title="之后的计算"></a>之后的计算</h2><p>我们将$s(h_t, h_i)$进行相似度计算的结果（记为$e_{ti}$）进行softmax归一化，再权值相加，得到最终的attention值：</p><script type="math/tex; mode=display">\alpha_{t,i}^{txt} = \frac{exp(e_{t,i}^{txt})}{\sum_{j=1}^N\exp(e_{t,j}^{txt})}</script><script type="math/tex; mode=display">c_{txt} = \sum_{i=1}^N\alpha_{t,i}^{txt}h_i</script><p>从统一的角度来看，如果将Decoder端的Query视为整体Q，Key视为Encoder端的整体K，Value在自然语言处理中一般直接使用K，记为V。所以，</p><script type="math/tex; mode=display">Q \in \mathbb{R}^{n*d_k}, K \in \mathbb{R}^{m*d_k}, V \in \mathbb{R}^{m*d_v}</script><p>所以上述Attention可以简要的抽象成下列公式：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其实质是将序列Q编码成了一个新的$n*d_v$的序列。如果将Query的逐个向量分开，那么公式也可以这么表达：</p><script type="math/tex; mode=display">Attention(Q_t, K, V) = \sum_{s=1}^m\frac{1}{z}\exp(\frac{<q_t, k_s>}{\sqrt{d_k}})v_s</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Attention的计算机制&quot;&gt;&lt;a href=&quot;#Attention的计算机制&quot; class=&quot;headerlink&quot; title=&quot;Attention的计算机制&quot;&gt;&lt;/a&gt;Attention的计算机制&lt;/h1&gt;&lt;h2 id=&quot;总说&quot;&gt;&lt;a href=&quot;#总说&quot;
      
    
    </summary>
    
      <category term="术语梳理" scheme="http://ldzhangyx.github.io/categories/%E6%9C%AF%E8%AF%AD%E6%A2%B3%E7%90%86/"/>
    
    
      <category term="attention" scheme="http://ldzhangyx.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>克苏鲁神话 中文合集</title>
    <link href="http://ldzhangyx.github.io/2018/10/10/cthulhu-book-share/"/>
    <id>http://ldzhangyx.github.io/2018/10/10/cthulhu-book-share/</id>
    <published>2018-10-10T08:41:45.000Z</published>
    <updated>2018-10-10T09:02:00.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>自己加入TRPG的团体已经有大半年了，然而一直对克苏鲁神话体系大背景不是特别了解。成为守秘人带团的时候，我的参照是规则书和剧本，却不能灵活地演绎一些新的剧情。</p><p>于是我Google了克苏鲁神话体系，终于在GitHub上找到了一个还算满意的开源项目。很多文章都是竹子翻译的，我个人认为翻译质量尚可，放在我的网站上，权当补档吧。</p><h1 id="版权"><a href="#版权" class="headerlink" title="版权"></a>版权</h1><p>本电子书的版权归作者和译者所有。</p><p>GitHub地址<a href="https://github.com/jokester/coc-zh" target="_blank" rel="noopener">点这里</a>。</p><h1 id="文档下载"><a href="#文档下载" class="headerlink" title="文档下载"></a>文档下载</h1><p>点击这里可以直接下载。</p><p><a href="coc-zh.pdf">点击下载</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;自己加入TRPG的团体已经有大半年了，然而一直对克苏鲁神话体系大背景不是特别了解。成为守秘人带团的时候，我的参照是规则书和剧本，却不能灵活地
      
    
    </summary>
    
      <category term="TRPG" scheme="http://ldzhangyx.github.io/categories/TRPG/"/>
    
    
      <category term="TRPG" scheme="http://ldzhangyx.github.io/tags/TRPG/"/>
    
      <category term="克苏鲁神话" scheme="http://ldzhangyx.github.io/tags/%E5%85%8B%E8%8B%8F%E9%B2%81%E7%A5%9E%E8%AF%9D/"/>
    
  </entry>
  
  <entry>
    <title>《A Deep Generative Framework for Paraphrase Generation》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/26/deep-para-generation/"/>
    <id>http://ldzhangyx.github.io/2018/09/26/deep-para-generation/</id>
    <published>2018-09-26T08:44:14.000Z</published>
    <updated>2018-09-28T21:10:12.212Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章发表在2018年的AAAI上，继承了VAE在自然语言处理上的应用，将其应用于句子复述上，用于生成相似的句子；同时因为RNN可以作为语言模型使用，语句的语法正确性也有一定提升。<br>论文地址：<a href="https://arxiv.org/pdf/1709.05074.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1709.05074.pdf</a></p><a id="more"></a><h1 id="背景材料"><a href="#背景材料" class="headerlink" title="背景材料"></a>背景材料</h1><p>关于变分自编码器在NLP领域的相关介绍，可以看<a href="http://rsarxiv.github.io/2017/03/02/PaperWeekly%E7%AC%AC%E4%BA%8C%E5%8D%81%E4%B8%83%E6%9C%9F/" target="_blank" rel="noopener">这篇讨论</a>。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.jpg" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>关于文本生成的背景研究，可以看<a href="https://www.msra.cn/zh-cn/news/features/ruihua-song-20161226" target="_blank" rel="noopener">这篇文章</a>。</p><p>关于VAE的原理（流形学习）和工程实现，可以看<a href="https://blog.csdn.net/JackyTintin/article/details/53641885" target="_blank" rel="noopener">这篇博文</a>。这篇博文详细介绍了VAE的训练过程，Encoder部分（识别模型）和Decoder部分（生成模型）的结构和各个参数的含义。这篇文章同时也提及了reparemetriazation trick的原理。</p><p>对于VAE结构的理解和讨论，可以看<a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">这篇文章</a>。</p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p>Keras实现<a href="https://github.com/paulx3/keras_generative_pg" target="_blank" rel="noopener">在这里</a>。</p><p>PyTorch实现<a href="https://github.com/ale3otik/paraphrases-generator" target="_blank" rel="noopener">在这里</a>。</p><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>复述生成是NLP一个重要的课题，本文的模型将VAE和LSTM结合起来生成复述。传统的VAE结合RNN时，生成的句子不符合复述的要求，所以我们将模型的Encoder和Decoder都将原始的句子作为condition进行干涉，这样就达到了复述的效果。这个模型简单、模块化，可以生成不同的多种复述。在量化评测里，本模型明显优于state-of-the-art模型；分析发现模型语法良好；在新数据集上进行了测试，提供了一个baseline。</p><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>Introduction这部分，作者介绍了Paraphrase在QA等方面的应用，以及之前的paraphrase方法，认为其多制约于规则，而深度学习的生成模型更加是数据驱动。<br>区别于VAE的在句子生成上的其他应用，本文的复述模型需要捕获原本句子的特征，所以unconditional的句子生成模型并不适用于此任务。conndition的机制在过去曾被应用到CV领域，然而CV的应用仅仅是用有限的class label作为condition，以及不需要任何的intermediate representation。本文的方法在Encoder和Decoder方面都进行了condition，并且通过LSTM得到intermediate representation。</p><p>本文的生成框架对比seq2seq模型，尽管后者可以使用beam search，但不能同时产生多个效果很好的结果，因为beam search的后面结果总是比最顶部的差。在Quora数据集上，本文的模型表现很好。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="VAE结构"><a href="#VAE结构" class="headerlink" title="VAE结构"></a>VAE结构</h3><p>VAE是一个深度生成式隐变量模型，可以从高维输入学习到丰富、非线性的表示。<br>编码器方面，相比AE使用了一个确切的编码器函数$q_{\Phi}$，VAE使用了后验分布$q_{\Phi}(\mathbf{z}|\mathbf{x})$（或者说识别模型）。这个后验分布通常被假设为高斯分部，所以参数$\Phi$只有均值和方差。VAE使后验分布$q_{\Phi}(\mathbf{z}|\mathbf{x})$尽可能地接近先验分布$p(\mathbf{z})$，通常也被视为高斯分布。<br>VAE的解码器模型，使用了另一个分布$p_{\theta}(\mathbf{x}|\mathbf{z})$，也就是输入隐变量$\mathbf(z)$，还原出$\mathbf{x}$。其参数$\theta$使用另一个前馈神经网络的输出值。<br>将观测数据$x^{(i)}$的对数似然可以写作：</p><script type="math/tex; mode=display">\log p_\theta(x^{(i)} = KL(q_\varPhi(z|x^{(i)})||p_\theta(z|x^{(i)})) + L(\theta, \varPhi; x^{(i)}))</script><p>将$ELBO$记为$L$，然后通过优化$L$，来间接优化似然。<br>$L$可以写作：</p><script type="math/tex; mode=display">L(\theta, \varPhi; x^{(i)})) = -KL(q_\varPhi(z|x^{(i)})||p_\theta(z)) + E_{q_\varPhi(z|x)}[\log p_\theta(x^{(i)}|z)]</script><p>优化目标变为后面两项。</p><p>具体的推导可以参考<a href="https://arxiv.org/pdf/1606.05908.pdf" target="_blank" rel="noopener">这个教程</a>。</p><p>更多地，在建模文字数据的时候，也可以使用KL-term annealing以及dropout of inputs of the decoder等训练技巧，避免一些问题。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>我们的训练集使用了N对句子，表示为${\mathbf{s}_n^{(o), \mathbf{s}_n^{(p)}}^N_{n=1}$，其中原始句子用$o$标注，复述句子用$p$标注。句子的向量表示标记为$x^{(o)}$与$x^{(p)}$，</p><p>去掉LSTM后，本文的宏观模型如图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>区别于传统的VAE，本文构建的模型将$x^{o}$作为了condition加在了Encoder和Decoder之上。</p><p>更加细节的结构如图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>实际上，本文的模型包括了三个LSTM encoder和一个LSTM decoder，总共有4个LSTM，如上图所示。</p><p>在VAE的Encoder方面，两个LSTM encoder被使用，第一个转换原始句子$s^{(o)}$到其向量表示$x^{o}$，与复述句子$s^{(p)}$送入第二个LSTM。将两个句子都编码进了LSTM之后，使用一个前馈网络输出$\Phi$，也就是向量表示的均值和方差，送入VAE。</p><p>在VAE的Decoder方面，VAE输出隐变量$z$，第三个LSTM编码原始句子，然后将原始句子的向量表示$x^{(o)}$与$z$一同送入第四个LSTM，产生复述的句子。</p><p>将LSTM抽象化，可以得到一个这样的模型：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>这个模型的variational lower-bound，也就是$ELBO$如下。最小化KL散度的过程等同于最大化$ELBO$的过程。</p><script type="math/tex; mode=display">L(\theta, \Phi; x^{(p)}, x^{(o)})) = E_{q_\Phi(z|x^{(p)}, x^{(o)})}[\log p_\theta(x^{(p)}|z, x^{(o)})] -KL(q_\Phi(z|x^{(p)}, x^{(o)})||p(z))</script><p>模型的训练方法与<a href="https://aclweb.org/anthology/K/K16/K16-1002.pdf" target="_blank" rel="noopener">《Generating sentences from a continuous space》</a>中的相同。这篇论文深深地影响了后续论文的思路。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>本文使用了两个数据集进行评测。</p><h4 id="MSCOCO数据集"><a href="#MSCOCO数据集" class="headerlink" title="MSCOCO数据集"></a>MSCOCO数据集</h4><p>这个数据集，也曾用于评测句子复述的方法，对120K张图片分别有人工标注的五句描述。构建数据集的时候，本文随机忽略一个描述，然后将剩下的描述两两配对组成数据集，同时限制长度在15个单词以内，以减少模型复杂度。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="Quora数据集"><a href="#Quora数据集" class="headerlink" title="Quora数据集"></a>Quora数据集</h3><p>Quora在2017年发布了一个数据集，包含400K行潜在的重复问题对。借助这个数据集，本文将重复的问题看作是复述的句子进行训练。重复的问题对大约有155K对，使用50K，100K，150K对进行训练，4K对作为测试。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="6.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>实验的主要参数设置参考了《Generating sentences from a continuous space》的<a href="https://github.com/kefirski/pytorch_RVAE" target="_blank" rel="noopener">实现代码</a>，未对任何数据集做fine-tuning。本文模型中，作者没有使用预先embedding好的词向量，而是自己进行了训练。embedding dimension设置为300，encoder和decoder的dimension是600，隐变量dimension是1100。Encoder只有一层而Decoder有两层。模型使用SGD进行训练，学习率固定为$5*10^{-5}$，dropout为0.3，batch size为32。</p><h2 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h2><p>本文使用了BLEU，METEOR和TER进行评测。因为这个过程类似翻译任务，所以借用翻译的指标效果会比较好。评测结果显示优于其他模型。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>评测包含了对实验结果的分析，可以直接阅读原文。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了一个深度生成框架，基于VAE，增加了seq2seq模型，用于生成释义。与传统的VAE和无条件句子生成模型不同，这个模型在输入句子上调节VAE的encoder和decoder侧，因此可以为给定句子生成多个释义。在一般复述生成数据集上评估所提出的方法，并且表明它在很大程度上优于现有技术，没有任何超参数调整。本文还评估了对最近发布的问题复述数据集的方法，并展示了其卓越的性能。生成的释义不仅在语义上与原始输入句子相似，而且还能够捕获与原始句子相关的新概念。</p><h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><p><strong>这篇文章是我阅读的第一篇关于VAE文本生成的论文</strong>，我将在本周使用PyTorch完成这个模型的复现工作。在学习VAE背后深层原理的过程中，我遇到了一些麻烦，只能说是初步理解。然而工程上VAE是一个相比GAN更加自然地适用于NLP的模型，其噪声与重建的对抗思路让我对文本生成有了全新的认识。</p><p><strong>对于VAE结构，我自己还需要做更多的推导、学习，反复阅读笔记最前面的几篇文章，加深理解。如果不能完整理解VAE的构建过程，就无从提起改进。</strong></p><h2 id="文章亮点"><a href="#文章亮点" class="headerlink" title="文章亮点"></a>文章亮点</h2><p>文章提出了一个基于VAE的模型，但这个模型并不是单纯的文本生成，而是使用了condition，那么这个condition对VAE公式要做一些改动，这个思维的推理过程结合了一定理论成分。<br>在句子复述的领域中，这个模型生成的句子在可读性（METEOR，人类评估）上有着一定优势，能不能用于语法改错？虽然语法改错上我猜测基于规则的模型会更加精确全面，但一定程度上自动改善语法问题，应该有一定帮助。</p><h2 id="想法-1"><a href="#想法-1" class="headerlink" title="想法"></a>想法</h2><p>这篇文章使用VAE来做句子复述，那么是否可以将condition的概念拓展到另外的领域？</p><p>在2016年的那篇VAE做NLP的开创性论文中，提及了这样一个问题，在实际的训练过程中，KL散度可能会出现collapse，导致后验等于先验。由于 RNN-based 的 decoder 有着非常强的 modeling power，直接导致即使依赖很少的 history 信息也可以让 reconstruction errors 降得很低，换句话说，decoder 不依赖 encoder 提供的 z 了，VAE-LSTM模型因此退化成RNN-LM语言模型。</p><p>在那篇论文里，使用的缓解办法就是KL cost annealing 和 Word dropout。目前对VAE结构的改进研究也很多，我认为改变VAE结构，进行微调，其效果有可能陷入GAN的一堆改进结构的泥淖里。</p><p>如果将VAE引入其他模型会怎么样呢？哈佛NLP组提出过<a href="https://zhuanlan.zhihu.com/p/40621695" target="_blank" rel="noopener">注意力模型的解决方案</a>。我觉得这方面的思考非常有吸引力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章发表在2018年的AAAI上，继承了VAE在自然语言处理上的应用，将其应用于句子复述上，用于生成相似的句子；同时因为RNN可以作为语言模型使用，语句的语法正确性也有一定提升。&lt;br&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1709.05074.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1709.05074.pdf&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="循环神经网络" scheme="http://ldzhangyx.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="变分自编码器" scheme="http://ldzhangyx.github.io/tags/%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    
      <category term="自动问答" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E5%8A%A8%E9%97%AE%E7%AD%94/"/>
    
      <category term="句子复述" scheme="http://ldzhangyx.github.io/tags/%E5%8F%A5%E5%AD%90%E5%A4%8D%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>《XiaoIce Band: A Melody and Arrangement Generation Framework for Pop Music》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/26/xiaoice-band/"/>
    <id>http://ldzhangyx.github.io/2018/09/26/xiaoice-band/</id>
    <published>2018-09-25T18:47:41.000Z</published>
    <updated>2018-10-02T07:47:29.997Z</updated>
    
    <content type="html"><![CDATA[<p>小冰乐队是我一直想做笔记的一篇论文，做音乐生成的人不多，这篇论文思路和我的想法又有些相似。这篇由中科大和微软合作撰写的论文，发表在KDD’18上。<br>论文地址：<a href="http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music" target="_blank" rel="noopener">http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music</a></p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>以前的模型在歌曲生成中有限制，因为歌曲对旋律和联系都有要求。除此之外，一些关系到歌曲质量的因子，例如和弦行进和节奏模式上做的不好。而且多轨音乐如何保证其和谐程度也是个未探索的问题。</p><p>本文提出了一个着力在流行音乐生成的端到端模型，“小冰乐队”，歌曲通过几种乐器进行演奏。对于旋律和和弦行进，本文设计了Chord based Rhythm and Melody CrossGeneration Model (CRMCG)架构；对于多轨音乐的组织，本文设计了Multi-Instrument Co-Arrangement Model (MICA)架构。</p><p>模型在一个真实世界数据集上做了拓展实验，证明了小冰乐队的有效性。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文章首先介绍了一下音乐生成的历史。2003年Conklin从真实音乐中采样生成音乐；之后N-gram和Markov模型也应用于生成上。这些方法需要手动选取特征。Magenta使用DRNN从MIDI中学习生成音乐，然而只是单轨。</p><p>现有的方法并没有很好的解决一些重要挑战。特别在这几个方向：</p><ul><li><p>和弦行进在流行音乐里通常是存在的，其指导了旋律的走向。如果能将和弦行进作为输入进行音乐生成，这是有好处的；流行音乐有几个固定的节奏模式（pattern），让歌曲更加结构化、暂停恰当。然而现有的逐个音符的生成模型无法考虑节奏模式，另一方面，这些模型尽管使用了和弦，但只是用单和弦作为输入的一个feature，而不考虑和弦的整体行进。</p></li><li><p>完整的歌曲考虑了多轨的和弦、鼓点、节奏模式的组织，以及其他乐器共同演奏的背景伴奏，但这些模型没能组织起多轨架构。</p></li><li><p>不同轨道和乐器有其自己的特点，然而它们也应该和谐演奏。极少现有工作研究了多轨音乐生成，但没有模型考虑到多轨之间的和谐性。</p></li></ul><p>本文推出了CRMCG和MICA模型。前者用来生成音乐，后者用来保证和谐。Attention和MLP用于捕捉其它task的有效信息。本文主要贡献如下：</p><ul><li><p>提出了端到端的多轨歌曲生成模型，包括旋律和组织。</p></li><li><p>基于音乐知识，作者利用和弦行进指导旋律进行和节奏模式，学习歌曲结构；然后使用旋律和节奏交叉生成的方法进行音乐生成。</p></li><li><p>作者发展了多任务的联合生成网络，在decoder层的每一个step里使用了其他的task states，以保证多轨音乐的和谐性。</p></li><li><p>大量的实验表明，模型优于baseline。</p></li></ul><h2 id="Ralated-Work"><a href="#Ralated-Work" class="headerlink" title="Ralated Work"></a>Ralated Work</h2><h3 id="音乐生成"><a href="#音乐生成" class="headerlink" title="音乐生成"></a>音乐生成</h3><p>在音乐生成上一开始有数据驱动的传统统计学方法，但是需要大量人力和领域知识。</p><p>深度学习上的方法有RNN，VAE和GAN几种方法。对于RNN来说，有一项工作利用分层RNN也生成了搭配的和弦和鼓点（看了一下引用列表，因为这个领域不大，除了VAE那篇以外我都是好好读过的）。</p><p>然而没有单一的研究考虑了音乐的特殊性（比如流行音乐里的和弦行进）。下图所示是各个模型的特点对比：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>多任务学习经常被用于在相关的任务间共享特征，因为一个任务中学到的特征可能对其他任务有帮助。多任务学习在NLP和CV里都广泛应用。</p><h2 id="背景知识-名词定义"><a href="#背景知识-名词定义" class="headerlink" title="背景知识/名词定义"></a>背景知识/名词定义</h2><h3 id="和弦的行进"><a href="#和弦的行进" class="headerlink" title="和弦的行进"></a>和弦的行进</h3><p>这段话是我自己总结的：</p><p>和弦的行进是人们在创作音乐时挖掘的套路。随着共同声部的保留和和弦顺/逆级进，一个成功的和弦走向能让人感到完善和谐。一般流行音乐中，在主和弦和结束式中插入调内和弦可以拓展歌曲长度，同时让歌曲流畅发展。</p><p>更多知识可以这个<a href="https://www.zhihu.com/question/46536599" target="_blank" rel="noopener">知乎问题</a>。</p><h3 id="节奏模式"><a href="#节奏模式" class="headerlink" title="节奏模式"></a>节奏模式</h3><p>流行音乐中会出现一些相同重复的结构，使得流行音乐有着结构化特点。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="乐器特点"><a href="#乐器特点" class="headerlink" title="乐器特点"></a>乐器特点</h3><p>很多流行歌是多轨的，而钢琴一般负责主旋律，其他乐器负责伴奏。</p><h2 id="问题陈述-amp-模型结构"><a href="#问题陈述-amp-模型结构" class="headerlink" title="问题陈述 &amp; 模型结构"></a>问题陈述 &amp; 模型结构</h2><p>表中为使用的数学符号含义。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="4.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="问题陈述"><a href="#问题陈述" class="headerlink" title="问题陈述"></a>问题陈述</h3><p>在音乐生成的任务上，给定输入为和弦进行式</p><script type="math/tex; mode=display">C = \{c_1, c_2, ..., c_{l_c}\}</script><p>其中$c_i$为和弦的one-hot编码，$l_c$是序列长度。</p><p>要生成的节奏</p><script type="math/tex; mode=display">R_i = \{r_{i1}, r_{i2}, ..., r_{il_r}\}</script><p>和旋律</p><script type="math/tex; mode=display">M_i = \{r_{i1}, r_{i2}, ..., r_{il_m}\}</script><p>。</p><p>框架分为四个部分（如图）：</p><ol><li>数据处理</li><li>CRMCG单音轨旋律生成</li><li>MICA多音轨组织生成</li><li>展示</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="CRMCG"><a href="#CRMCG" class="headerlink" title="CRMCG"></a>CRMCG</h3><p>CRMCG的模型框架如图所示：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="5.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图中我用Surface Pen做了一些不影响阅读的标记。</p><p>给定一个长度为N的和弦行进，我们要生成的当然是N个音乐片段。我们将这N个音乐片段标记为${p_1, p_2, …, p_N}$，每一个片段里（假设片段$i$），有着节奏$R_i$和旋律$M_i$。</p><p>和弦的one-hot编码需要降维。通过一个Embedding矩阵输出和弦的低维表示：</p><script type="math/tex; mode=display">\overline{C} = E_cC, E_c \in \mathbb{R}^{V_c * d}</script><p>然后使用一个GRU来处理和弦。然后使用这些hidden states来帮助生成节奏和旋律。</p><script type="math/tex; mode=display">\overline{h}^c_{i,0} = GRU(\overline{c}_i), i= 1,2,...,l_c</script><h4 id="节奏生成"><a href="#节奏生成" class="headerlink" title="节奏生成"></a>节奏生成</h4><p>生成的节奏需要与音乐已经存在的部分保持和谐。所以，要把之前的音乐部分考虑进去。</p><p>首先我们将之前生成的节奏Embedding回来：</p><script type="math/tex; mode=display">\overline{R}_{t-1} = E_rR_{t-1}, E_r \in \mathbb{R}^{V_r * d}</script><script type="math/tex; mode=display">\overline{M}_{t-1} = E_mM_{t-1}, E_m \in \mathbb{R}^{V_m * d}</script><p>其中$V_m$与$V_r$是音符和鼓点的字典尺寸。然后输入GRU Encoder：</p><script type="math/tex; mode=display">\overline{h}^m_{t-1,i} = GRU(\overline{m}_{t-1,i}), i= 1,2,...,l_m</script><script type="math/tex; mode=display">\overline{h}^r_{t-1,i} = GRU(\overline{r}_{t-1,i}), i= 1,2,...,l_r</script><p>将两个Encoder最后的hidden states进行concat拼接，然后线性变换作为Decoder的初始state，依次进行decode。</p><script type="math/tex; mode=display">s^r_0 = g(W[\overline{h}^m_{t-1,l_m}, \overline{h}^r_{t-1,l_r}]+b), W \in \mathbb{R}^{b*b}</script><script type="math/tex; mode=display">s^r_i = GRU(u^r_{i-1}, s^r_{i-1}), i>0</script><script type="math/tex; mode=display">y^r_i = softmax(s^r_i)</script><h4 id="旋律生成"><a href="#旋律生成" class="headerlink" title="旋律生成"></a>旋律生成</h4><p>不同于节奏生成，旋律的生成i要考虑到和弦的因素。与节奏部分类似地，将三个hidden state进行拼接，然后进行Decode。</p><script type="math/tex; mode=display">s^m_0 = g(W[\overline{h}^m_{t-1,l_m}, \overline{h}^r_{t,l_r}, \overline{h}^c_t]+b), W \in \mathbb{R}^{b*b}</script><script type="math/tex; mode=display">s^m_i = GRU(u^m_{i-1}, s^m_{i-1}), i>0</script><script type="math/tex; mode=display">y^m_i = softmax(s^m_i)</script><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>对于两个部分，分别设置一个损失函数，形式都为softmax cross-entropy函数。在节奏模块，只更新与节奏损失$L^r$有关的参数；在旋律模块，更新所有参数。</p><h3 id="MICA"><a href="#MICA" class="headerlink" title="MICA"></a>MICA</h3><h4 id="MICA-1"><a href="#MICA-1" class="headerlink" title="MICA"></a>MICA</h4><p>设计了一个One-to-Many Sequence Generation任务（OSMG）。不同于常规的多序列学习，OSMG生成的序列有着紧密的联系。</p><p>如同下图所示，Decoder的hidden state包含了序列信息。所以，在生成一个音轨中的音符的时候，自然地引入了其他音轨的hidden state。作者设计了两个合作的cell在decoder的隐层之间进行hidden states的整合。</p><h4 id="Attention-Cell"><a href="#Attention-Cell" class="headerlink" title="Attention Cell"></a>Attention Cell</h4><p>作者设计了一个attention cell，捕获其它task的state供current states使用，结构如下图：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="7.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>attention机制如下列公式：</p><script type="math/tex; mode=display">a^i_{t,k} = \sum^T_{j=1}\alpha_{t,ij}h^j_{t,k-1},</script><script type="math/tex; mode=display">e_{t,ij} = v^T\tanh(Wh^i_{t,k-1}+Uh^j_{t,k-1}), W,U \in \mathbb{R}^{b*b},</script><script type="math/tex; mode=display">\alpha_{t,ij} = \frac{exp(e_{t,ij})}{\sum^T_{s=1}exp(e_{t,is})}</script><p>其中$a^i_{t.k}$代表task $i$在step $k$，period $t$的合作向量；$h^j_{t,k-1}$代表了第$j$个任务在step $k-1$，period $t$的hidden state。</p><p>之后，修改GRU单元，允许其他音轨的信息能够充分影响到当前音轨的形成。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="8.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="MLP-Cell"><a href="#MLP-Cell" class="headerlink" title="MLP Cell"></a>MLP Cell</h4><p>为了同时考虑到每种乐器的重要程度，通过其重要程度对hidden states进行整合，作者设计了这样一个模块：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="9.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其公式为：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="10.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h4><p>受上述公式的启发，作者优化了以相同Encoder生成的表示为条件的几个条件概率项的总和。</p><script type="math/tex; mode=display">L(\theta) = {argmax}_\theta(\sum_{T_k}(frac{1}{N_p}\sum^{N_p}_ilogp(Y_i^{T_k}|X_i^{T_k};\theta)))</script><p>其中$\theta = {\theta_{src}, \theta_{trgT_k},T_k = 1,2,…, T_m}$，$m$是任务的总数。$\theta_{src}$是源encoder的参数集，$\theta_{trgT_k}$是第$T_k$个目标音轨的参数集，$N_p$是第$p$个序列对，平行training corpus的大小。</p><h4 id="生成"><a href="#生成" class="headerlink" title="生成"></a>生成</h4><p>假设已经有了旋律序列$M_i = {m_{i1}, m|_{i2), …,m_{il_m}}$，下一个步骤是生成其他的乐器音轨，去配合这个旋律。使用GRU处理序列，得到Decoder的初始state：</p><script type="math/tex; mode=display">\overline{M} = E_mM, Em \in \mathbb{R}^{V_m * d}</script><script type="math/tex; mode=display">s^m_0 = GRU(\overline{m}_i, l_m)</script><p>多序列decoder的输出，与其他乐器音轨联系，所以考虑旋律和其他的伴奏音轨：</p><script type="math/tex; mode=display">s^i_t = AttentionCell(y^i_{t-1}, s^i_{t-1}), t>0, or</script><script type="math/tex; mode=display">s^i_t = MLPCell(u^i_{t-1}, s^i_{t-1})， t>0</script><script type="math/tex; mode=display">y^i_t = softmax(s^i_t)</script><p>其中，$s^i_t$是第$i$个任务在第$t$个step的hidden state。作者使用softmax层，利用这个state得到第$i$个乐器的序列。Attention Cell和MLP Cell，用来得到合作的state，包含了本身的乐器state和其他乐器的state，保持所有乐器和谐。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在两个数据集上做了实验，包括旋律生成和组织生成。</p><h3 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h3><p>做实验的数据集叫做真实世界数据集（a real-world dataset），包含了超过50000个MIDI文件。去除掉没有区分音轨的文件，最后14,077个文件得以保留。每个MIDI文件包含了几种不同的音轨，如：旋律、鼓、贝斯、弦乐。</p><p>为保证实验结果可靠性，作者将所有的MIDI文件移到了C大调或者A小调，保证音乐在同样的调上。然后作者设置速度为60bpm，将一个period片段设置为两个小节长。数据集统计如下表：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="11.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>采用9855个文件作为训练集，2855个用于调整参数，最后1407个文件作为训练集。</p><p>每个GRU的hidden unit设为256，Attention Cell和MLP Cell的参数维度为256。模型使用SGD进行训练，batch size为64，验证集使用交叉熵损失函数。</p><h3 id="旋律生成-1"><a href="#旋律生成-1" class="headerlink" title="旋律生成"></a>旋律生成</h3><p>只使用MIDI的旋律音轨。</p><h4 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h4><ul><li>Magenta，Google的RNN项目</li><li>GANMidi，Yi-Hsuan Yang的MidiNet</li></ul><p>设计了三种不同的方法进行评测：</p><ul><li>完整版</li><li>移除了和弦信息</li><li>分别基于两个损失函数进行交叉训练。</li></ul><h4 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h4><p>使用8个志愿者打分。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="12.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>考虑下列因素：</p><ul><li>节奏</li><li>旋律</li><li>整合性/整体结构</li><li>适合唱的程度</li></ul><h4 id="和弦进行的分析"><a href="#和弦进行的分析" class="headerlink" title="和弦进行的分析"></a>和弦进行的分析</h4><p>作者定义了和弦精确度，来度量生成的旋律是否符合输入的和弦序列：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="13.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>其中$P$是period片段的数量，$y_i$是第$i$个生成旋律，检测出来的和弦，而$\widetilde{y_i}$是输入的和弦。</p><p>准确度达到了82.25%，同时发现随着和弦精确度提升，旋律质量得到了保证。</p><h4 id="休止符分析"><a href="#休止符分析" class="headerlink" title="休止符分析"></a>休止符分析</h4><p>作者通过对比生成音乐片段的最小、最大、平均长度，对比数据集音乐，分布如果接近，证明模型的休止是恰当的。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="14.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="组织生成"><a href="#组织生成" class="headerlink" title="组织生成"></a>组织生成</h3><h4 id="Baseline-1"><a href="#Baseline-1" class="headerlink" title="Baseline"></a>Baseline</h4><ul><li>HRNN，《Song From Pi》的模型</li></ul><p>对比下面两种方法：</p><ul><li>Attention Cell</li><li>MLP Cell</li></ul><h4 id="整体Performance"><a href="#整体Performance" class="headerlink" title="整体Performance"></a>整体Performance</h4><ul><li>性能优于HRNN</li><li>多音轨得分高于单音轨（但是评测更倾向于整体氛围）</li><li>MLP比Attention得分更高</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="15.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h4 id="和谐程度分析"><a href="#和谐程度分析" class="headerlink" title="和谐程度分析"></a>和谐程度分析</h4><p>考虑：如果两个音轨有相似的和弦进行，我们认为它们和谐。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="16.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>实验发现音轨少，和谐度会更高。因此对于多音轨的生成，和谐度有着更高的要求。</p><h4 id="组织分析"><a href="#组织分析" class="headerlink" title="组织分析"></a>组织分析</h4><ul><li>音符准确度</li><li>莱文斯坦相似度</li><li>音符分布的均方误差</li><li>空白结果</li></ul><p>具体的分析在这里略过。</p><h1 id="见解和感想"><a href="#见解和感想" class="headerlink" title="见解和感想"></a>见解和感想</h1><p>这篇论文的亮点是显而易见的。</p><p>首先在单音轨的音乐生成里，在流行音乐的框架中捕捉了和弦行进式，将其作为输入，进行训练。和弦行进式很好的把握了音乐的主要节奏，是一个很好的突破口。</p><p>我大二刚接触音乐生成的时候，想的思路就是通过和弦行进式进行音乐生成。但是那个时候限于我编码能力太弱，想法未能落地，也接触不到相关的老师来指点我，完全是自己摸索的。总而言之，和弦行进式这个总体思路我非常赞同。</p><p>在单音轨的音乐训练时，数据的预处理非常重要。现在Kaggle有很多音乐相关的数据集，这是我以前没有注意到的。也许有非常合适的数据集来做相关的研究，但是就我目前所知，自带和弦行进式的数据集非常难找。<strong>也就是说数据预处理本身就是一项有挑战性的工作</strong>。对我来说，这个数据的来源非常有趣，我以后也会去Kaggle里找找有趣的数据集。</p><p>音乐相关的工具也是音乐生成的难点之一。怎么表示音乐？这篇文章提到的处理方法是将音符和节奏的one-hot编码embedding处理，作为输入。我认为这一点处理得比较模糊。音符的时长怎么办？一些问题在论文里没有讲述清楚。目前通用的编码是music21编码，<strong>我认为在工程实现上使用music21去处理移调、变速等问题是一个比较现实的做法</strong>。</p><p>那么多音轨音乐呢？这是我认为本文最出彩的一点，可以说是多音轨生成开先河的一个方法。我阅读论文的大部分时间都在理解多音轨音乐的结构上。Attention Cell和MLP Cell我认为非常有创意——如果是我自己来做多音轨，我可能会想到利用Attention，但无法以明确的形式将这个方法落地。</p><p>本文提出了一些分析方法，我认为是具有其独到之处的。音乐生成和文本生成一样，都有着没法量化的困难，而本文所做的分析恰恰是对Evaluation一个很好的补充。本文结合了和弦行进式，提出了一个Chord Accuracy，我在文中翻译为和弦精确度。这是一个直接的分析方法，又没有像BLEU那样放在音符的层次，而是放在了和弦的层次。我在这里好奇一点，作者是怎么识别生成音乐所属的和弦的？如果有一个自动的办法，<strong>可不可以将其加入Loss函数</strong>？</p><p>对于音符分布的分析方法对我来说也是一个启发，因为音乐生成的休止也一直是悬而未决的难题，或者说没有根本理论来解决。现在通过分布来衡量，那么对于休止就有了一个漂亮的量化过程。下面是我的想法：<strong>如果我们训练一个VAE，类似句子复述的问题一样，将生成的音乐进行微调，使两个分布尽量接近，会怎么样呢</strong></p><p>总而言之，这篇文章在多音轨音乐的生成上做了贡献，是一篇非常棒的文章。美中不足的是，作者在细节上面没有完全说明；没有放出音乐的sample导致无法客观评估其生成水准，以及最终的效果理应还有调整与改进的途径。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;小冰乐队是我一直想做笔记的一篇论文，做音乐生成的人不多，这篇论文思路和我的想法又有些相似。这篇由中科大和微软合作撰写的论文，发表在KDD’18上。&lt;br&gt;论文地址：&lt;a href=&quot;http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.kdd.org/kdd2018/accepted-papers/view/xiaoice-banda-melody-and-arrangement-generation-framework-for-pop-music&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="音乐生成" scheme="http://ldzhangyx.github.io/tags/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>《Multi-modal Sentence Summarization with Modality Attention and Image Filtering》论文笔记</title>
    <link href="http://ldzhangyx.github.io/2018/09/25/Multi-modal%20Sentence%20Summarization%20with%20Modality%20Attention%20and%20Image%20Filtering/"/>
    <id>http://ldzhangyx.github.io/2018/09/25/Multi-modal Sentence Summarization with Modality Attention and Image Filtering/</id>
    <published>2018-09-25T06:07:31.000Z</published>
    <updated>2018-09-26T07:07:45.350Z</updated>
    
    <content type="html"><![CDATA[<p>宗成庆老师的这篇文章发表于<a href="https://acl2018.org/programme/papers/" target="_blank" rel="noopener">ACL’18</a>，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。<br>原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf</p><a id="more"></a><h1 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文介绍了一种多模态句子摘要任务，输入为一张图片和一对句子，输出为摘要。作者称其为MMSS任务。任务的难点在于怎么将视觉信息输入到框架里，以及怎么减小噪音。针对这两个问题，作者分别提出了两个方法：同时施加不同的注意力到文本和图片上；使用图像过滤Image Filter来降噪。</p><p>介绍主要是讲了文本摘要的历史，以及多模态方法最早应用在翻译领域，表现特别好，但是作者认为在摘要上表现得应该更好。</p><p>在解决MMSS任务的时候，作者准备使用分层注意力机制，底层分别关注图片和文本的内部，而上层对两个模态进行平衡。因为图片不能表现很多抽象内容，所以图片特征需要过滤去噪；为了解决生成句子结巴的问题，使用了coverage方法。</p><p>顺便他们做了一个数据集，真是让人肝疼的工作量。</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="1.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>图片展示了多模态模型的实际效果要好于文本模型。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>模型可见由句子编码器，图片编码器，摘要解码器和图片过滤器四个部分组成。</p><h3 id="模型图"><a href="#模型图" class="headerlink" title="模型图"></a>模型图</h3><p>模型简图如下：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="2.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h3 id="句子Encoder"><a href="#句子Encoder" class="headerlink" title="句子Encoder"></a>句子Encoder</h3><p>模型的句子Encoder使用了Bi-GRU。</p><script type="math/tex; mode=display">\overrightarrow{h}_i =  GRU(E[x_i], \overrightarrow{h}_{i-1})</script><script type="math/tex; mode=display">\overleftarrow{h}_i =  GRU(E[x_i], \overleftarrow{h}_{i-1})</script><h3 id="图片Encoder"><a href="#图片Encoder" class="headerlink" title="图片Encoder"></a>图片Encoder</h3><p>图片Encoder使用了VGG19，抽取了两种特征：7x7x512的局部特征（flatten之后成为了49x512），和4096维的全局特征。其中局部特征表示为：</p><script type="math/tex; mode=display">A = (a_1, ..., a_L), L=49 \\a_l \in \mathbb{R}^{512}</script><h3 id="摘要Decoder"><a href="#摘要Decoder" class="headerlink" title="摘要Decoder"></a>摘要Decoder</h3><p>Decoder使用了单向GRU：</p><script type="math/tex; mode=display">s_t = GRU(s_{t-1}, y_{t-1}, c_t)</script><p>其中，初始状态$s_0$，作者提出了几种初始化策略：</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h2}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{q}q)</script><p>利用了全局特征；</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_i)</script><p>与</p><script type="math/tex; mode=display">s_0 = \tanh(\mathbf{W}_{h3}[\overrightarrow{h}_{n};\overleftarrow{h}_{1}]+ \mathbf{W}_{v}\frac{1}{L}\sum_{i=1}^{L}a_ia_i)</script><p>利用了局部特征；</p><p>其中，$q$为全局特征，$a$为各个局部特征，如上文所说。</p><h3 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h3><p>在Decoder使用的context向量$c_t$，使用一个顶层的Attention，对图片context和文本context进行平衡统一：</p><script type="math/tex; mode=display">c_t = \beta_t^{txt}\mathbf{V}_Tc_{txt} + \beta_t^{img}\mathbf{V}_Ic_{img}</script><p>其中，</p><script type="math/tex; mode=display">\beta_t^{txt} = \sigma(\mathbf{U}_as_{t-1}+\mathbf{W}_ac_{txt})</script><script type="math/tex; mode=display">\beta_t^{img} = \sigma(\mathbf{U}_bs_{t-1}+\mathbf{W}_bc_{img})</script><p>对于文字的attention，使用普通的attention：</p><script type="math/tex; mode=display">c_{txt} = \sum_{i=1}^N\alpha_{t,i}^{txt}h_i</script><p>其中$N$为输入的文字序列长度，也就是embedding过后的向量总数量，$h_{i}$是Encoder的第$i$个Hidden State，这个公式将其加权求和。其中$\alpha_{t,i}^{txt}$与Decoder上一个时态的状态$s_{t-1}$与第$i$个Encoder的Hidden State，$h_{i}$有关。</p><script type="math/tex; mode=display">\alpha_{t,i}^{txt} = \frac{exp(e_{t,i}^{txt})}{\sum_{j=1}^Nexp(e_{t,j}^{txt})}</script><script type="math/tex; mode=display">e_{t,i}^{txt} = f(s_{t-1}, h_i) = v_a^T\tanh(\mathbf{U}_cs_{t-1} + \mathbf{W}_ch_i)</script><p>而对于图片attention，作者将每个feature map作为输入，进行attention处理：</p><script type="math/tex; mode=display">c_{img} = \sum_{i=1}^L\alpha_{t,i}^{img}a_l</script><p>其中L为feature map数量，$a_l$为局部特征。</p><script type="math/tex; mode=display">\alpha_{t,i}^{img} = \frac{exp(e_{t,i}^{img})}{\sum_{j=1}^Nexp(e_{t,j}^{img})}</script><script type="math/tex; mode=display">e_{t,i}^{img} = f(s_{t-1}, h_i) = v_b^T\tanh(\mathbf{U}_ds_{t-1} + \mathbf{W}_da_l)</script><h3 id="Coverage-Model-覆盖率模型"><a href="#Coverage-Model-覆盖率模型" class="headerlink" title="Coverage Model(覆盖率模型)"></a>Coverage Model(覆盖率模型)</h3><p>覆盖率模型可以参考《Get to the point》一文的论文笔记。</p><h3 id="Image-Filter"><a href="#Image-Filter" class="headerlink" title="Image Filter"></a>Image Filter</h3><p>作者设计了两个filter：Image Attention Filter和Image Context Filter。</p><h4 id="Image-Attention-Filter"><a href="#Image-Attention-Filter" class="headerlink" title="Image Attention Filter"></a>Image Attention Filter</h4><p>Image Attention Filter的目的在于“directly applied to change the attention scale between image and text”，即根据图片与文本的相关性进行数值控制。</p><script type="math/tex; mode=display">I_a = \sigma(v_s^Ts_0 + c_q^Tq + v_r^Ts_{t-1})</script><p>然后用这个系数更新：</p><script type="math/tex; mode=display">\beta_t^{img} = I_a \cdot \beta_t^{img}</script><p>其中$s_{0}$是decoder的初始状态，$q$是图片全局特征，这两个参数用来表示图片相关性；$s_{t−1}$是decoder上一个time step的状态，用来表示与下一个单词的联系。</p><h4 id="Image-Context-Filter"><a href="#Image-Context-Filter" class="headerlink" title="Image Context Filter"></a>Image Context Filter</h4><p>对于Image Context Vector，作者解释是脱胎于以前的思路（“Image context filter is partially inspired by gating mechanism which has gained great popularity in neural network models”），但是应用在多模态方法上仍属创新。</p><script type="math/tex; mode=display">I_a = \sigma(\mathbf{W}_ss_0 + \mathbf{W}_qq + \mathbf{W}_rs_{t-1})</script><p>这里的矩阵相乘可以使用Dense Layer实现，但是$I_c$与$I_a$最大的不同在于，$I_c$是一个向量，其元素用于控制特征的选择。</p><script type="math/tex; mode=display">c_{img} = I_{c} \odot c_{img}</script><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>本文贡献了一个MMSS任务使用的数据集。数据集首先从Gigaword Corpus里作为基础，对每个Headline，使用Yahoo!搜索图片，并取出top 5图片。之后删除无关图片（人物，缩略图，广告），雇佣10个研究生选择最匹配的图片（无图片标0），最后得到66,000条可用数据。随机将62,000作为训练集，2,000作为测试集，2,000作为开发集。</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>作者做了广泛的对比：</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="3.png" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>Q：对Decoder状态进行的初始化有效吗？<br>A：有效。</p><p>Q：图片有助于提升摘要质量吗？<br>A：是的，可能是因为图片提供了更多信息。</p><p>Q：哪种图片更有效？<br>A：要求3个研究生标注300条数据中图片的匹配程度（匹配从1到3），发现更加匹配的图片所在的那条数据，模型能获得更高的ROUGE分数。</p><p>Q：多模态coverage的有效性？<br>A：通过计算重复词，textual与visual coverage确实能减少重复词的出现。</p><h1 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h1><h2 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h2><p>这篇文章的思路与一般的额外信息有一些区别，同是用额外信息干涉指导文本生成，这个模型同时使用了两个attention，并提出了一种加权机制将两个attention组合起来。在我读过的另一篇文章《Diversity driven Attention Model for Query-based Abstractive Summarization》中，作者试图用query的context干涉document的context，而不直接显性参与decoder的过程。</p><p>文中计算权值的时候，充分考虑了各种可能性；在Image Attention Filter那一块，将数个特征非线性组合起来，虽然显得参数有点多，好在不无道理。</p><p>贡献了一个全新的数据集（动用了10个研究生，真有钱），脱胎于Gigawords，对这个领域做出了基础性贡献。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>首先，对于数据集，我认为可以选择替代的数据集，可能是我之前做过中文摘要，我很自然地就想到了LCSTS，可以用同样的方法构建数据集，但是这个很费时间精力，而且并不是什么突出的想法贡献。</p><p>其次，为什么用VGG提取特征？在这个框架里，VGG提取特征取到的效果我持保留态度。即使有了图片特征又能怎么样呢？到底是一个怎样的机制让VGG的图片特征与关键字对上的？我认为这样的attention拼凑框架思路时非常棒的，但是图片特征与文字的多模态映射我始终不明白how it works. 作为替代方法，我很自然地想到了CV里的目标检测，使用选择性搜索，SVM判断图像中的实体，再作为特征送进模型，我认为这是一个更接近直觉的做法。</p><p>再次，Image Filtering这个做法我认为需要改进为更加reasonable的做法。我们完全可以做一个key-word版本的Filter。Filter有两个版本，Image Attention Filter的系数$I_a$是根据图像与文本的相关性来控制图像干预的程度；而Image Context Filter的系数$T_c$是用来突出图像特征的。这个想法理应可以迁移到word的使用上。</p><p>最后，文中用到的小trick，textual coverage mechanism，为了解决结巴问题，我们可以考虑其他的机制，比如将context vector做软正交化处理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;宗成庆老师的这篇文章发表于&lt;a href=&quot;https://acl2018.org/programme/papers/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ACL’18&lt;/a&gt;，同时获得了国家自然科学基金的支持。文章着眼于利用图片信息提升摘要与原文本的相关性。&lt;br&gt;原文：www.nlpr.ia.ac.cn/cip/ZongPublications/2018/2018HaoranLiIJCAI.pdf&lt;/p&gt;
    
    </summary>
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文笔记" scheme="http://ldzhangyx.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="多模态" scheme="http://ldzhangyx.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
      <category term="生成式摘要" scheme="http://ldzhangyx.github.io/tags/%E7%94%9F%E6%88%90%E5%BC%8F%E6%91%98%E8%A6%81/"/>
    
      <category term="自然语言处理" scheme="http://ldzhangyx.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
